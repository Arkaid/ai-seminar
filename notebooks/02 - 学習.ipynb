{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習\n",
    "\n",
    "ステップ１で作った関数をもう一度実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# numpyを導入\n",
    "import numpy as np\n",
    "\n",
    "# モデル作成\n",
    "def create_model():    \n",
    "    model = {\n",
    "        # 荷重を -5 ~ 5 の乱数で初期化\n",
    "        \"weights\" : np.random.uniform(-5, 5, 2),    \n",
    "        # バイアスも！\n",
    "        \"bias\"    : np.random.uniform(-5, 5, 1)}\n",
    "    return model\n",
    "\n",
    "# 推論\n",
    "def predict(model, activation, x):\n",
    "    \n",
    "    # 足し算を計算し…\n",
    "    y = model[\"weights\"][0] * x[0] + model[\"weights\"][1] * x[1] + model[\"bias\"]\n",
    "    \n",
    "    # 活性化で処理し、その結果を返す\n",
    "    y = activation(y)\n",
    "    return y\n",
    "\n",
    "# 線形活性化関数\n",
    "def linear_activation(x):\n",
    "    return x\n",
    "\n",
    "# ステップ活性化関数\n",
    "def step_activation(x):\n",
    "    if x >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差を計算\n",
    "\n",
    "正しい答え（ラベル）と推論した答えの差分を用い、学習させる。\n",
    "\n",
    "まず、「AND」のラベルを準備しよう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 入力\n",
    "x_list = np.array([\n",
    "    [0, 0], \n",
    "    [0, 1], \n",
    "    [1, 0], \n",
    "    [1, 1]\n",
    "], dtype = float)\n",
    "print(x_list.shape)\n",
    "\n",
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [0], \n",
    "    [0], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差（損失）関数を実装しよう。\n",
    "\n",
    "課題により、適切な関数を使うべきが、今回の入門課題はただの「差分」にしよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "def error(y_true, y_pred):\n",
    "    return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示する\n",
    "def print_results(model, activation, x_list, y_true):\n",
    "    \n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        err = error(y_t, y_p)\n",
    "        print(x, y_t, \"->\", y_p, \"err:\", err)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [4.09281912] err: [-4.09281912]\n",
      "[0. 1.] [0.] -> [2.88840141] err: [-2.88840141]\n",
      "[1. 0.] [0.] -> [1.07864801] err: [-1.07864801]\n",
      "[1. 1.] [1.] -> [-0.1257697] err: [1.1257697]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 1 err: [-1.]\n",
      "[0. 1.] [0.] -> 1 err: [-1.]\n",
      "[1. 0.] [0.] -> 1 err: [-1.]\n",
      "[1. 1.] [1.] -> 0 err: [1.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "誤差にて、荷重を調整しよう。ただ、「入力」は「０」であると、出力に影響がないため、入力は「１」のときだけに荷重を調整する、つまり：\n",
    "\n",
    "$$ w_i' = w_i + x_i \\cdot error(y_{true}, y_{pred}) $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 荷重を更新する関数\n",
    "def update_weight(w, x, err):\n",
    "    return w + x * err\n",
    "\n",
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "    \n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "        \n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        \n",
    "        # 誤差を計算\n",
    "        err = error(y_t, y_p)\n",
    "        mse = err * err\n",
    "            \n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "        \n",
    "        w0   = update_weight(w0, x[0], err)\n",
    "        w1   = update_weight(w1, x[1], err)\n",
    "        bias = update_weight(bias, 1 , err)\n",
    "        \n",
    "        model[\"weights\"][0] = w0\n",
    "        model[\"weights\"][1] = w1\n",
    "        model[\"bias\"] = bias\n",
    "    \n",
    "    #誤差（損失）としては、平均値を返す\n",
    "    return mse / data_size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [0.16392515]\n"
     ]
    }
   ],
   "source": [
    "loss = fit_single_step(model, linear_activation, x_list, y_true)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [2.20441771] err: [-2.20441771]\n",
      "[0. 1.] [0.] -> [1.3946643] err: [-1.3946643]\n",
      "[1. 0.] [0.] -> [0.19024659] err: [-0.19024659]\n",
      "[1. 1.] [1.] -> [-0.61950681] err: [1.61950681]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, activation, x_list, y_true, epochs):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss = fit_single_step(model, activation, x_list, y_true)\n",
    "        print(\"epoch:\", i, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [0.01044665]\n",
      "epoch: 1 loss: [0.00904844]\n",
      "epoch: 2 loss: [0.1582378]\n",
      "epoch: 3 loss: [0.35417174]\n",
      "epoch: 4 loss: [0.80602894]\n",
      "epoch: 5 loss: [1.19929503]\n",
      "epoch: 6 loss: [1.95382009]\n",
      "epoch: 7 loss: [2.54441833]\n",
      "epoch: 8 loss: [3.60161123]\n",
      "epoch: 9 loss: [4.38954163]\n",
      "epoch: 10 loss: [5.74940238]\n",
      "epoch: 11 loss: [6.73466492]\n",
      "epoch: 12 loss: [8.39719353]\n",
      "epoch: 13 loss: [9.57978822]\n",
      "epoch: 14 loss: [11.54498467]\n",
      "epoch: 15 loss: [12.92491152]\n",
      "epoch: 16 loss: [15.19277582]\n",
      "epoch: 17 loss: [16.77003481]\n",
      "epoch: 18 loss: [19.34056696]\n",
      "epoch: 19 loss: [21.11515811]\n",
      "epoch: 20 loss: [23.98835811]\n",
      "epoch: 21 loss: [25.96028141]\n",
      "epoch: 22 loss: [29.13614926]\n",
      "epoch: 23 loss: [31.3054047]\n",
      "epoch: 24 loss: [34.7839404]\n",
      "epoch: 25 loss: [37.150528]\n",
      "epoch: 26 loss: [40.93173155]\n",
      "epoch: 27 loss: [43.4956513]\n",
      "epoch: 28 loss: [47.5795227]\n",
      "epoch: 29 loss: [50.34077459]\n",
      "epoch: 30 loss: [54.72731384]\n",
      "epoch: 31 loss: [57.68589789]\n",
      "epoch: 32 loss: [62.37510499]\n",
      "epoch: 33 loss: [65.53102118]\n",
      "epoch: 34 loss: [70.52289613]\n",
      "epoch: 35 loss: [73.87614448]\n",
      "epoch: 36 loss: [79.17068728]\n",
      "epoch: 37 loss: [82.72126778]\n",
      "epoch: 38 loss: [88.31847843]\n",
      "epoch: 39 loss: [92.06639107]\n",
      "epoch: 40 loss: [97.96626957]\n",
      "epoch: 41 loss: [101.91151437]\n",
      "epoch: 42 loss: [108.11406072]\n",
      "epoch: 43 loss: [112.25663767]\n",
      "epoch: 44 loss: [118.76185186]\n",
      "epoch: 45 loss: [123.10176096]\n",
      "epoch: 46 loss: [129.90964301]\n",
      "epoch: 47 loss: [134.44688426]\n",
      "epoch: 48 loss: [141.55743416]\n",
      "epoch: 49 loss: [146.29200756]\n",
      "epoch: 50 loss: [153.7052253]\n",
      "epoch: 51 loss: [158.63713085]\n",
      "epoch: 52 loss: [166.35301645]\n",
      "epoch: 53 loss: [171.48225415]\n",
      "epoch: 54 loss: [179.5008076]\n",
      "epoch: 55 loss: [184.82737745]\n",
      "epoch: 56 loss: [193.14859874]\n",
      "epoch: 57 loss: [198.67250074]\n",
      "epoch: 58 loss: [207.29638989]\n",
      "epoch: 59 loss: [213.01762404]\n",
      "epoch: 60 loss: [221.94418103]\n",
      "epoch: 61 loss: [227.86274733]\n",
      "epoch: 62 loss: [237.09197218]\n",
      "epoch: 63 loss: [243.20787063]\n",
      "epoch: 64 loss: [252.73976333]\n",
      "epoch: 65 loss: [259.05299393]\n",
      "epoch: 66 loss: [268.88755447]\n",
      "epoch: 67 loss: [275.39811722]\n",
      "epoch: 68 loss: [285.53534562]\n",
      "epoch: 69 loss: [292.24324052]\n",
      "epoch: 70 loss: [302.68313676]\n",
      "epoch: 71 loss: [309.58836382]\n",
      "epoch: 72 loss: [320.33092791]\n",
      "epoch: 73 loss: [327.43348711]\n",
      "epoch: 74 loss: [338.47871906]\n",
      "epoch: 75 loss: [345.77861041]\n",
      "epoch: 76 loss: [357.1265102]\n",
      "epoch: 77 loss: [364.62373371]\n",
      "epoch: 78 loss: [376.27430135]\n",
      "epoch: 79 loss: [383.968857]\n",
      "epoch: 80 loss: [395.92209249]\n",
      "epoch: 81 loss: [403.8139803]\n",
      "epoch: 82 loss: [416.06988364]\n",
      "epoch: 83 loss: [424.1591036]\n",
      "epoch: 84 loss: [436.71767479]\n",
      "epoch: 85 loss: [445.00422689]\n",
      "epoch: 86 loss: [457.86546593]\n",
      "epoch: 87 loss: [466.34935019]\n",
      "epoch: 88 loss: [479.51325708]\n",
      "epoch: 89 loss: [488.19447348]\n",
      "epoch: 90 loss: [501.66104823]\n",
      "epoch: 91 loss: [510.53959678]\n",
      "epoch: 92 loss: [524.30883937]\n",
      "epoch: 93 loss: [533.38472008]\n",
      "epoch: 94 loss: [547.45663052]\n",
      "epoch: 95 loss: [556.72984337]\n",
      "epoch: 96 loss: [571.10442166]\n",
      "epoch: 97 loss: [580.57496667]\n",
      "epoch: 98 loss: [595.25221281]\n",
      "epoch: 99 loss: [604.92008997]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習率とは\n",
    "\n",
    "上記のように、差分だけを直そうとすると、平均的な誤差がお大きくなってしまう。その理由は、確か、勾配の方向が正しいが、ステップが大きいすぎる。つまり、最適な数値から大幅に超えてしまい、段々離れてしまう。\n",
    "\n",
    "「学習率」という係数で、ステップの大きさを小さくし、少しずつ最適な数値に近づくようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "    \n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "        \n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        \n",
    "        # 誤差を計算\n",
    "        err = error(y_t, y_p)\n",
    "        mse = err * err\n",
    "        \n",
    "        # 学習率\n",
    "        learning_rate = 0.01\n",
    "\n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "        \n",
    "        w0   = update_weight(w0, x[0], err * learning_rate)\n",
    "        w1   = update_weight(w1, x[1], err * learning_rate)\n",
    "        bias = update_weight(bias, 1 , err * learning_rate)\n",
    "        \n",
    "        model[\"weights\"][0] = w0\n",
    "        model[\"weights\"][1] = w1\n",
    "        model[\"bias\"] = bias\n",
    "        \n",
    "    return mse / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [2391.60156233]\n",
      "epoch: 1 loss: [2227.90434023]\n",
      "epoch: 2 loss: [2078.91963666]\n",
      "epoch: 3 loss: [1943.17141778]\n",
      "epoch: 4 loss: [1819.34200535]\n",
      "epoch: 5 loss: [1706.25429315]\n",
      "epoch: 6 loss: [1602.85602104]\n",
      "epoch: 7 loss: [1508.20586425]\n",
      "epoch: 8 loss: [1421.46112409]\n",
      "epoch: 9 loss: [1341.86683215]\n",
      "epoch: 10 loss: [1268.74610183]\n",
      "epoch: 11 loss: [1201.4915814]\n",
      "epoch: 12 loss: [1139.55787968]\n",
      "epoch: 13 loss: [1082.45485092]\n",
      "epoch: 14 loss: [1029.74163872]\n",
      "epoch: 15 loss: [981.02139101]\n",
      "epoch: 16 loss: [935.93656807]\n",
      "epoch: 17 loss: [894.16477513]\n",
      "epoch: 18 loss: [855.41505902]\n",
      "epoch: 19 loss: [819.42461539]\n",
      "epoch: 20 loss: [785.9558595]\n",
      "epoch: 21 loss: [754.79381886]\n",
      "epoch: 22 loss: [725.74381111]\n",
      "epoch: 23 loss: [698.62937459]\n",
      "epoch: 24 loss: [673.29042301]\n",
      "epoch: 25 loss: [649.58159896]\n",
      "epoch: 26 loss: [627.37080371]\n",
      "epoch: 27 loss: [606.53788375]\n",
      "epoch: 28 loss: [586.97345631]\n",
      "epoch: 29 loss: [568.57785865]\n",
      "epoch: 30 loss: [551.26020721]\n",
      "epoch: 31 loss: [534.93755461]\n",
      "epoch: 32 loss: [519.53413368]\n",
      "epoch: 33 loss: [504.98067911]\n",
      "epoch: 34 loss: [491.21381818]\n",
      "epoch: 35 loss: [478.17552315]\n",
      "epoch: 36 loss: [465.81261865]\n",
      "epoch: 37 loss: [454.0763382]\n",
      "epoch: 38 loss: [442.92192463]\n",
      "epoch: 39 loss: [432.30826978]\n",
      "epoch: 40 loss: [422.19758921]\n",
      "epoch: 41 loss: [412.55512854]\n",
      "epoch: 42 loss: [403.34889781]\n",
      "epoch: 43 loss: [394.5494312]\n",
      "epoch: 44 loss: [386.12956947]\n",
      "epoch: 45 loss: [378.06426267]\n",
      "epoch: 46 loss: [370.33039126]\n",
      "epoch: 47 loss: [362.9066037]\n",
      "epoch: 48 loss: [355.77316882]\n",
      "epoch: 49 loss: [348.91184165]\n",
      "epoch: 50 loss: [342.30574126]\n",
      "epoch: 51 loss: [335.93923943]\n",
      "epoch: 52 loss: [329.79785931]\n",
      "epoch: 53 loss: [323.8681828]\n",
      "epoch: 54 loss: [318.13776612]\n",
      "epoch: 55 loss: [312.59506255]\n",
      "epoch: 56 loss: [307.22935186]\n",
      "epoch: 57 loss: [302.03067569]\n",
      "epoch: 58 loss: [296.98977841]\n",
      "epoch: 59 loss: [292.09805286]\n",
      "epoch: 60 loss: [287.34749068]\n",
      "epoch: 61 loss: [282.73063663]\n",
      "epoch: 62 loss: [278.24054675]\n",
      "epoch: 63 loss: [273.8707498]\n",
      "epoch: 64 loss: [269.61521193]\n",
      "epoch: 65 loss: [265.46830407]\n",
      "epoch: 66 loss: [261.42477197]\n",
      "epoch: 67 loss: [257.47970862]\n",
      "epoch: 68 loss: [253.62852877]\n",
      "epoch: 69 loss: [249.86694552]\n",
      "epoch: 70 loss: [246.19094862]\n",
      "epoch: 71 loss: [242.5967846]\n",
      "epoch: 72 loss: [239.08093822]\n",
      "epoch: 73 loss: [235.64011553]\n",
      "epoch: 74 loss: [232.27122806]\n",
      "epoch: 75 loss: [228.97137832]\n",
      "epoch: 76 loss: [225.7378463]\n",
      "epoch: 77 loss: [222.56807702]\n",
      "epoch: 78 loss: [219.45966902]\n",
      "epoch: 79 loss: [216.41036366]\n",
      "epoch: 80 loss: [213.41803524]\n",
      "epoch: 81 loss: [210.48068181]\n",
      "epoch: 82 loss: [207.59641671]\n",
      "epoch: 83 loss: [204.76346068]\n",
      "epoch: 84 loss: [201.98013454]\n",
      "epoch: 85 loss: [199.2448524]\n",
      "epoch: 86 loss: [196.55611542]\n",
      "epoch: 87 loss: [193.9125059]\n",
      "epoch: 88 loss: [191.31268192]\n",
      "epoch: 89 loss: [188.75537226]\n",
      "epoch: 90 loss: [186.23937172]\n",
      "epoch: 91 loss: [183.76353682]\n",
      "epoch: 92 loss: [181.3267817]\n",
      "epoch: 93 loss: [178.92807439]\n",
      "epoch: 94 loss: [176.56643331]\n",
      "epoch: 95 loss: [174.24092404]\n",
      "epoch: 96 loss: [171.95065626]\n",
      "epoch: 97 loss: [169.69478096]\n",
      "epoch: 98 loss: [167.47248783]\n",
      "epoch: 99 loss: [165.28300281]\n",
      "epoch: 100 loss: [163.12558582]\n",
      "epoch: 101 loss: [160.99952865]\n",
      "epoch: 102 loss: [158.90415301]\n",
      "epoch: 103 loss: [156.83880868]\n",
      "epoch: 104 loss: [154.80287178]\n",
      "epoch: 105 loss: [152.79574324]\n",
      "epoch: 106 loss: [150.81684725]\n",
      "epoch: 107 loss: [148.86562992]\n",
      "epoch: 108 loss: [146.941558]\n",
      "epoch: 109 loss: [145.04411761]\n",
      "epoch: 110 loss: [143.17281319]\n",
      "epoch: 111 loss: [141.32716645]\n",
      "epoch: 112 loss: [139.50671533]\n",
      "epoch: 113 loss: [137.71101314]\n",
      "epoch: 114 loss: [135.93962772]\n",
      "epoch: 115 loss: [134.1921406]\n",
      "epoch: 116 loss: [132.46814628]\n",
      "epoch: 117 loss: [130.76725156]\n",
      "epoch: 118 loss: [129.08907484]\n",
      "epoch: 119 loss: [127.43324555]\n",
      "epoch: 120 loss: [125.7994036]\n",
      "epoch: 121 loss: [124.18719878]\n",
      "epoch: 122 loss: [122.59629036]\n",
      "epoch: 123 loss: [121.02634654]\n",
      "epoch: 124 loss: [119.47704407]\n",
      "epoch: 125 loss: [117.94806781]\n",
      "epoch: 126 loss: [116.43911038]\n",
      "epoch: 127 loss: [114.94987178]\n",
      "epoch: 128 loss: [113.48005906]\n",
      "epoch: 129 loss: [112.02938601]\n",
      "epoch: 130 loss: [110.59757287]\n",
      "epoch: 131 loss: [109.18434605]\n",
      "epoch: 132 loss: [107.78943785]\n",
      "epoch: 133 loss: [106.41258626]\n",
      "epoch: 134 loss: [105.05353469]\n",
      "epoch: 135 loss: [103.71203177]\n",
      "epoch: 136 loss: [102.38783117]\n",
      "epoch: 137 loss: [101.08069136]\n",
      "epoch: 138 loss: [99.79037548]\n",
      "epoch: 139 loss: [98.51665114]\n",
      "epoch: 140 loss: [97.25929028]\n",
      "epoch: 141 loss: [96.018069]\n",
      "epoch: 142 loss: [94.79276743]\n",
      "epoch: 143 loss: [93.58316961]\n",
      "epoch: 144 loss: [92.38906333]\n",
      "epoch: 145 loss: [91.21024004]\n",
      "epoch: 146 loss: [90.04649473]\n",
      "epoch: 147 loss: [88.89762581]\n",
      "epoch: 148 loss: [87.76343502]\n",
      "epoch: 149 loss: [86.64372733]\n",
      "epoch: 150 loss: [85.53831087]\n",
      "epoch: 151 loss: [84.4469968]\n",
      "epoch: 152 loss: [83.36959926]\n",
      "epoch: 153 loss: [82.30593528]\n",
      "epoch: 154 loss: [81.25582473]\n",
      "epoch: 155 loss: [80.2190902]\n",
      "epoch: 156 loss: [79.19555697]\n",
      "epoch: 157 loss: [78.18505295]\n",
      "epoch: 158 loss: [77.18740859]\n",
      "epoch: 159 loss: [76.20245685]\n",
      "epoch: 160 loss: [75.2300331]\n",
      "epoch: 161 loss: [74.26997513]\n",
      "epoch: 162 loss: [73.32212305]\n",
      "epoch: 163 loss: [72.38631925]\n",
      "epoch: 164 loss: [71.46240836]\n",
      "epoch: 165 loss: [70.55023722]\n",
      "epoch: 166 loss: [69.64965479]\n",
      "epoch: 167 loss: [68.76051215]\n",
      "epoch: 168 loss: [67.88266245]\n",
      "epoch: 169 loss: [67.01596086]\n",
      "epoch: 170 loss: [66.16026453]\n",
      "epoch: 171 loss: [65.31543259]\n",
      "epoch: 172 loss: [64.48132605]\n",
      "epoch: 173 loss: [63.65780783]\n",
      "epoch: 174 loss: [62.8447427]\n",
      "epoch: 175 loss: [62.04199722]\n",
      "epoch: 176 loss: [61.24943978]\n",
      "epoch: 177 loss: [60.46694049]\n",
      "epoch: 178 loss: [59.69437122]\n",
      "epoch: 179 loss: [58.93160551]\n",
      "epoch: 180 loss: [58.17851859]\n",
      "epoch: 181 loss: [57.43498735]\n",
      "epoch: 182 loss: [56.70089026]\n",
      "epoch: 183 loss: [55.97610743]\n",
      "epoch: 184 loss: [55.26052051]\n",
      "epoch: 185 loss: [54.55401271]\n",
      "epoch: 186 loss: [53.85646877]\n",
      "epoch: 187 loss: [53.16777491]\n",
      "epoch: 188 loss: [52.48781884]\n",
      "epoch: 189 loss: [51.81648973]\n",
      "epoch: 190 loss: [51.15367818]\n",
      "epoch: 191 loss: [50.49927621]\n",
      "epoch: 192 loss: [49.85317722]\n",
      "epoch: 193 loss: [49.21527601]\n",
      "epoch: 194 loss: [48.5854687]\n",
      "epoch: 195 loss: [47.96365278]\n",
      "epoch: 196 loss: [47.34972703]\n",
      "epoch: 197 loss: [46.74359155]\n",
      "epoch: 198 loss: [46.1451477]\n",
      "epoch: 199 loss: [45.55429812]\n",
      "epoch: 200 loss: [44.97094668]\n",
      "epoch: 201 loss: [44.3949985]\n",
      "epoch: 202 loss: [43.82635989]\n",
      "epoch: 203 loss: [43.26493837]\n",
      "epoch: 204 loss: [42.71064262]\n",
      "epoch: 205 loss: [42.16338251]\n",
      "epoch: 206 loss: [41.62306904]\n",
      "epoch: 207 loss: [41.08961435]\n",
      "epoch: 208 loss: [40.5629317]\n",
      "epoch: 209 loss: [40.04293543]\n",
      "epoch: 210 loss: [39.529541]\n",
      "epoch: 211 loss: [39.02266492]\n",
      "epoch: 212 loss: [38.52222478]\n",
      "epoch: 213 loss: [38.02813919]\n",
      "epoch: 214 loss: [37.54032781]\n",
      "epoch: 215 loss: [37.05871133]\n",
      "epoch: 216 loss: [36.58321141]\n",
      "epoch: 217 loss: [36.11375073]\n",
      "epoch: 218 loss: [35.65025295]\n",
      "epoch: 219 loss: [35.19264267]\n",
      "epoch: 220 loss: [34.74084548]\n",
      "epoch: 221 loss: [34.29478789]\n",
      "epoch: 222 loss: [33.85439733]\n",
      "epoch: 223 loss: [33.41960218]\n",
      "epoch: 224 loss: [32.9903317]\n",
      "epoch: 225 loss: [32.56651604]\n",
      "epoch: 226 loss: [32.14808627]\n",
      "epoch: 227 loss: [31.73497428]\n",
      "epoch: 228 loss: [31.32711286]\n",
      "epoch: 229 loss: [30.92443563]\n",
      "epoch: 230 loss: [30.52687705]\n",
      "epoch: 231 loss: [30.13437243]\n",
      "epoch: 232 loss: [29.74685785]\n",
      "epoch: 233 loss: [29.36427025]\n",
      "epoch: 234 loss: [28.98654733]\n",
      "epoch: 235 loss: [28.61362759]\n",
      "epoch: 236 loss: [28.2454503]\n",
      "epoch: 237 loss: [27.88195551]\n",
      "epoch: 238 loss: [27.52308401]\n",
      "epoch: 239 loss: [27.16877735]\n",
      "epoch: 240 loss: [26.81897781]\n",
      "epoch: 241 loss: [26.4736284]\n",
      "epoch: 242 loss: [26.13267285]\n",
      "epoch: 243 loss: [25.7960556]\n",
      "epoch: 244 loss: [25.46372179]\n",
      "epoch: 245 loss: [25.13561725]\n",
      "epoch: 246 loss: [24.8116885]\n",
      "epoch: 247 loss: [24.49188273]\n",
      "epoch: 248 loss: [24.1761478]\n",
      "epoch: 249 loss: [23.86443222]\n",
      "epoch: 250 loss: [23.55668515]\n",
      "epoch: 251 loss: [23.2528564]\n",
      "epoch: 252 loss: [22.95289641]\n",
      "epoch: 253 loss: [22.65675624]\n",
      "epoch: 254 loss: [22.36438757]\n",
      "epoch: 255 loss: [22.07574269]\n",
      "epoch: 256 loss: [21.79077449]\n",
      "epoch: 257 loss: [21.50943647]\n",
      "epoch: 258 loss: [21.23168268]\n",
      "epoch: 259 loss: [20.95746778]\n",
      "epoch: 260 loss: [20.686747]\n",
      "epoch: 261 loss: [20.41947612]\n",
      "epoch: 262 loss: [20.15561149]\n",
      "epoch: 263 loss: [19.89511001]\n",
      "epoch: 264 loss: [19.63792911]\n",
      "epoch: 265 loss: [19.38402677]\n",
      "epoch: 266 loss: [19.1333615]\n",
      "epoch: 267 loss: [18.88589233]\n",
      "epoch: 268 loss: [18.6415788]\n",
      "epoch: 269 loss: [18.40038097]\n",
      "epoch: 270 loss: [18.1622594]\n",
      "epoch: 271 loss: [17.92717516]\n",
      "epoch: 272 loss: [17.69508978]\n",
      "epoch: 273 loss: [17.4659653]\n",
      "epoch: 274 loss: [17.23976423]\n",
      "epoch: 275 loss: [17.01644957]\n",
      "epoch: 276 loss: [16.79598476]\n",
      "epoch: 277 loss: [16.57833372]\n",
      "epoch: 278 loss: [16.36346081]\n",
      "epoch: 279 loss: [16.15133086]\n",
      "epoch: 280 loss: [15.94190912]\n",
      "epoch: 281 loss: [15.73516129]\n",
      "epoch: 282 loss: [15.53105352]\n",
      "epoch: 283 loss: [15.32955235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 284 loss: [15.13062477]\n",
      "epoch: 285 loss: [14.93423818]\n",
      "epoch: 286 loss: [14.74036039]\n",
      "epoch: 287 loss: [14.54895962]\n",
      "epoch: 288 loss: [14.36000447]\n",
      "epoch: 289 loss: [14.17346397]\n",
      "epoch: 290 loss: [13.98930752]\n",
      "epoch: 291 loss: [13.8075049]\n",
      "epoch: 292 loss: [13.62802629]\n",
      "epoch: 293 loss: [13.45084223]\n",
      "epoch: 294 loss: [13.27592365]\n",
      "epoch: 295 loss: [13.10324183]\n",
      "epoch: 296 loss: [12.93276842]\n",
      "epoch: 297 loss: [12.76447542]\n",
      "epoch: 298 loss: [12.5983352]\n",
      "epoch: 299 loss: [12.43432046]\n",
      "epoch: 300 loss: [12.27240425]\n",
      "epoch: 301 loss: [12.11255997]\n",
      "epoch: 302 loss: [11.95476134]\n",
      "epoch: 303 loss: [11.79898243]\n",
      "epoch: 304 loss: [11.64519762]\n",
      "epoch: 305 loss: [11.49338162]\n",
      "epoch: 306 loss: [11.34350947]\n",
      "epoch: 307 loss: [11.19555651]\n",
      "epoch: 308 loss: [11.04949839]\n",
      "epoch: 309 loss: [10.90531108]\n",
      "epoch: 310 loss: [10.76297084]\n",
      "epoch: 311 loss: [10.62245425]\n",
      "epoch: 312 loss: [10.48373816]\n",
      "epoch: 313 loss: [10.34679973]\n",
      "epoch: 314 loss: [10.2116164]\n",
      "epoch: 315 loss: [10.0781659]\n",
      "epoch: 316 loss: [9.94642624]\n",
      "epoch: 317 loss: [9.81637571]\n",
      "epoch: 318 loss: [9.68799287]\n",
      "epoch: 319 loss: [9.56125654]\n",
      "epoch: 320 loss: [9.43614583]\n",
      "epoch: 321 loss: [9.3126401]\n",
      "epoch: 322 loss: [9.19071898]\n",
      "epoch: 323 loss: [9.07036234]\n",
      "epoch: 324 loss: [8.95155032]\n",
      "epoch: 325 loss: [8.8342633]\n",
      "epoch: 326 loss: [8.71848193]\n",
      "epoch: 327 loss: [8.60418708]\n",
      "epoch: 328 loss: [8.49135986]\n",
      "epoch: 329 loss: [8.37998164]\n",
      "epoch: 330 loss: [8.27003401]\n",
      "epoch: 331 loss: [8.16149879]\n",
      "epoch: 332 loss: [8.05435805]\n",
      "epoch: 333 loss: [7.94859406]\n",
      "epoch: 334 loss: [7.84418933]\n",
      "epoch: 335 loss: [7.74112659]\n",
      "epoch: 336 loss: [7.63938879]\n",
      "epoch: 337 loss: [7.53895908]\n",
      "epoch: 338 loss: [7.43982085]\n",
      "epoch: 339 loss: [7.34195766]\n",
      "epoch: 340 loss: [7.24535333]\n",
      "epoch: 341 loss: [7.14999183]\n",
      "epoch: 342 loss: [7.05585738]\n",
      "epoch: 343 loss: [6.96293436]\n",
      "epoch: 344 loss: [6.87120738]\n",
      "epoch: 345 loss: [6.78066123]\n",
      "epoch: 346 loss: [6.69128088]\n",
      "epoch: 347 loss: [6.60305151]\n",
      "epoch: 348 loss: [6.51595849]\n",
      "epoch: 349 loss: [6.42998734]\n",
      "epoch: 350 loss: [6.34512381]\n",
      "epoch: 351 loss: [6.26135381]\n",
      "epoch: 352 loss: [6.1786634]\n",
      "epoch: 353 loss: [6.09703887]\n",
      "epoch: 354 loss: [6.01646664]\n",
      "epoch: 355 loss: [5.93693333]\n",
      "epoch: 356 loss: [5.8584257]\n",
      "epoch: 357 loss: [5.78093071]\n",
      "epoch: 358 loss: [5.70443546]\n",
      "epoch: 359 loss: [5.62892722]\n",
      "epoch: 360 loss: [5.55439343]\n",
      "epoch: 361 loss: [5.48082168]\n",
      "epoch: 362 loss: [5.40819972]\n",
      "epoch: 363 loss: [5.33651546]\n",
      "epoch: 364 loss: [5.26575694]\n",
      "epoch: 365 loss: [5.19591238]\n",
      "epoch: 366 loss: [5.12697013]\n",
      "epoch: 367 loss: [5.05891871]\n",
      "epoch: 368 loss: [4.99174675]\n",
      "epoch: 369 loss: [4.92544306]\n",
      "epoch: 370 loss: [4.85999657]\n",
      "epoch: 371 loss: [4.79539635]\n",
      "epoch: 372 loss: [4.73163162]\n",
      "epoch: 373 loss: [4.66869173]\n",
      "epoch: 374 loss: [4.60656616]\n",
      "epoch: 375 loss: [4.54524453]\n",
      "epoch: 376 loss: [4.48471659]\n",
      "epoch: 377 loss: [4.42497222]\n",
      "epoch: 378 loss: [4.36600143]\n",
      "epoch: 379 loss: [4.30779434]\n",
      "epoch: 380 loss: [4.25034123]\n",
      "epoch: 381 loss: [4.19363247]\n",
      "epoch: 382 loss: [4.13765857]\n",
      "epoch: 383 loss: [4.08241014]\n",
      "epoch: 384 loss: [4.02787794]\n",
      "epoch: 385 loss: [3.97405281]\n",
      "epoch: 386 loss: [3.92092575]\n",
      "epoch: 387 loss: [3.86848783]\n",
      "epoch: 388 loss: [3.81673026]\n",
      "epoch: 389 loss: [3.76564436]\n",
      "epoch: 390 loss: [3.71522155]\n",
      "epoch: 391 loss: [3.66545336]\n",
      "epoch: 392 loss: [3.61633143]\n",
      "epoch: 393 loss: [3.56784751]\n",
      "epoch: 394 loss: [3.51999345]\n",
      "epoch: 395 loss: [3.4727612]\n",
      "epoch: 396 loss: [3.42614281]\n",
      "epoch: 397 loss: [3.38013046]\n",
      "epoch: 398 loss: [3.33471638]\n",
      "epoch: 399 loss: [3.28989294]\n",
      "epoch: 400 loss: [3.24565258]\n",
      "epoch: 401 loss: [3.20198785]\n",
      "epoch: 402 loss: [3.1588914]\n",
      "epoch: 403 loss: [3.11635596]\n",
      "epoch: 404 loss: [3.07437434]\n",
      "epoch: 405 loss: [3.03293948]\n",
      "epoch: 406 loss: [2.99204438]\n",
      "epoch: 407 loss: [2.95168213]\n",
      "epoch: 408 loss: [2.91184592]\n",
      "epoch: 409 loss: [2.87252901]\n",
      "epoch: 410 loss: [2.83372476]\n",
      "epoch: 411 loss: [2.79542662]\n",
      "epoch: 412 loss: [2.7576281]\n",
      "epoch: 413 loss: [2.7203228]\n",
      "epoch: 414 loss: [2.68350442]\n",
      "epoch: 415 loss: [2.64716672]\n",
      "epoch: 416 loss: [2.61130355]\n",
      "epoch: 417 loss: [2.57590882]\n",
      "epoch: 418 loss: [2.54097654]\n",
      "epoch: 419 loss: [2.50650078]\n",
      "epoch: 420 loss: [2.47247569]\n",
      "epoch: 421 loss: [2.43889551]\n",
      "epoch: 422 loss: [2.40575453]\n",
      "epoch: 423 loss: [2.37304712]\n",
      "epoch: 424 loss: [2.34076772]\n",
      "epoch: 425 loss: [2.30891084]\n",
      "epoch: 426 loss: [2.27747108]\n",
      "epoch: 427 loss: [2.24644307]\n",
      "epoch: 428 loss: [2.21582155]\n",
      "epoch: 429 loss: [2.18560129]\n",
      "epoch: 430 loss: [2.15577715]\n",
      "epoch: 431 loss: [2.12634405]\n",
      "epoch: 432 loss: [2.09729697]\n",
      "epoch: 433 loss: [2.06863095]\n",
      "epoch: 434 loss: [2.04034111]\n",
      "epoch: 435 loss: [2.01242262]\n",
      "epoch: 436 loss: [1.9848707]\n",
      "epoch: 437 loss: [1.95768066]\n",
      "epoch: 438 loss: [1.93084784]\n",
      "epoch: 439 loss: [1.90436766]\n",
      "epoch: 440 loss: [1.87823558]\n",
      "epoch: 441 loss: [1.85244714]\n",
      "epoch: 442 loss: [1.82699792]\n",
      "epoch: 443 loss: [1.80188356]\n",
      "epoch: 444 loss: [1.77709976]\n",
      "epoch: 445 loss: [1.75264226]\n",
      "epoch: 446 loss: [1.72850687]\n",
      "epoch: 447 loss: [1.70468946]\n",
      "epoch: 448 loss: [1.68118592]\n",
      "epoch: 449 loss: [1.65799223]\n",
      "epoch: 450 loss: [1.6351044]\n",
      "epoch: 451 loss: [1.61251849]\n",
      "epoch: 452 loss: [1.59023062]\n",
      "epoch: 453 loss: [1.56823695]\n",
      "epoch: 454 loss: [1.54653369]\n",
      "epoch: 455 loss: [1.52511711]\n",
      "epoch: 456 loss: [1.50398351]\n",
      "epoch: 457 loss: [1.48312926]\n",
      "epoch: 458 loss: [1.46255074]\n",
      "epoch: 459 loss: [1.44224442]\n",
      "epoch: 460 loss: [1.42220677]\n",
      "epoch: 461 loss: [1.40243435]\n",
      "epoch: 462 loss: [1.38292373]\n",
      "epoch: 463 loss: [1.36367154]\n",
      "epoch: 464 loss: [1.34467445]\n",
      "epoch: 465 loss: [1.32592916]\n",
      "epoch: 466 loss: [1.30743242]\n",
      "epoch: 467 loss: [1.28918104]\n",
      "epoch: 468 loss: [1.27117185]\n",
      "epoch: 469 loss: [1.25340172]\n",
      "epoch: 470 loss: [1.23586757]\n",
      "epoch: 471 loss: [1.21856634]\n",
      "epoch: 472 loss: [1.20149504]\n",
      "epoch: 473 loss: [1.18465069]\n",
      "epoch: 474 loss: [1.16803037]\n",
      "epoch: 475 loss: [1.15163117]\n",
      "epoch: 476 loss: [1.13545025]\n",
      "epoch: 477 loss: [1.11948477]\n",
      "epoch: 478 loss: [1.10373197]\n",
      "epoch: 479 loss: [1.08818908]\n",
      "epoch: 480 loss: [1.0728534]\n",
      "epoch: 481 loss: [1.05772224]\n",
      "epoch: 482 loss: [1.04279296]\n",
      "epoch: 483 loss: [1.02806295]\n",
      "epoch: 484 loss: [1.01352963]\n",
      "epoch: 485 loss: [0.99919045]\n",
      "epoch: 486 loss: [0.9850429]\n",
      "epoch: 487 loss: [0.9710845]\n",
      "epoch: 488 loss: [0.9573128]\n",
      "epoch: 489 loss: [0.94372539]\n",
      "epoch: 490 loss: [0.93031987]\n",
      "epoch: 491 loss: [0.91709389]\n",
      "epoch: 492 loss: [0.90404512]\n",
      "epoch: 493 loss: [0.89117127]\n",
      "epoch: 494 loss: [0.87847006]\n",
      "epoch: 495 loss: [0.86593926]\n",
      "epoch: 496 loss: [0.85357666]\n",
      "epoch: 497 loss: [0.84138008]\n",
      "epoch: 498 loss: [0.82934736]\n",
      "epoch: 499 loss: [0.81747637]\n",
      "epoch: 500 loss: [0.80576502]\n",
      "epoch: 501 loss: [0.79421124]\n",
      "epoch: 502 loss: [0.78281297]\n",
      "epoch: 503 loss: [0.7715682]\n",
      "epoch: 504 loss: [0.76047494]\n",
      "epoch: 505 loss: [0.7495312]\n",
      "epoch: 506 loss: [0.73873506]\n",
      "epoch: 507 loss: [0.7280846]\n",
      "epoch: 508 loss: [0.71757791]\n",
      "epoch: 509 loss: [0.70721312]\n",
      "epoch: 510 loss: [0.6969884]\n",
      "epoch: 511 loss: [0.68690192]\n",
      "epoch: 512 loss: [0.67695188]\n",
      "epoch: 513 loss: [0.66713651]\n",
      "epoch: 514 loss: [0.65745404]\n",
      "epoch: 515 loss: [0.64790276]\n",
      "epoch: 516 loss: [0.63848094]\n",
      "epoch: 517 loss: [0.62918691]\n",
      "epoch: 518 loss: [0.62001899]\n",
      "epoch: 519 loss: [0.61097555]\n",
      "epoch: 520 loss: [0.60205497]\n",
      "epoch: 521 loss: [0.59325562]\n",
      "epoch: 522 loss: [0.58457595]\n",
      "epoch: 523 loss: [0.57601438]\n",
      "epoch: 524 loss: [0.56756938]\n",
      "epoch: 525 loss: [0.55923941]\n",
      "epoch: 526 loss: [0.55102299]\n",
      "epoch: 527 loss: [0.54291862]\n",
      "epoch: 528 loss: [0.53492484]\n",
      "epoch: 529 loss: [0.52704021]\n",
      "epoch: 530 loss: [0.5192633]\n",
      "epoch: 531 loss: [0.5115927]\n",
      "epoch: 532 loss: [0.50402702]\n",
      "epoch: 533 loss: [0.49656489]\n",
      "epoch: 534 loss: [0.48920495]\n",
      "epoch: 535 loss: [0.48194587]\n",
      "epoch: 536 loss: [0.47478632]\n",
      "epoch: 537 loss: [0.467725]\n",
      "epoch: 538 loss: [0.46076063]\n",
      "epoch: 539 loss: [0.45389193]\n",
      "epoch: 540 loss: [0.44711765]\n",
      "epoch: 541 loss: [0.44043655]\n",
      "epoch: 542 loss: [0.4338474]\n",
      "epoch: 543 loss: [0.42734902]\n",
      "epoch: 544 loss: [0.42094019]\n",
      "epoch: 545 loss: [0.41461975]\n",
      "epoch: 546 loss: [0.40838654]\n",
      "epoch: 547 loss: [0.4022394]\n",
      "epoch: 548 loss: [0.39617721]\n",
      "epoch: 549 loss: [0.39019886]\n",
      "epoch: 550 loss: [0.38430324]\n",
      "epoch: 551 loss: [0.37848926]\n",
      "epoch: 552 loss: [0.37275584]\n",
      "epoch: 553 loss: [0.36710194]\n",
      "epoch: 554 loss: [0.3615265]\n",
      "epoch: 555 loss: [0.35602848]\n",
      "epoch: 556 loss: [0.35060688]\n",
      "epoch: 557 loss: [0.34526067]\n",
      "epoch: 558 loss: [0.33998888]\n",
      "epoch: 559 loss: [0.33479051]\n",
      "epoch: 560 loss: [0.3296646]\n",
      "epoch: 561 loss: [0.3246102]\n",
      "epoch: 562 loss: [0.31962635]\n",
      "epoch: 563 loss: [0.31471213]\n",
      "epoch: 564 loss: [0.30986663]\n",
      "epoch: 565 loss: [0.30508892]\n",
      "epoch: 566 loss: [0.30037812]\n",
      "epoch: 567 loss: [0.29573334]\n",
      "epoch: 568 loss: [0.2911537]\n",
      "epoch: 569 loss: [0.28663835]\n",
      "epoch: 570 loss: [0.28218644]\n",
      "epoch: 571 loss: [0.27779713]\n",
      "epoch: 572 loss: [0.27346958]\n",
      "epoch: 573 loss: [0.26920298]\n",
      "epoch: 574 loss: [0.26499652]\n",
      "epoch: 575 loss: [0.26084941]\n",
      "epoch: 576 loss: [0.25676085]\n",
      "epoch: 577 loss: [0.25273007]\n",
      "epoch: 578 loss: [0.24875631]\n",
      "epoch: 579 loss: [0.2448388]\n",
      "epoch: 580 loss: [0.2409768]\n",
      "epoch: 581 loss: [0.23716958]\n",
      "epoch: 582 loss: [0.2334164]\n",
      "epoch: 583 loss: [0.22971654]\n",
      "epoch: 584 loss: [0.2260693]\n",
      "epoch: 585 loss: [0.22247398]\n",
      "epoch: 586 loss: [0.21892989]\n",
      "epoch: 587 loss: [0.21543633]\n",
      "epoch: 588 loss: [0.21199265]\n",
      "epoch: 589 loss: [0.20859817]\n",
      "epoch: 590 loss: [0.20525224]\n",
      "epoch: 591 loss: [0.20195422]\n",
      "epoch: 592 loss: [0.19870345]\n",
      "epoch: 593 loss: [0.19549932]\n",
      "epoch: 594 loss: [0.1923412]\n",
      "epoch: 595 loss: [0.18922848]\n",
      "epoch: 596 loss: [0.18616054]\n",
      "epoch: 597 loss: [0.18313679]\n",
      "epoch: 598 loss: [0.18015663]\n",
      "epoch: 599 loss: [0.1772195]\n",
      "epoch: 600 loss: [0.1743248]\n",
      "epoch: 601 loss: [0.17147197]\n",
      "epoch: 602 loss: [0.16866045]\n",
      "epoch: 603 loss: [0.16588969]\n",
      "epoch: 604 loss: [0.16315914]\n",
      "epoch: 605 loss: [0.16046825]\n",
      "epoch: 606 loss: [0.15781651]\n",
      "epoch: 607 loss: [0.15520337]\n",
      "epoch: 608 loss: [0.15262833]\n",
      "epoch: 609 loss: [0.15009087]\n",
      "epoch: 610 loss: [0.14759048]\n",
      "epoch: 611 loss: [0.14512667]\n",
      "epoch: 612 loss: [0.14269895]\n",
      "epoch: 613 loss: [0.14030682]\n",
      "epoch: 614 loss: [0.13794981]\n",
      "epoch: 615 loss: [0.13562744]\n",
      "epoch: 616 loss: [0.13333926]\n",
      "epoch: 617 loss: [0.13108479]\n",
      "epoch: 618 loss: [0.12886358]\n",
      "epoch: 619 loss: [0.12667519]\n",
      "epoch: 620 loss: [0.12451916]\n",
      "epoch: 621 loss: [0.12239507]\n",
      "epoch: 622 loss: [0.12030247]\n",
      "epoch: 623 loss: [0.11824095]\n",
      "epoch: 624 loss: [0.11621008]\n",
      "epoch: 625 loss: [0.11420944]\n",
      "epoch: 626 loss: [0.11223864]\n",
      "epoch: 627 loss: [0.11029725]\n",
      "epoch: 628 loss: [0.10838489]\n",
      "epoch: 629 loss: [0.10650116]\n",
      "epoch: 630 loss: [0.10464567]\n",
      "epoch: 631 loss: [0.10281803]\n",
      "epoch: 632 loss: [0.10101788]\n",
      "epoch: 633 loss: [0.09924482]\n",
      "epoch: 634 loss: [0.0974985]\n",
      "epoch: 635 loss: [0.09577855]\n",
      "epoch: 636 loss: [0.09408462]\n",
      "epoch: 637 loss: [0.09241633]\n",
      "epoch: 638 loss: [0.09077336]\n",
      "epoch: 639 loss: [0.08915535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 640 loss: [0.08756195]\n",
      "epoch: 641 loss: [0.08599284]\n",
      "epoch: 642 loss: [0.08444768]\n",
      "epoch: 643 loss: [0.08292614]\n",
      "epoch: 644 loss: [0.0814279]\n",
      "epoch: 645 loss: [0.07995264]\n",
      "epoch: 646 loss: [0.07850005]\n",
      "epoch: 647 loss: [0.07706981]\n",
      "epoch: 648 loss: [0.07566162]\n",
      "epoch: 649 loss: [0.07427517]\n",
      "epoch: 650 loss: [0.07291017]\n",
      "epoch: 651 loss: [0.07156632]\n",
      "epoch: 652 loss: [0.07024333]\n",
      "epoch: 653 loss: [0.06894091]\n",
      "epoch: 654 loss: [0.06765877]\n",
      "epoch: 655 loss: [0.06639665]\n",
      "epoch: 656 loss: [0.06515426]\n",
      "epoch: 657 loss: [0.06393133]\n",
      "epoch: 658 loss: [0.06272759]\n",
      "epoch: 659 loss: [0.06154278]\n",
      "epoch: 660 loss: [0.06037663]\n",
      "epoch: 661 loss: [0.05922889]\n",
      "epoch: 662 loss: [0.05809929]\n",
      "epoch: 663 loss: [0.0569876]\n",
      "epoch: 664 loss: [0.05589355]\n",
      "epoch: 665 loss: [0.05481691]\n",
      "epoch: 666 loss: [0.05375744]\n",
      "epoch: 667 loss: [0.05271489]\n",
      "epoch: 668 loss: [0.05168902]\n",
      "epoch: 669 loss: [0.05067962]\n",
      "epoch: 670 loss: [0.04968644]\n",
      "epoch: 671 loss: [0.04870926]\n",
      "epoch: 672 loss: [0.04774786]\n",
      "epoch: 673 loss: [0.04680202]\n",
      "epoch: 674 loss: [0.04587151]\n",
      "epoch: 675 loss: [0.04495613]\n",
      "epoch: 676 loss: [0.04405566]\n",
      "epoch: 677 loss: [0.0431699]\n",
      "epoch: 678 loss: [0.04229864]\n",
      "epoch: 679 loss: [0.04144167]\n",
      "epoch: 680 loss: [0.04059879]\n",
      "epoch: 681 loss: [0.03976981]\n",
      "epoch: 682 loss: [0.03895452]\n",
      "epoch: 683 loss: [0.03815275]\n",
      "epoch: 684 loss: [0.03736429]\n",
      "epoch: 685 loss: [0.03658896]\n",
      "epoch: 686 loss: [0.03582657]\n",
      "epoch: 687 loss: [0.03507694]\n",
      "epoch: 688 loss: [0.0343399]\n",
      "epoch: 689 loss: [0.03361525]\n",
      "epoch: 690 loss: [0.03290283]\n",
      "epoch: 691 loss: [0.03220246]\n",
      "epoch: 692 loss: [0.03151398]\n",
      "epoch: 693 loss: [0.0308372]\n",
      "epoch: 694 loss: [0.03017197]\n",
      "epoch: 695 loss: [0.02951813]\n",
      "epoch: 696 loss: [0.0288755]\n",
      "epoch: 697 loss: [0.02824394]\n",
      "epoch: 698 loss: [0.02762327]\n",
      "epoch: 699 loss: [0.02701335]\n",
      "epoch: 700 loss: [0.02641402]\n",
      "epoch: 701 loss: [0.02582514]\n",
      "epoch: 702 loss: [0.02524654]\n",
      "epoch: 703 loss: [0.02467809]\n",
      "epoch: 704 loss: [0.02411964]\n",
      "epoch: 705 loss: [0.02357103]\n",
      "epoch: 706 loss: [0.02303215]\n",
      "epoch: 707 loss: [0.02250283]\n",
      "epoch: 708 loss: [0.02198295]\n",
      "epoch: 709 loss: [0.02147236]\n",
      "epoch: 710 loss: [0.02097094]\n",
      "epoch: 711 loss: [0.02047855]\n",
      "epoch: 712 loss: [0.01999507]\n",
      "epoch: 713 loss: [0.01952035]\n",
      "epoch: 714 loss: [0.01905428]\n",
      "epoch: 715 loss: [0.01859672]\n",
      "epoch: 716 loss: [0.01814756]\n",
      "epoch: 717 loss: [0.01770667]\n",
      "epoch: 718 loss: [0.01727394]\n",
      "epoch: 719 loss: [0.01684923]\n",
      "epoch: 720 loss: [0.01643245]\n",
      "epoch: 721 loss: [0.01602346]\n",
      "epoch: 722 loss: [0.01562216]\n",
      "epoch: 723 loss: [0.01522843]\n",
      "epoch: 724 loss: [0.01484216]\n",
      "epoch: 725 loss: [0.01446324]\n",
      "epoch: 726 loss: [0.01409157]\n",
      "epoch: 727 loss: [0.01372703]\n",
      "epoch: 728 loss: [0.01336953]\n",
      "epoch: 729 loss: [0.01301895]\n",
      "epoch: 730 loss: [0.0126752]\n",
      "epoch: 731 loss: [0.01233818]\n",
      "epoch: 732 loss: [0.01200777]\n",
      "epoch: 733 loss: [0.0116839]\n",
      "epoch: 734 loss: [0.01136645]\n",
      "epoch: 735 loss: [0.01105534]\n",
      "epoch: 736 loss: [0.01075046]\n",
      "epoch: 737 loss: [0.01045173]\n",
      "epoch: 738 loss: [0.01015906]\n",
      "epoch: 739 loss: [0.00987235]\n",
      "epoch: 740 loss: [0.00959151]\n",
      "epoch: 741 loss: [0.00931646]\n",
      "epoch: 742 loss: [0.0090471]\n",
      "epoch: 743 loss: [0.00878336]\n",
      "epoch: 744 loss: [0.00852515]\n",
      "epoch: 745 loss: [0.00827239]\n",
      "epoch: 746 loss: [0.00802498]\n",
      "epoch: 747 loss: [0.00778286]\n",
      "epoch: 748 loss: [0.00754594]\n",
      "epoch: 749 loss: [0.00731414]\n",
      "epoch: 750 loss: [0.00708738]\n",
      "epoch: 751 loss: [0.00686559]\n",
      "epoch: 752 loss: [0.0066487]\n",
      "epoch: 753 loss: [0.00643662]\n",
      "epoch: 754 loss: [0.00622928]\n",
      "epoch: 755 loss: [0.00602661]\n",
      "epoch: 756 loss: [0.00582854]\n",
      "epoch: 757 loss: [0.005635]\n",
      "epoch: 758 loss: [0.00544592]\n",
      "epoch: 759 loss: [0.00526123]\n",
      "epoch: 760 loss: [0.00508085]\n",
      "epoch: 761 loss: [0.00490474]\n",
      "epoch: 762 loss: [0.00473281]\n",
      "epoch: 763 loss: [0.004565]\n",
      "epoch: 764 loss: [0.00440125]\n",
      "epoch: 765 loss: [0.0042415]\n",
      "epoch: 766 loss: [0.00408568]\n",
      "epoch: 767 loss: [0.00393373]\n",
      "epoch: 768 loss: [0.00378559]\n",
      "epoch: 769 loss: [0.00364121]\n",
      "epoch: 770 loss: [0.00350051]\n",
      "epoch: 771 loss: [0.00336345]\n",
      "epoch: 772 loss: [0.00322997]\n",
      "epoch: 773 loss: [0.0031]\n",
      "epoch: 774 loss: [0.0029735]\n",
      "epoch: 775 loss: [0.0028504]\n",
      "epoch: 776 loss: [0.00273066]\n",
      "epoch: 777 loss: [0.00261422]\n",
      "epoch: 778 loss: [0.00250103]\n",
      "epoch: 779 loss: [0.00239103]\n",
      "epoch: 780 loss: [0.00228418]\n",
      "epoch: 781 loss: [0.00218042]\n",
      "epoch: 782 loss: [0.00207971]\n",
      "epoch: 783 loss: [0.00198199]\n",
      "epoch: 784 loss: [0.00188721]\n",
      "epoch: 785 loss: [0.00179533]\n",
      "epoch: 786 loss: [0.00170631]\n",
      "epoch: 787 loss: [0.00162008]\n",
      "epoch: 788 loss: [0.00153662]\n",
      "epoch: 789 loss: [0.00145587]\n",
      "epoch: 790 loss: [0.00137778]\n",
      "epoch: 791 loss: [0.00130232]\n",
      "epoch: 792 loss: [0.00122944]\n",
      "epoch: 793 loss: [0.0011591]\n",
      "epoch: 794 loss: [0.00109126]\n",
      "epoch: 795 loss: [0.00102587]\n",
      "epoch: 796 loss: [0.0009629]\n",
      "epoch: 797 loss: [0.00090229]\n",
      "epoch: 798 loss: [0.00084402]\n",
      "epoch: 799 loss: [0.00078805]\n",
      "epoch: 800 loss: [0.00073433]\n",
      "epoch: 801 loss: [0.00068283]\n",
      "epoch: 802 loss: [0.0006335]\n",
      "epoch: 803 loss: [0.00058632]\n",
      "epoch: 804 loss: [0.00054125]\n",
      "epoch: 805 loss: [0.00049824]\n",
      "epoch: 806 loss: [0.00045727]\n",
      "epoch: 807 loss: [0.0004183]\n",
      "epoch: 808 loss: [0.0003813]\n",
      "epoch: 809 loss: [0.00034622]\n",
      "epoch: 810 loss: [0.00031304]\n",
      "epoch: 811 loss: [0.00028173]\n",
      "epoch: 812 loss: [0.00025225]\n",
      "epoch: 813 loss: [0.00022456]\n",
      "epoch: 814 loss: [0.00019865]\n",
      "epoch: 815 loss: [0.00017447]\n",
      "epoch: 816 loss: [0.000152]\n",
      "epoch: 817 loss: [0.00013121]\n",
      "epoch: 818 loss: [0.00011206]\n",
      "epoch: 819 loss: [9.45219553e-05]\n",
      "epoch: 820 loss: [7.8575844e-05]\n",
      "epoch: 821 loss: [6.41885493e-05]\n",
      "epoch: 822 loss: [5.13318957e-05]\n",
      "epoch: 823 loss: [3.99781163e-05]\n",
      "epoch: 824 loss: [3.00998469e-05]\n",
      "epoch: 825 loss: [2.16701211e-05]\n",
      "epoch: 826 loss: [1.46623646e-05]\n",
      "epoch: 827 loss: [9.05039002e-06]\n",
      "epoch: 828 loss: [4.80839178e-06]\n",
      "epoch: 829 loss: [1.91094087e-06]\n",
      "epoch: 830 loss: [3.32979855e-07]\n",
      "epoch: 831 loss: [4.98178202e-08]\n",
      "epoch: 832 loss: [1.03712548e-06]\n",
      "epoch: 833 loss: [3.27093029e-06]\n",
      "epoch: 834 loss: [6.72761164e-06]\n",
      "epoch: 835 loss: [1.13838961e-05]\n",
      "epoch: 836 loss: [1.72168528e-05]\n",
      "epoch: 837 loss: [2.42038887e-05]\n",
      "epoch: 838 loss: [3.2322744e-05]\n",
      "epoch: 839 loss: [4.1551488e-05]\n",
      "epoch: 840 loss: [5.18685141e-05]\n",
      "epoch: 841 loss: [6.32525359e-05]\n",
      "epoch: 842 loss: [7.56825824e-05]\n",
      "epoch: 843 loss: [8.91379943e-05]\n",
      "epoch: 844 loss: [0.0001036]\n",
      "epoch: 845 loss: [0.00011904]\n",
      "epoch: 846 loss: [0.00013545]\n",
      "epoch: 847 loss: [0.00015281]\n",
      "epoch: 848 loss: [0.00017109]\n",
      "epoch: 849 loss: [0.00019028]\n",
      "epoch: 850 loss: [0.00021036]\n",
      "epoch: 851 loss: [0.00023131]\n",
      "epoch: 852 loss: [0.00025312]\n",
      "epoch: 853 loss: [0.00027576]\n",
      "epoch: 854 loss: [0.00029921]\n",
      "epoch: 855 loss: [0.00032347]\n",
      "epoch: 856 loss: [0.00034851]\n",
      "epoch: 857 loss: [0.00037431]\n",
      "epoch: 858 loss: [0.00040087]\n",
      "epoch: 859 loss: [0.00042816]\n",
      "epoch: 860 loss: [0.00045617]\n",
      "epoch: 861 loss: [0.00048488]\n",
      "epoch: 862 loss: [0.00051427]\n",
      "epoch: 863 loss: [0.00054434]\n",
      "epoch: 864 loss: [0.00057507]\n",
      "epoch: 865 loss: [0.00060643]\n",
      "epoch: 866 loss: [0.00063843]\n",
      "epoch: 867 loss: [0.00067103]\n",
      "epoch: 868 loss: [0.00070424]\n",
      "epoch: 869 loss: [0.00073803]\n",
      "epoch: 870 loss: [0.00077239]\n",
      "epoch: 871 loss: [0.00080731]\n",
      "epoch: 872 loss: [0.00084277]\n",
      "epoch: 873 loss: [0.00087876]\n",
      "epoch: 874 loss: [0.00091528]\n",
      "epoch: 875 loss: [0.00095229]\n",
      "epoch: 876 loss: [0.00098981]\n",
      "epoch: 877 loss: [0.0010278]\n",
      "epoch: 878 loss: [0.00106626]\n",
      "epoch: 879 loss: [0.00110518]\n",
      "epoch: 880 loss: [0.00114454]\n",
      "epoch: 881 loss: [0.00118434]\n",
      "epoch: 882 loss: [0.00122456]\n",
      "epoch: 883 loss: [0.0012652]\n",
      "epoch: 884 loss: [0.00130623]\n",
      "epoch: 885 loss: [0.00134765]\n",
      "epoch: 886 loss: [0.00138945]\n",
      "epoch: 887 loss: [0.00143162]\n",
      "epoch: 888 loss: [0.00147415]\n",
      "epoch: 889 loss: [0.00151703]\n",
      "epoch: 890 loss: [0.00156025]\n",
      "epoch: 891 loss: [0.00160379]\n",
      "epoch: 892 loss: [0.00164765]\n",
      "epoch: 893 loss: [0.00169183]\n",
      "epoch: 894 loss: [0.0017363]\n",
      "epoch: 895 loss: [0.00178106]\n",
      "epoch: 896 loss: [0.00182611]\n",
      "epoch: 897 loss: [0.00187143]\n",
      "epoch: 898 loss: [0.00191701]\n",
      "epoch: 899 loss: [0.00196285]\n",
      "epoch: 900 loss: [0.00200893]\n",
      "epoch: 901 loss: [0.00205526]\n",
      "epoch: 902 loss: [0.00210182]\n",
      "epoch: 903 loss: [0.00214859]\n",
      "epoch: 904 loss: [0.00219559]\n",
      "epoch: 905 loss: [0.00224279]\n",
      "epoch: 906 loss: [0.00229019]\n",
      "epoch: 907 loss: [0.00233778]\n",
      "epoch: 908 loss: [0.00238556]\n",
      "epoch: 909 loss: [0.00243351]\n",
      "epoch: 910 loss: [0.00248163]\n",
      "epoch: 911 loss: [0.00252992]\n",
      "epoch: 912 loss: [0.00257836]\n",
      "epoch: 913 loss: [0.00262695]\n",
      "epoch: 914 loss: [0.00267569]\n",
      "epoch: 915 loss: [0.00272456]\n",
      "epoch: 916 loss: [0.00277356]\n",
      "epoch: 917 loss: [0.00282269]\n",
      "epoch: 918 loss: [0.00287193]\n",
      "epoch: 919 loss: [0.00292128]\n",
      "epoch: 920 loss: [0.00297074]\n",
      "epoch: 921 loss: [0.0030203]\n",
      "epoch: 922 loss: [0.00306995]\n",
      "epoch: 923 loss: [0.00311969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 924 loss: [0.00316951]\n",
      "epoch: 925 loss: [0.00321941]\n",
      "epoch: 926 loss: [0.00326937]\n",
      "epoch: 927 loss: [0.00331941]\n",
      "epoch: 928 loss: [0.0033695]\n",
      "epoch: 929 loss: [0.00341965]\n",
      "epoch: 930 loss: [0.00346986]\n",
      "epoch: 931 loss: [0.0035201]\n",
      "epoch: 932 loss: [0.00357039]\n",
      "epoch: 933 loss: [0.00362071]\n",
      "epoch: 934 loss: [0.00367107]\n",
      "epoch: 935 loss: [0.00372145]\n",
      "epoch: 936 loss: [0.00377185]\n",
      "epoch: 937 loss: [0.00382228]\n",
      "epoch: 938 loss: [0.00387271]\n",
      "epoch: 939 loss: [0.00392316]\n",
      "epoch: 940 loss: [0.00397361]\n",
      "epoch: 941 loss: [0.00402406]\n",
      "epoch: 942 loss: [0.0040745]\n",
      "epoch: 943 loss: [0.00412495]\n",
      "epoch: 944 loss: [0.00417538]\n",
      "epoch: 945 loss: [0.00422579]\n",
      "epoch: 946 loss: [0.00427619]\n",
      "epoch: 947 loss: [0.00432656]\n",
      "epoch: 948 loss: [0.00437691]\n",
      "epoch: 949 loss: [0.00442723]\n",
      "epoch: 950 loss: [0.00447752]\n",
      "epoch: 951 loss: [0.00452777]\n",
      "epoch: 952 loss: [0.00457798]\n",
      "epoch: 953 loss: [0.00462815]\n",
      "epoch: 954 loss: [0.00467828]\n",
      "epoch: 955 loss: [0.00472835]\n",
      "epoch: 956 loss: [0.00477837]\n",
      "epoch: 957 loss: [0.00482834]\n",
      "epoch: 958 loss: [0.00487825]\n",
      "epoch: 959 loss: [0.0049281]\n",
      "epoch: 960 loss: [0.00497789]\n",
      "epoch: 961 loss: [0.00502761]\n",
      "epoch: 962 loss: [0.00507726]\n",
      "epoch: 963 loss: [0.00512684]\n",
      "epoch: 964 loss: [0.00517635]\n",
      "epoch: 965 loss: [0.00522577]\n",
      "epoch: 966 loss: [0.00527512]\n",
      "epoch: 967 loss: [0.00532439]\n",
      "epoch: 968 loss: [0.00537358]\n",
      "epoch: 969 loss: [0.00542267]\n",
      "epoch: 970 loss: [0.00547168]\n",
      "epoch: 971 loss: [0.0055206]\n",
      "epoch: 972 loss: [0.00556943]\n",
      "epoch: 973 loss: [0.00561816]\n",
      "epoch: 974 loss: [0.00566679]\n",
      "epoch: 975 loss: [0.00571532]\n",
      "epoch: 976 loss: [0.00576375]\n",
      "epoch: 977 loss: [0.00581208]\n",
      "epoch: 978 loss: [0.00586031]\n",
      "epoch: 979 loss: [0.00590843]\n",
      "epoch: 980 loss: [0.00595643]\n",
      "epoch: 981 loss: [0.00600433]\n",
      "epoch: 982 loss: [0.00605212]\n",
      "epoch: 983 loss: [0.00609979]\n",
      "epoch: 984 loss: [0.00614735]\n",
      "epoch: 985 loss: [0.00619479]\n",
      "epoch: 986 loss: [0.00624211]\n",
      "epoch: 987 loss: [0.00628931]\n",
      "epoch: 988 loss: [0.00633639]\n",
      "epoch: 989 loss: [0.00638334]\n",
      "epoch: 990 loss: [0.00643017]\n",
      "epoch: 991 loss: [0.00647688]\n",
      "epoch: 992 loss: [0.00652346]\n",
      "epoch: 993 loss: [0.00656991]\n",
      "epoch: 994 loss: [0.00661623]\n",
      "epoch: 995 loss: [0.00666241]\n",
      "epoch: 996 loss: [0.00670847]\n",
      "epoch: 997 loss: [0.00675439]\n",
      "epoch: 998 loss: [0.00680018]\n",
      "epoch: 999 loss: [0.00684584]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [-0.37938392] err: [0.37938392]\n",
      "[0. 1.] [0.] -> [0.22804577] err: [-0.22804577]\n",
      "[1. 0.] [0.] -> [0.23205556] err: [-0.23205556]\n",
      "[1. 1.] [1.] -> [0.83948525] err: [0.16051475]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [0.] -> 0 err: [0.]\n",
      "[1. 0.] [0.] -> 0 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 練習:「OR」を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [1], \n",
    "    [1], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [11.23987784]\n",
      "epoch: 1 loss: [9.86187663]\n",
      "epoch: 2 loss: [8.65155654]\n",
      "epoch: 3 loss: [7.58858332]\n",
      "epoch: 4 loss: [6.65508502]\n",
      "epoch: 5 loss: [5.83535398]\n",
      "epoch: 6 loss: [5.11558496]\n",
      "epoch: 7 loss: [4.48364482]\n",
      "epoch: 8 loss: [3.9288702]\n",
      "epoch: 9 loss: [3.44188958]\n",
      "epoch: 10 loss: [3.01446697]\n",
      "epoch: 11 loss: [2.63936438]\n",
      "epoch: 12 loss: [2.31022107]\n",
      "epoch: 13 loss: [2.02144734]\n",
      "epoch: 14 loss: [1.76813116]\n",
      "epoch: 15 loss: [1.54595612]\n",
      "epoch: 16 loss: [1.35112935]\n",
      "epoch: 17 loss: [1.18031808]\n",
      "epoch: 18 loss: [1.03059393]\n",
      "epoch: 19 loss: [0.89938395]\n",
      "epoch: 20 loss: [0.78442755]\n",
      "epoch: 21 loss: [0.68373866]\n",
      "epoch: 22 loss: [0.59557249]\n",
      "epoch: 23 loss: [0.51839626]\n",
      "epoch: 24 loss: [0.45086354]\n",
      "epoch: 25 loss: [0.39179164]\n",
      "epoch: 26 loss: [0.34014177]\n",
      "epoch: 27 loss: [0.29500159]\n",
      "epoch: 28 loss: [0.25556989]\n",
      "epoch: 29 loss: [0.22114307]\n",
      "epoch: 30 loss: [0.19110333]\n",
      "epoch: 31 loss: [0.16490827]\n",
      "epoch: 32 loss: [0.14208169]\n",
      "epoch: 33 loss: [0.12220559]\n",
      "epoch: 34 loss: [0.10491309]\n",
      "epoch: 35 loss: [0.0898822]\n",
      "epoch: 36 loss: [0.07683041]\n",
      "epoch: 37 loss: [0.06550982]\n",
      "epoch: 38 loss: [0.055703]\n",
      "epoch: 39 loss: [0.04721923]\n",
      "epoch: 40 loss: [0.03989129]\n",
      "epoch: 41 loss: [0.03357253]\n",
      "epoch: 42 loss: [0.02813446]\n",
      "epoch: 43 loss: [0.02346442]\n",
      "epoch: 44 loss: [0.01946376]\n",
      "epoch: 45 loss: [0.01604604]\n",
      "epoch: 46 loss: [0.01313557]\n",
      "epoch: 47 loss: [0.01066613]\n",
      "epoch: 48 loss: [0.00857974]\n",
      "epoch: 49 loss: [0.0068257]\n",
      "epoch: 50 loss: [0.00535967]\n",
      "epoch: 51 loss: [0.00414292]\n",
      "epoch: 52 loss: [0.00314159]\n",
      "epoch: 53 loss: [0.00232614]\n",
      "epoch: 54 loss: [0.00167079]\n",
      "epoch: 55 loss: [0.00115306]\n",
      "epoch: 56 loss: [0.00075334]\n",
      "epoch: 57 loss: [0.00045459]\n",
      "epoch: 58 loss: [0.00024196]\n",
      "epoch: 59 loss: [0.00010255]\n",
      "epoch: 60 loss: [2.51462984e-05]\n",
      "epoch: 61 loss: [3.98534746e-08]\n",
      "epoch: 62 loss: [1.88074999e-05]\n",
      "epoch: 63 loss: [7.41635449e-05]\n",
      "epoch: 64 loss: [0.00015981]\n",
      "epoch: 65 loss: [0.00027033]\n",
      "epoch: 66 loss: [0.00040103]\n",
      "epoch: 67 loss: [0.00054791]\n",
      "epoch: 68 loss: [0.00070752]\n",
      "epoch: 69 loss: [0.00087692]\n",
      "epoch: 70 loss: [0.00105359]\n",
      "epoch: 71 loss: [0.00123542]\n",
      "epoch: 72 loss: [0.00142058]\n",
      "epoch: 73 loss: [0.00160757]\n",
      "epoch: 74 loss: [0.00179511]\n",
      "epoch: 75 loss: [0.00198213]\n",
      "epoch: 76 loss: [0.00216777]\n",
      "epoch: 77 loss: [0.0023513]\n",
      "epoch: 78 loss: [0.00253214]\n",
      "epoch: 79 loss: [0.00270983]\n",
      "epoch: 80 loss: [0.002884]\n",
      "epoch: 81 loss: [0.00305437]\n",
      "epoch: 82 loss: [0.00322074]\n",
      "epoch: 83 loss: [0.00338296]\n",
      "epoch: 84 loss: [0.00354094]\n",
      "epoch: 85 loss: [0.00369464]\n",
      "epoch: 86 loss: [0.00384404]\n",
      "epoch: 87 loss: [0.00398917]\n",
      "epoch: 88 loss: [0.00413006]\n",
      "epoch: 89 loss: [0.00426679]\n",
      "epoch: 90 loss: [0.00439943]\n",
      "epoch: 91 loss: [0.00452809]\n",
      "epoch: 92 loss: [0.00465287]\n",
      "epoch: 93 loss: [0.00477388]\n",
      "epoch: 94 loss: [0.00489124]\n",
      "epoch: 95 loss: [0.00500508]\n",
      "epoch: 96 loss: [0.00511553]\n",
      "epoch: 97 loss: [0.0052227]\n",
      "epoch: 98 loss: [0.00532672]\n",
      "epoch: 99 loss: [0.00542773]\n",
      "epoch: 100 loss: [0.00552585]\n",
      "epoch: 101 loss: [0.00562119]\n",
      "epoch: 102 loss: [0.00571388]\n",
      "epoch: 103 loss: [0.00580403]\n",
      "epoch: 104 loss: [0.00589176]\n",
      "epoch: 105 loss: [0.00597718]\n",
      "epoch: 106 loss: [0.00606039]\n",
      "epoch: 107 loss: [0.0061415]\n",
      "epoch: 108 loss: [0.00622059]\n",
      "epoch: 109 loss: [0.00629778]\n",
      "epoch: 110 loss: [0.00637314]\n",
      "epoch: 111 loss: [0.00644677]\n",
      "epoch: 112 loss: [0.00651874]\n",
      "epoch: 113 loss: [0.00658914]\n",
      "epoch: 114 loss: [0.00665803]\n",
      "epoch: 115 loss: [0.0067255]\n",
      "epoch: 116 loss: [0.00679161]\n",
      "epoch: 117 loss: [0.00685643]\n",
      "epoch: 118 loss: [0.00692001]\n",
      "epoch: 119 loss: [0.00698242]\n",
      "epoch: 120 loss: [0.00704371]\n",
      "epoch: 121 loss: [0.00710393]\n",
      "epoch: 122 loss: [0.00716313]\n",
      "epoch: 123 loss: [0.00722137]\n",
      "epoch: 124 loss: [0.00727868]\n",
      "epoch: 125 loss: [0.00733511]\n",
      "epoch: 126 loss: [0.0073907]\n",
      "epoch: 127 loss: [0.00744549]\n",
      "epoch: 128 loss: [0.00749951]\n",
      "epoch: 129 loss: [0.0075528]\n",
      "epoch: 130 loss: [0.00760538]\n",
      "epoch: 131 loss: [0.00765729]\n",
      "epoch: 132 loss: [0.00770856]\n",
      "epoch: 133 loss: [0.00775922]\n",
      "epoch: 134 loss: [0.00780928]\n",
      "epoch: 135 loss: [0.00785878]\n",
      "epoch: 136 loss: [0.00790774]\n",
      "epoch: 137 loss: [0.00795617]\n",
      "epoch: 138 loss: [0.0080041]\n",
      "epoch: 139 loss: [0.00805155]\n",
      "epoch: 140 loss: [0.00809854]\n",
      "epoch: 141 loss: [0.00814507]\n",
      "epoch: 142 loss: [0.00819118]\n",
      "epoch: 143 loss: [0.00823687]\n",
      "epoch: 144 loss: [0.00828216]\n",
      "epoch: 145 loss: [0.00832706]\n",
      "epoch: 146 loss: [0.00837158]\n",
      "epoch: 147 loss: [0.00841574]\n",
      "epoch: 148 loss: [0.00845954]\n",
      "epoch: 149 loss: [0.008503]\n",
      "epoch: 150 loss: [0.00854613]\n",
      "epoch: 151 loss: [0.00858894]\n",
      "epoch: 152 loss: [0.00863143]\n",
      "epoch: 153 loss: [0.00867362]\n",
      "epoch: 154 loss: [0.00871551]\n",
      "epoch: 155 loss: [0.00875711]\n",
      "epoch: 156 loss: [0.00879843]\n",
      "epoch: 157 loss: [0.00883946]\n",
      "epoch: 158 loss: [0.00888023]\n",
      "epoch: 159 loss: [0.00892073]\n",
      "epoch: 160 loss: [0.00896098]\n",
      "epoch: 161 loss: [0.00900097]\n",
      "epoch: 162 loss: [0.00904071]\n",
      "epoch: 163 loss: [0.00908021]\n",
      "epoch: 164 loss: [0.00911946]\n",
      "epoch: 165 loss: [0.00915848]\n",
      "epoch: 166 loss: [0.00919727]\n",
      "epoch: 167 loss: [0.00923583]\n",
      "epoch: 168 loss: [0.00927417]\n",
      "epoch: 169 loss: [0.00931229]\n",
      "epoch: 170 loss: [0.00935019]\n",
      "epoch: 171 loss: [0.00938787]\n",
      "epoch: 172 loss: [0.00942534]\n",
      "epoch: 173 loss: [0.0094626]\n",
      "epoch: 174 loss: [0.00949966]\n",
      "epoch: 175 loss: [0.00953651]\n",
      "epoch: 176 loss: [0.00957315]\n",
      "epoch: 177 loss: [0.0096096]\n",
      "epoch: 178 loss: [0.00964585]\n",
      "epoch: 179 loss: [0.00968191]\n",
      "epoch: 180 loss: [0.00971777]\n",
      "epoch: 181 loss: [0.00975344]\n",
      "epoch: 182 loss: [0.00978892]\n",
      "epoch: 183 loss: [0.00982421]\n",
      "epoch: 184 loss: [0.00985931]\n",
      "epoch: 185 loss: [0.00989422]\n",
      "epoch: 186 loss: [0.00992896]\n",
      "epoch: 187 loss: [0.00996351]\n",
      "epoch: 188 loss: [0.00999788]\n",
      "epoch: 189 loss: [0.01003206]\n",
      "epoch: 190 loss: [0.01006607]\n",
      "epoch: 191 loss: [0.01009991]\n",
      "epoch: 192 loss: [0.01013356]\n",
      "epoch: 193 loss: [0.01016704]\n",
      "epoch: 194 loss: [0.01020034]\n",
      "epoch: 195 loss: [0.01023348]\n",
      "epoch: 196 loss: [0.01026644]\n",
      "epoch: 197 loss: [0.01029922]\n",
      "epoch: 198 loss: [0.01033184]\n",
      "epoch: 199 loss: [0.01036429]\n",
      "epoch: 200 loss: [0.01039657]\n",
      "epoch: 201 loss: [0.01042868]\n",
      "epoch: 202 loss: [0.01046062]\n",
      "epoch: 203 loss: [0.0104924]\n",
      "epoch: 204 loss: [0.01052401]\n",
      "epoch: 205 loss: [0.01055546]\n",
      "epoch: 206 loss: [0.01058675]\n",
      "epoch: 207 loss: [0.01061787]\n",
      "epoch: 208 loss: [0.01064883]\n",
      "epoch: 209 loss: [0.01067963]\n",
      "epoch: 210 loss: [0.01071026]\n",
      "epoch: 211 loss: [0.01074074]\n",
      "epoch: 212 loss: [0.01077106]\n",
      "epoch: 213 loss: [0.01080122]\n",
      "epoch: 214 loss: [0.01083122]\n",
      "epoch: 215 loss: [0.01086106]\n",
      "epoch: 216 loss: [0.01089075]\n",
      "epoch: 217 loss: [0.01092028]\n",
      "epoch: 218 loss: [0.01094966]\n",
      "epoch: 219 loss: [0.01097888]\n",
      "epoch: 220 loss: [0.01100795]\n",
      "epoch: 221 loss: [0.01103686]\n",
      "epoch: 222 loss: [0.01106562]\n",
      "epoch: 223 loss: [0.01109423]\n",
      "epoch: 224 loss: [0.01112269]\n",
      "epoch: 225 loss: [0.011151]\n",
      "epoch: 226 loss: [0.01117915]\n",
      "epoch: 227 loss: [0.01120716]\n",
      "epoch: 228 loss: [0.01123502]\n",
      "epoch: 229 loss: [0.01126273]\n",
      "epoch: 230 loss: [0.01129029]\n",
      "epoch: 231 loss: [0.0113177]\n",
      "epoch: 232 loss: [0.01134497]\n",
      "epoch: 233 loss: [0.01137209]\n",
      "epoch: 234 loss: [0.01139907]\n",
      "epoch: 235 loss: [0.0114259]\n",
      "epoch: 236 loss: [0.01145259]\n",
      "epoch: 237 loss: [0.01147914]\n",
      "epoch: 238 loss: [0.01150554]\n",
      "epoch: 239 loss: [0.0115318]\n",
      "epoch: 240 loss: [0.01155791]\n",
      "epoch: 241 loss: [0.01158389]\n",
      "epoch: 242 loss: [0.01160972]\n",
      "epoch: 243 loss: [0.01163542]\n",
      "epoch: 244 loss: [0.01166097]\n",
      "epoch: 245 loss: [0.01168639]\n",
      "epoch: 246 loss: [0.01171167]\n",
      "epoch: 247 loss: [0.01173681]\n",
      "epoch: 248 loss: [0.01176181]\n",
      "epoch: 249 loss: [0.01178668]\n",
      "epoch: 250 loss: [0.01181141]\n",
      "epoch: 251 loss: [0.01183601]\n",
      "epoch: 252 loss: [0.01186047]\n",
      "epoch: 253 loss: [0.0118848]\n",
      "epoch: 254 loss: [0.01190899]\n",
      "epoch: 255 loss: [0.01193305]\n",
      "epoch: 256 loss: [0.01195698]\n",
      "epoch: 257 loss: [0.01198077]\n",
      "epoch: 258 loss: [0.01200444]\n",
      "epoch: 259 loss: [0.01202797]\n",
      "epoch: 260 loss: [0.01205137]\n",
      "epoch: 261 loss: [0.01207465]\n",
      "epoch: 262 loss: [0.01209779]\n",
      "epoch: 263 loss: [0.01212081]\n",
      "epoch: 264 loss: [0.0121437]\n",
      "epoch: 265 loss: [0.01216646]\n",
      "epoch: 266 loss: [0.01218909]\n",
      "epoch: 267 loss: [0.0122116]\n",
      "epoch: 268 loss: [0.01223398]\n",
      "epoch: 269 loss: [0.01225624]\n",
      "epoch: 270 loss: [0.01227838]\n",
      "epoch: 271 loss: [0.01230038]\n",
      "epoch: 272 loss: [0.01232227]\n",
      "epoch: 273 loss: [0.01234403]\n",
      "epoch: 274 loss: [0.01236567]\n",
      "epoch: 275 loss: [0.01238719]\n",
      "epoch: 276 loss: [0.01240859]\n",
      "epoch: 277 loss: [0.01242987]\n",
      "epoch: 278 loss: [0.01245103]\n",
      "epoch: 279 loss: [0.01247207]\n",
      "epoch: 280 loss: [0.01249299]\n",
      "epoch: 281 loss: [0.01251379]\n",
      "epoch: 282 loss: [0.01253447]\n",
      "epoch: 283 loss: [0.01255504]\n",
      "epoch: 284 loss: [0.01257549]\n",
      "epoch: 285 loss: [0.01259582]\n",
      "epoch: 286 loss: [0.01261604]\n",
      "epoch: 287 loss: [0.01263614]\n",
      "epoch: 288 loss: [0.01265613]\n",
      "epoch: 289 loss: [0.01267601]\n",
      "epoch: 290 loss: [0.01269577]\n",
      "epoch: 291 loss: [0.01271542]\n",
      "epoch: 292 loss: [0.01273496]\n",
      "epoch: 293 loss: [0.01275438]\n",
      "epoch: 294 loss: [0.0127737]\n",
      "epoch: 295 loss: [0.0127929]\n",
      "epoch: 296 loss: [0.01281199]\n",
      "epoch: 297 loss: [0.01283098]\n",
      "epoch: 298 loss: [0.01284985]\n",
      "epoch: 299 loss: [0.01286862]\n",
      "epoch: 300 loss: [0.01288728]\n",
      "epoch: 301 loss: [0.01290583]\n",
      "epoch: 302 loss: [0.01292428]\n",
      "epoch: 303 loss: [0.01294261]\n",
      "epoch: 304 loss: [0.01296085]\n",
      "epoch: 305 loss: [0.01297898]\n",
      "epoch: 306 loss: [0.012997]\n",
      "epoch: 307 loss: [0.01301492]\n",
      "epoch: 308 loss: [0.01303273]\n",
      "epoch: 309 loss: [0.01305044]\n",
      "epoch: 310 loss: [0.01306805]\n",
      "epoch: 311 loss: [0.01308556]\n",
      "epoch: 312 loss: [0.01310297]\n",
      "epoch: 313 loss: [0.01312027]\n",
      "epoch: 314 loss: [0.01313748]\n",
      "epoch: 315 loss: [0.01315458]\n",
      "epoch: 316 loss: [0.01317159]\n",
      "epoch: 317 loss: [0.01318849]\n",
      "epoch: 318 loss: [0.0132053]\n",
      "epoch: 319 loss: [0.01322201]\n",
      "epoch: 320 loss: [0.01323862]\n",
      "epoch: 321 loss: [0.01325514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 322 loss: [0.01327155]\n",
      "epoch: 323 loss: [0.01328788]\n",
      "epoch: 324 loss: [0.0133041]\n",
      "epoch: 325 loss: [0.01332024]\n",
      "epoch: 326 loss: [0.01333627]\n",
      "epoch: 327 loss: [0.01335222]\n",
      "epoch: 328 loss: [0.01336807]\n",
      "epoch: 329 loss: [0.01338382]\n",
      "epoch: 330 loss: [0.01339949]\n",
      "epoch: 331 loss: [0.01341506]\n",
      "epoch: 332 loss: [0.01343054]\n",
      "epoch: 333 loss: [0.01344593]\n",
      "epoch: 334 loss: [0.01346123]\n",
      "epoch: 335 loss: [0.01347644]\n",
      "epoch: 336 loss: [0.01349156]\n",
      "epoch: 337 loss: [0.01350659]\n",
      "epoch: 338 loss: [0.01352154]\n",
      "epoch: 339 loss: [0.01353639]\n",
      "epoch: 340 loss: [0.01355116]\n",
      "epoch: 341 loss: [0.01356584]\n",
      "epoch: 342 loss: [0.01358043]\n",
      "epoch: 343 loss: [0.01359493]\n",
      "epoch: 344 loss: [0.01360935]\n",
      "epoch: 345 loss: [0.01362369]\n",
      "epoch: 346 loss: [0.01363794]\n",
      "epoch: 347 loss: [0.0136521]\n",
      "epoch: 348 loss: [0.01366619]\n",
      "epoch: 349 loss: [0.01368018]\n",
      "epoch: 350 loss: [0.0136941]\n",
      "epoch: 351 loss: [0.01370793]\n",
      "epoch: 352 loss: [0.01372168]\n",
      "epoch: 353 loss: [0.01373535]\n",
      "epoch: 354 loss: [0.01374894]\n",
      "epoch: 355 loss: [0.01376244]\n",
      "epoch: 356 loss: [0.01377587]\n",
      "epoch: 357 loss: [0.01378921]\n",
      "epoch: 358 loss: [0.01380248]\n",
      "epoch: 359 loss: [0.01381567]\n",
      "epoch: 360 loss: [0.01382877]\n",
      "epoch: 361 loss: [0.0138418]\n",
      "epoch: 362 loss: [0.01385476]\n",
      "epoch: 363 loss: [0.01386763]\n",
      "epoch: 364 loss: [0.01388043]\n",
      "epoch: 365 loss: [0.01389315]\n",
      "epoch: 366 loss: [0.0139058]\n",
      "epoch: 367 loss: [0.01391837]\n",
      "epoch: 368 loss: [0.01393086]\n",
      "epoch: 369 loss: [0.01394328]\n",
      "epoch: 370 loss: [0.01395562]\n",
      "epoch: 371 loss: [0.0139679]\n",
      "epoch: 372 loss: [0.01398009]\n",
      "epoch: 373 loss: [0.01399222]\n",
      "epoch: 374 loss: [0.01400427]\n",
      "epoch: 375 loss: [0.01401625]\n",
      "epoch: 376 loss: [0.01402816]\n",
      "epoch: 377 loss: [0.01403999]\n",
      "epoch: 378 loss: [0.01405176]\n",
      "epoch: 379 loss: [0.01406345]\n",
      "epoch: 380 loss: [0.01407507]\n",
      "epoch: 381 loss: [0.01408663]\n",
      "epoch: 382 loss: [0.01409811]\n",
      "epoch: 383 loss: [0.01410952]\n",
      "epoch: 384 loss: [0.01412087]\n",
      "epoch: 385 loss: [0.01413215]\n",
      "epoch: 386 loss: [0.01414336]\n",
      "epoch: 387 loss: [0.0141545]\n",
      "epoch: 388 loss: [0.01416557]\n",
      "epoch: 389 loss: [0.01417658]\n",
      "epoch: 390 loss: [0.01418752]\n",
      "epoch: 391 loss: [0.0141984]\n",
      "epoch: 392 loss: [0.01420921]\n",
      "epoch: 393 loss: [0.01421995]\n",
      "epoch: 394 loss: [0.01423063]\n",
      "epoch: 395 loss: [0.01424124]\n",
      "epoch: 396 loss: [0.01425179]\n",
      "epoch: 397 loss: [0.01426228]\n",
      "epoch: 398 loss: [0.0142727]\n",
      "epoch: 399 loss: [0.01428306]\n",
      "epoch: 400 loss: [0.01429336]\n",
      "epoch: 401 loss: [0.01430359]\n",
      "epoch: 402 loss: [0.01431377]\n",
      "epoch: 403 loss: [0.01432388]\n",
      "epoch: 404 loss: [0.01433392]\n",
      "epoch: 405 loss: [0.01434391]\n",
      "epoch: 406 loss: [0.01435384]\n",
      "epoch: 407 loss: [0.01436371]\n",
      "epoch: 408 loss: [0.01437351]\n",
      "epoch: 409 loss: [0.01438326]\n",
      "epoch: 410 loss: [0.01439295]\n",
      "epoch: 411 loss: [0.01440258]\n",
      "epoch: 412 loss: [0.01441215]\n",
      "epoch: 413 loss: [0.01442166]\n",
      "epoch: 414 loss: [0.01443112]\n",
      "epoch: 415 loss: [0.01444051]\n",
      "epoch: 416 loss: [0.01444985]\n",
      "epoch: 417 loss: [0.01445914]\n",
      "epoch: 418 loss: [0.01446836]\n",
      "epoch: 419 loss: [0.01447753]\n",
      "epoch: 420 loss: [0.01448665]\n",
      "epoch: 421 loss: [0.01449571]\n",
      "epoch: 422 loss: [0.01450471]\n",
      "epoch: 423 loss: [0.01451366]\n",
      "epoch: 424 loss: [0.01452255]\n",
      "epoch: 425 loss: [0.01453139]\n",
      "epoch: 426 loss: [0.01454017]\n",
      "epoch: 427 loss: [0.01454891]\n",
      "epoch: 428 loss: [0.01455758]\n",
      "epoch: 429 loss: [0.01456621]\n",
      "epoch: 430 loss: [0.01457478]\n",
      "epoch: 431 loss: [0.0145833]\n",
      "epoch: 432 loss: [0.01459177]\n",
      "epoch: 433 loss: [0.01460019]\n",
      "epoch: 434 loss: [0.01460855]\n",
      "epoch: 435 loss: [0.01461686]\n",
      "epoch: 436 loss: [0.01462513]\n",
      "epoch: 437 loss: [0.01463334]\n",
      "epoch: 438 loss: [0.0146415]\n",
      "epoch: 439 loss: [0.01464961]\n",
      "epoch: 440 loss: [0.01465767]\n",
      "epoch: 441 loss: [0.01466568]\n",
      "epoch: 442 loss: [0.01467365]\n",
      "epoch: 443 loss: [0.01468156]\n",
      "epoch: 444 loss: [0.01468943]\n",
      "epoch: 445 loss: [0.01469724]\n",
      "epoch: 446 loss: [0.01470501]\n",
      "epoch: 447 loss: [0.01471273]\n",
      "epoch: 448 loss: [0.01472041]\n",
      "epoch: 449 loss: [0.01472803]\n",
      "epoch: 450 loss: [0.01473561]\n",
      "epoch: 451 loss: [0.01474315]\n",
      "epoch: 452 loss: [0.01475064]\n",
      "epoch: 453 loss: [0.01475808]\n",
      "epoch: 454 loss: [0.01476547]\n",
      "epoch: 455 loss: [0.01477282]\n",
      "epoch: 456 loss: [0.01478013]\n",
      "epoch: 457 loss: [0.01478739]\n",
      "epoch: 458 loss: [0.0147946]\n",
      "epoch: 459 loss: [0.01480177]\n",
      "epoch: 460 loss: [0.0148089]\n",
      "epoch: 461 loss: [0.01481598]\n",
      "epoch: 462 loss: [0.01482302]\n",
      "epoch: 463 loss: [0.01483001]\n",
      "epoch: 464 loss: [0.01483697]\n",
      "epoch: 465 loss: [0.01484388]\n",
      "epoch: 466 loss: [0.01485074]\n",
      "epoch: 467 loss: [0.01485757]\n",
      "epoch: 468 loss: [0.01486435]\n",
      "epoch: 469 loss: [0.01487109]\n",
      "epoch: 470 loss: [0.01487779]\n",
      "epoch: 471 loss: [0.01488445]\n",
      "epoch: 472 loss: [0.01489106]\n",
      "epoch: 473 loss: [0.01489764]\n",
      "epoch: 474 loss: [0.01490417]\n",
      "epoch: 475 loss: [0.01491067]\n",
      "epoch: 476 loss: [0.01491712]\n",
      "epoch: 477 loss: [0.01492354]\n",
      "epoch: 478 loss: [0.01492991]\n",
      "epoch: 479 loss: [0.01493625]\n",
      "epoch: 480 loss: [0.01494255]\n",
      "epoch: 481 loss: [0.0149488]\n",
      "epoch: 482 loss: [0.01495502]\n",
      "epoch: 483 loss: [0.0149612]\n",
      "epoch: 484 loss: [0.01496734]\n",
      "epoch: 485 loss: [0.01497345]\n",
      "epoch: 486 loss: [0.01497951]\n",
      "epoch: 487 loss: [0.01498554]\n",
      "epoch: 488 loss: [0.01499153]\n",
      "epoch: 489 loss: [0.01499749]\n",
      "epoch: 490 loss: [0.01500341]\n",
      "epoch: 491 loss: [0.01500929]\n",
      "epoch: 492 loss: [0.01501513]\n",
      "epoch: 493 loss: [0.01502094]\n",
      "epoch: 494 loss: [0.01502671]\n",
      "epoch: 495 loss: [0.01503245]\n",
      "epoch: 496 loss: [0.01503815]\n",
      "epoch: 497 loss: [0.01504382]\n",
      "epoch: 498 loss: [0.01504945]\n",
      "epoch: 499 loss: [0.01505504]\n",
      "epoch: 500 loss: [0.0150606]\n",
      "epoch: 501 loss: [0.01506613]\n",
      "epoch: 502 loss: [0.01507162]\n",
      "epoch: 503 loss: [0.01507708]\n",
      "epoch: 504 loss: [0.0150825]\n",
      "epoch: 505 loss: [0.01508789]\n",
      "epoch: 506 loss: [0.01509325]\n",
      "epoch: 507 loss: [0.01509857]\n",
      "epoch: 508 loss: [0.01510386]\n",
      "epoch: 509 loss: [0.01510912]\n",
      "epoch: 510 loss: [0.01511435]\n",
      "epoch: 511 loss: [0.01511954]\n",
      "epoch: 512 loss: [0.0151247]\n",
      "epoch: 513 loss: [0.01512983]\n",
      "epoch: 514 loss: [0.01513492]\n",
      "epoch: 515 loss: [0.01513999]\n",
      "epoch: 516 loss: [0.01514502]\n",
      "epoch: 517 loss: [0.01515002]\n",
      "epoch: 518 loss: [0.01515499]\n",
      "epoch: 519 loss: [0.01515993]\n",
      "epoch: 520 loss: [0.01516484]\n",
      "epoch: 521 loss: [0.01516972]\n",
      "epoch: 522 loss: [0.01517457]\n",
      "epoch: 523 loss: [0.01517939]\n",
      "epoch: 524 loss: [0.01518418]\n",
      "epoch: 525 loss: [0.01518894]\n",
      "epoch: 526 loss: [0.01519366]\n",
      "epoch: 527 loss: [0.01519836]\n",
      "epoch: 528 loss: [0.01520303]\n",
      "epoch: 529 loss: [0.01520767]\n",
      "epoch: 530 loss: [0.01521229]\n",
      "epoch: 531 loss: [0.01521687]\n",
      "epoch: 532 loss: [0.01522142]\n",
      "epoch: 533 loss: [0.01522595]\n",
      "epoch: 534 loss: [0.01523045]\n",
      "epoch: 535 loss: [0.01523492]\n",
      "epoch: 536 loss: [0.01523936]\n",
      "epoch: 537 loss: [0.01524378]\n",
      "epoch: 538 loss: [0.01524816]\n",
      "epoch: 539 loss: [0.01525252]\n",
      "epoch: 540 loss: [0.01525686]\n",
      "epoch: 541 loss: [0.01526116]\n",
      "epoch: 542 loss: [0.01526544]\n",
      "epoch: 543 loss: [0.01526969]\n",
      "epoch: 544 loss: [0.01527392]\n",
      "epoch: 545 loss: [0.01527812]\n",
      "epoch: 546 loss: [0.01528229]\n",
      "epoch: 547 loss: [0.01528644]\n",
      "epoch: 548 loss: [0.01529056]\n",
      "epoch: 549 loss: [0.01529465]\n",
      "epoch: 550 loss: [0.01529872]\n",
      "epoch: 551 loss: [0.01530277]\n",
      "epoch: 552 loss: [0.01530679]\n",
      "epoch: 553 loss: [0.01531078]\n",
      "epoch: 554 loss: [0.01531475]\n",
      "epoch: 555 loss: [0.01531869]\n",
      "epoch: 556 loss: [0.01532261]\n",
      "epoch: 557 loss: [0.01532651]\n",
      "epoch: 558 loss: [0.01533038]\n",
      "epoch: 559 loss: [0.01533423]\n",
      "epoch: 560 loss: [0.01533805]\n",
      "epoch: 561 loss: [0.01534185]\n",
      "epoch: 562 loss: [0.01534562]\n",
      "epoch: 563 loss: [0.01534938]\n",
      "epoch: 564 loss: [0.0153531]\n",
      "epoch: 565 loss: [0.01535681]\n",
      "epoch: 566 loss: [0.01536049]\n",
      "epoch: 567 loss: [0.01536415]\n",
      "epoch: 568 loss: [0.01536778]\n",
      "epoch: 569 loss: [0.0153714]\n",
      "epoch: 570 loss: [0.01537499]\n",
      "epoch: 571 loss: [0.01537856]\n",
      "epoch: 572 loss: [0.0153821]\n",
      "epoch: 573 loss: [0.01538562]\n",
      "epoch: 574 loss: [0.01538913]\n",
      "epoch: 575 loss: [0.0153926]\n",
      "epoch: 576 loss: [0.01539606]\n",
      "epoch: 577 loss: [0.0153995]\n",
      "epoch: 578 loss: [0.01540291]\n",
      "epoch: 579 loss: [0.01540631]\n",
      "epoch: 580 loss: [0.01540968]\n",
      "epoch: 581 loss: [0.01541303]\n",
      "epoch: 582 loss: [0.01541636]\n",
      "epoch: 583 loss: [0.01541967]\n",
      "epoch: 584 loss: [0.01542296]\n",
      "epoch: 585 loss: [0.01542622]\n",
      "epoch: 586 loss: [0.01542947]\n",
      "epoch: 587 loss: [0.0154327]\n",
      "epoch: 588 loss: [0.01543591]\n",
      "epoch: 589 loss: [0.01543909]\n",
      "epoch: 590 loss: [0.01544226]\n",
      "epoch: 591 loss: [0.01544541]\n",
      "epoch: 592 loss: [0.01544853]\n",
      "epoch: 593 loss: [0.01545164]\n",
      "epoch: 594 loss: [0.01545473]\n",
      "epoch: 595 loss: [0.0154578]\n",
      "epoch: 596 loss: [0.01546085]\n",
      "epoch: 597 loss: [0.01546388]\n",
      "epoch: 598 loss: [0.01546689]\n",
      "epoch: 599 loss: [0.01546988]\n",
      "epoch: 600 loss: [0.01547286]\n",
      "epoch: 601 loss: [0.01547581]\n",
      "epoch: 602 loss: [0.01547875]\n",
      "epoch: 603 loss: [0.01548167]\n",
      "epoch: 604 loss: [0.01548457]\n",
      "epoch: 605 loss: [0.01548745]\n",
      "epoch: 606 loss: [0.01549031]\n",
      "epoch: 607 loss: [0.01549316]\n",
      "epoch: 608 loss: [0.01549599]\n",
      "epoch: 609 loss: [0.0154988]\n",
      "epoch: 610 loss: [0.01550159]\n",
      "epoch: 611 loss: [0.01550436]\n",
      "epoch: 612 loss: [0.01550712]\n",
      "epoch: 613 loss: [0.01550986]\n",
      "epoch: 614 loss: [0.01551258]\n",
      "epoch: 615 loss: [0.01551529]\n",
      "epoch: 616 loss: [0.01551798]\n",
      "epoch: 617 loss: [0.01552065]\n",
      "epoch: 618 loss: [0.01552331]\n",
      "epoch: 619 loss: [0.01552595]\n",
      "epoch: 620 loss: [0.01552857]\n",
      "epoch: 621 loss: [0.01553117]\n",
      "epoch: 622 loss: [0.01553376]\n",
      "epoch: 623 loss: [0.01553634]\n",
      "epoch: 624 loss: [0.01553889]\n",
      "epoch: 625 loss: [0.01554143]\n",
      "epoch: 626 loss: [0.01554396]\n",
      "epoch: 627 loss: [0.01554647]\n",
      "epoch: 628 loss: [0.01554896]\n",
      "epoch: 629 loss: [0.01555144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 630 loss: [0.0155539]\n",
      "epoch: 631 loss: [0.01555635]\n",
      "epoch: 632 loss: [0.01555878]\n",
      "epoch: 633 loss: [0.0155612]\n",
      "epoch: 634 loss: [0.0155636]\n",
      "epoch: 635 loss: [0.01556598]\n",
      "epoch: 636 loss: [0.01556835]\n",
      "epoch: 637 loss: [0.01557071]\n",
      "epoch: 638 loss: [0.01557305]\n",
      "epoch: 639 loss: [0.01557538]\n",
      "epoch: 640 loss: [0.01557769]\n",
      "epoch: 641 loss: [0.01557999]\n",
      "epoch: 642 loss: [0.01558227]\n",
      "epoch: 643 loss: [0.01558454]\n",
      "epoch: 644 loss: [0.01558679]\n",
      "epoch: 645 loss: [0.01558903]\n",
      "epoch: 646 loss: [0.01559126]\n",
      "epoch: 647 loss: [0.01559347]\n",
      "epoch: 648 loss: [0.01559567]\n",
      "epoch: 649 loss: [0.01559785]\n",
      "epoch: 650 loss: [0.01560003]\n",
      "epoch: 651 loss: [0.01560218]\n",
      "epoch: 652 loss: [0.01560433]\n",
      "epoch: 653 loss: [0.01560646]\n",
      "epoch: 654 loss: [0.01560857]\n",
      "epoch: 655 loss: [0.01561068]\n",
      "epoch: 656 loss: [0.01561277]\n",
      "epoch: 657 loss: [0.01561484]\n",
      "epoch: 658 loss: [0.01561691]\n",
      "epoch: 659 loss: [0.01561896]\n",
      "epoch: 660 loss: [0.015621]\n",
      "epoch: 661 loss: [0.01562302]\n",
      "epoch: 662 loss: [0.01562503]\n",
      "epoch: 663 loss: [0.01562703]\n",
      "epoch: 664 loss: [0.01562902]\n",
      "epoch: 665 loss: [0.015631]\n",
      "epoch: 666 loss: [0.01563296]\n",
      "epoch: 667 loss: [0.01563491]\n",
      "epoch: 668 loss: [0.01563685]\n",
      "epoch: 669 loss: [0.01563877]\n",
      "epoch: 670 loss: [0.01564068]\n",
      "epoch: 671 loss: [0.01564259]\n",
      "epoch: 672 loss: [0.01564448]\n",
      "epoch: 673 loss: [0.01564635]\n",
      "epoch: 674 loss: [0.01564822]\n",
      "epoch: 675 loss: [0.01565007]\n",
      "epoch: 676 loss: [0.01565192]\n",
      "epoch: 677 loss: [0.01565375]\n",
      "epoch: 678 loss: [0.01565557]\n",
      "epoch: 679 loss: [0.01565737]\n",
      "epoch: 680 loss: [0.01565917]\n",
      "epoch: 681 loss: [0.01566095]\n",
      "epoch: 682 loss: [0.01566273]\n",
      "epoch: 683 loss: [0.01566449]\n",
      "epoch: 684 loss: [0.01566624]\n",
      "epoch: 685 loss: [0.01566798]\n",
      "epoch: 686 loss: [0.01566971]\n",
      "epoch: 687 loss: [0.01567143]\n",
      "epoch: 688 loss: [0.01567314]\n",
      "epoch: 689 loss: [0.01567484]\n",
      "epoch: 690 loss: [0.01567652]\n",
      "epoch: 691 loss: [0.0156782]\n",
      "epoch: 692 loss: [0.01567987]\n",
      "epoch: 693 loss: [0.01568152]\n",
      "epoch: 694 loss: [0.01568316]\n",
      "epoch: 695 loss: [0.0156848]\n",
      "epoch: 696 loss: [0.01568642]\n",
      "epoch: 697 loss: [0.01568804]\n",
      "epoch: 698 loss: [0.01568964]\n",
      "epoch: 699 loss: [0.01569123]\n",
      "epoch: 700 loss: [0.01569282]\n",
      "epoch: 701 loss: [0.01569439]\n",
      "epoch: 702 loss: [0.01569595]\n",
      "epoch: 703 loss: [0.01569751]\n",
      "epoch: 704 loss: [0.01569905]\n",
      "epoch: 705 loss: [0.01570058]\n",
      "epoch: 706 loss: [0.01570211]\n",
      "epoch: 707 loss: [0.01570362]\n",
      "epoch: 708 loss: [0.01570513]\n",
      "epoch: 709 loss: [0.01570662]\n",
      "epoch: 710 loss: [0.01570811]\n",
      "epoch: 711 loss: [0.01570959]\n",
      "epoch: 712 loss: [0.01571106]\n",
      "epoch: 713 loss: [0.01571251]\n",
      "epoch: 714 loss: [0.01571396]\n",
      "epoch: 715 loss: [0.0157154]\n",
      "epoch: 716 loss: [0.01571683]\n",
      "epoch: 717 loss: [0.01571826]\n",
      "epoch: 718 loss: [0.01571967]\n",
      "epoch: 719 loss: [0.01572107]\n",
      "epoch: 720 loss: [0.01572247]\n",
      "epoch: 721 loss: [0.01572386]\n",
      "epoch: 722 loss: [0.01572523]\n",
      "epoch: 723 loss: [0.0157266]\n",
      "epoch: 724 loss: [0.01572796]\n",
      "epoch: 725 loss: [0.01572932]\n",
      "epoch: 726 loss: [0.01573066]\n",
      "epoch: 727 loss: [0.01573199]\n",
      "epoch: 728 loss: [0.01573332]\n",
      "epoch: 729 loss: [0.01573464]\n",
      "epoch: 730 loss: [0.01573595]\n",
      "epoch: 731 loss: [0.01573725]\n",
      "epoch: 732 loss: [0.01573854]\n",
      "epoch: 733 loss: [0.01573983]\n",
      "epoch: 734 loss: [0.01574111]\n",
      "epoch: 735 loss: [0.01574238]\n",
      "epoch: 736 loss: [0.01574364]\n",
      "epoch: 737 loss: [0.01574489]\n",
      "epoch: 738 loss: [0.01574614]\n",
      "epoch: 739 loss: [0.01574737]\n",
      "epoch: 740 loss: [0.0157486]\n",
      "epoch: 741 loss: [0.01574982]\n",
      "epoch: 742 loss: [0.01575104]\n",
      "epoch: 743 loss: [0.01575225]\n",
      "epoch: 744 loss: [0.01575344]\n",
      "epoch: 745 loss: [0.01575464]\n",
      "epoch: 746 loss: [0.01575582]\n",
      "epoch: 747 loss: [0.015757]\n",
      "epoch: 748 loss: [0.01575817]\n",
      "epoch: 749 loss: [0.01575933]\n",
      "epoch: 750 loss: [0.01576048]\n",
      "epoch: 751 loss: [0.01576163]\n",
      "epoch: 752 loss: [0.01576277]\n",
      "epoch: 753 loss: [0.0157639]\n",
      "epoch: 754 loss: [0.01576503]\n",
      "epoch: 755 loss: [0.01576614]\n",
      "epoch: 756 loss: [0.01576726]\n",
      "epoch: 757 loss: [0.01576836]\n",
      "epoch: 758 loss: [0.01576946]\n",
      "epoch: 759 loss: [0.01577055]\n",
      "epoch: 760 loss: [0.01577163]\n",
      "epoch: 761 loss: [0.01577271]\n",
      "epoch: 762 loss: [0.01577378]\n",
      "epoch: 763 loss: [0.01577484]\n",
      "epoch: 764 loss: [0.0157759]\n",
      "epoch: 765 loss: [0.01577695]\n",
      "epoch: 766 loss: [0.01577799]\n",
      "epoch: 767 loss: [0.01577903]\n",
      "epoch: 768 loss: [0.01578006]\n",
      "epoch: 769 loss: [0.01578108]\n",
      "epoch: 770 loss: [0.0157821]\n",
      "epoch: 771 loss: [0.01578311]\n",
      "epoch: 772 loss: [0.01578411]\n",
      "epoch: 773 loss: [0.01578511]\n",
      "epoch: 774 loss: [0.0157861]\n",
      "epoch: 775 loss: [0.01578709]\n",
      "epoch: 776 loss: [0.01578807]\n",
      "epoch: 777 loss: [0.01578904]\n",
      "epoch: 778 loss: [0.01579001]\n",
      "epoch: 779 loss: [0.01579097]\n",
      "epoch: 780 loss: [0.01579192]\n",
      "epoch: 781 loss: [0.01579287]\n",
      "epoch: 782 loss: [0.01579382]\n",
      "epoch: 783 loss: [0.01579475]\n",
      "epoch: 784 loss: [0.01579568]\n",
      "epoch: 785 loss: [0.01579661]\n",
      "epoch: 786 loss: [0.01579753]\n",
      "epoch: 787 loss: [0.01579844]\n",
      "epoch: 788 loss: [0.01579935]\n",
      "epoch: 789 loss: [0.01580025]\n",
      "epoch: 790 loss: [0.01580115]\n",
      "epoch: 791 loss: [0.01580204]\n",
      "epoch: 792 loss: [0.01580292]\n",
      "epoch: 793 loss: [0.0158038]\n",
      "epoch: 794 loss: [0.01580468]\n",
      "epoch: 795 loss: [0.01580555]\n",
      "epoch: 796 loss: [0.01580641]\n",
      "epoch: 797 loss: [0.01580727]\n",
      "epoch: 798 loss: [0.01580812]\n",
      "epoch: 799 loss: [0.01580896]\n",
      "epoch: 800 loss: [0.01580981]\n",
      "epoch: 801 loss: [0.01581064]\n",
      "epoch: 802 loss: [0.01581147]\n",
      "epoch: 803 loss: [0.0158123]\n",
      "epoch: 804 loss: [0.01581312]\n",
      "epoch: 805 loss: [0.01581393]\n",
      "epoch: 806 loss: [0.01581474]\n",
      "epoch: 807 loss: [0.01581555]\n",
      "epoch: 808 loss: [0.01581635]\n",
      "epoch: 809 loss: [0.01581714]\n",
      "epoch: 810 loss: [0.01581793]\n",
      "epoch: 811 loss: [0.01581872]\n",
      "epoch: 812 loss: [0.0158195]\n",
      "epoch: 813 loss: [0.01582027]\n",
      "epoch: 814 loss: [0.01582104]\n",
      "epoch: 815 loss: [0.01582181]\n",
      "epoch: 816 loss: [0.01582257]\n",
      "epoch: 817 loss: [0.01582332]\n",
      "epoch: 818 loss: [0.01582407]\n",
      "epoch: 819 loss: [0.01582482]\n",
      "epoch: 820 loss: [0.01582556]\n",
      "epoch: 821 loss: [0.0158263]\n",
      "epoch: 822 loss: [0.01582703]\n",
      "epoch: 823 loss: [0.01582776]\n",
      "epoch: 824 loss: [0.01582848]\n",
      "epoch: 825 loss: [0.0158292]\n",
      "epoch: 826 loss: [0.01582991]\n",
      "epoch: 827 loss: [0.01583062]\n",
      "epoch: 828 loss: [0.01583133]\n",
      "epoch: 829 loss: [0.01583203]\n",
      "epoch: 830 loss: [0.01583272]\n",
      "epoch: 831 loss: [0.01583341]\n",
      "epoch: 832 loss: [0.0158341]\n",
      "epoch: 833 loss: [0.01583478]\n",
      "epoch: 834 loss: [0.01583546]\n",
      "epoch: 835 loss: [0.01583613]\n",
      "epoch: 836 loss: [0.0158368]\n",
      "epoch: 837 loss: [0.01583747]\n",
      "epoch: 838 loss: [0.01583813]\n",
      "epoch: 839 loss: [0.01583879]\n",
      "epoch: 840 loss: [0.01583944]\n",
      "epoch: 841 loss: [0.01584009]\n",
      "epoch: 842 loss: [0.01584074]\n",
      "epoch: 843 loss: [0.01584138]\n",
      "epoch: 844 loss: [0.01584201]\n",
      "epoch: 845 loss: [0.01584265]\n",
      "epoch: 846 loss: [0.01584328]\n",
      "epoch: 847 loss: [0.0158439]\n",
      "epoch: 848 loss: [0.01584452]\n",
      "epoch: 849 loss: [0.01584514]\n",
      "epoch: 850 loss: [0.01584575]\n",
      "epoch: 851 loss: [0.01584636]\n",
      "epoch: 852 loss: [0.01584697]\n",
      "epoch: 853 loss: [0.01584757]\n",
      "epoch: 854 loss: [0.01584817]\n",
      "epoch: 855 loss: [0.01584876]\n",
      "epoch: 856 loss: [0.01584935]\n",
      "epoch: 857 loss: [0.01584994]\n",
      "epoch: 858 loss: [0.01585052]\n",
      "epoch: 859 loss: [0.0158511]\n",
      "epoch: 860 loss: [0.01585167]\n",
      "epoch: 861 loss: [0.01585225]\n",
      "epoch: 862 loss: [0.01585281]\n",
      "epoch: 863 loss: [0.01585338]\n",
      "epoch: 864 loss: [0.01585394]\n",
      "epoch: 865 loss: [0.0158545]\n",
      "epoch: 866 loss: [0.01585505]\n",
      "epoch: 867 loss: [0.0158556]\n",
      "epoch: 868 loss: [0.01585615]\n",
      "epoch: 869 loss: [0.01585669]\n",
      "epoch: 870 loss: [0.01585723]\n",
      "epoch: 871 loss: [0.01585777]\n",
      "epoch: 872 loss: [0.0158583]\n",
      "epoch: 873 loss: [0.01585883]\n",
      "epoch: 874 loss: [0.01585936]\n",
      "epoch: 875 loss: [0.01585988]\n",
      "epoch: 876 loss: [0.0158604]\n",
      "epoch: 877 loss: [0.01586092]\n",
      "epoch: 878 loss: [0.01586143]\n",
      "epoch: 879 loss: [0.01586194]\n",
      "epoch: 880 loss: [0.01586245]\n",
      "epoch: 881 loss: [0.01586295]\n",
      "epoch: 882 loss: [0.01586345]\n",
      "epoch: 883 loss: [0.01586395]\n",
      "epoch: 884 loss: [0.01586445]\n",
      "epoch: 885 loss: [0.01586494]\n",
      "epoch: 886 loss: [0.01586542]\n",
      "epoch: 887 loss: [0.01586591]\n",
      "epoch: 888 loss: [0.01586639]\n",
      "epoch: 889 loss: [0.01586687]\n",
      "epoch: 890 loss: [0.01586735]\n",
      "epoch: 891 loss: [0.01586782]\n",
      "epoch: 892 loss: [0.01586829]\n",
      "epoch: 893 loss: [0.01586876]\n",
      "epoch: 894 loss: [0.01586922]\n",
      "epoch: 895 loss: [0.01586968]\n",
      "epoch: 896 loss: [0.01587014]\n",
      "epoch: 897 loss: [0.0158706]\n",
      "epoch: 898 loss: [0.01587105]\n",
      "epoch: 899 loss: [0.0158715]\n",
      "epoch: 900 loss: [0.01587194]\n",
      "epoch: 901 loss: [0.01587239]\n",
      "epoch: 902 loss: [0.01587283]\n",
      "epoch: 903 loss: [0.01587327]\n",
      "epoch: 904 loss: [0.0158737]\n",
      "epoch: 905 loss: [0.01587414]\n",
      "epoch: 906 loss: [0.01587457]\n",
      "epoch: 907 loss: [0.01587499]\n",
      "epoch: 908 loss: [0.01587542]\n",
      "epoch: 909 loss: [0.01587584]\n",
      "epoch: 910 loss: [0.01587626]\n",
      "epoch: 911 loss: [0.01587668]\n",
      "epoch: 912 loss: [0.01587709]\n",
      "epoch: 913 loss: [0.0158775]\n",
      "epoch: 914 loss: [0.01587791]\n",
      "epoch: 915 loss: [0.01587832]\n",
      "epoch: 916 loss: [0.01587872]\n",
      "epoch: 917 loss: [0.01587912]\n",
      "epoch: 918 loss: [0.01587952]\n",
      "epoch: 919 loss: [0.01587991]\n",
      "epoch: 920 loss: [0.01588031]\n",
      "epoch: 921 loss: [0.0158807]\n",
      "epoch: 922 loss: [0.01588109]\n",
      "epoch: 923 loss: [0.01588147]\n",
      "epoch: 924 loss: [0.01588186]\n",
      "epoch: 925 loss: [0.01588224]\n",
      "epoch: 926 loss: [0.01588262]\n",
      "epoch: 927 loss: [0.01588299]\n",
      "epoch: 928 loss: [0.01588337]\n",
      "epoch: 929 loss: [0.01588374]\n",
      "epoch: 930 loss: [0.01588411]\n",
      "epoch: 931 loss: [0.01588448]\n",
      "epoch: 932 loss: [0.01588484]\n",
      "epoch: 933 loss: [0.0158852]\n",
      "epoch: 934 loss: [0.01588556]\n",
      "epoch: 935 loss: [0.01588592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 936 loss: [0.01588628]\n",
      "epoch: 937 loss: [0.01588663]\n",
      "epoch: 938 loss: [0.01588698]\n",
      "epoch: 939 loss: [0.01588733]\n",
      "epoch: 940 loss: [0.01588768]\n",
      "epoch: 941 loss: [0.01588802]\n",
      "epoch: 942 loss: [0.01588836]\n",
      "epoch: 943 loss: [0.01588871]\n",
      "epoch: 944 loss: [0.01588904]\n",
      "epoch: 945 loss: [0.01588938]\n",
      "epoch: 946 loss: [0.01588971]\n",
      "epoch: 947 loss: [0.01589004]\n",
      "epoch: 948 loss: [0.01589037]\n",
      "epoch: 949 loss: [0.0158907]\n",
      "epoch: 950 loss: [0.01589103]\n",
      "epoch: 951 loss: [0.01589135]\n",
      "epoch: 952 loss: [0.01589167]\n",
      "epoch: 953 loss: [0.01589199]\n",
      "epoch: 954 loss: [0.01589231]\n",
      "epoch: 955 loss: [0.01589262]\n",
      "epoch: 956 loss: [0.01589294]\n",
      "epoch: 957 loss: [0.01589325]\n",
      "epoch: 958 loss: [0.01589356]\n",
      "epoch: 959 loss: [0.01589387]\n",
      "epoch: 960 loss: [0.01589417]\n",
      "epoch: 961 loss: [0.01589447]\n",
      "epoch: 962 loss: [0.01589478]\n",
      "epoch: 963 loss: [0.01589508]\n",
      "epoch: 964 loss: [0.01589537]\n",
      "epoch: 965 loss: [0.01589567]\n",
      "epoch: 966 loss: [0.01589596]\n",
      "epoch: 967 loss: [0.01589626]\n",
      "epoch: 968 loss: [0.01589655]\n",
      "epoch: 969 loss: [0.01589683]\n",
      "epoch: 970 loss: [0.01589712]\n",
      "epoch: 971 loss: [0.01589741]\n",
      "epoch: 972 loss: [0.01589769]\n",
      "epoch: 973 loss: [0.01589797]\n",
      "epoch: 974 loss: [0.01589825]\n",
      "epoch: 975 loss: [0.01589853]\n",
      "epoch: 976 loss: [0.0158988]\n",
      "epoch: 977 loss: [0.01589908]\n",
      "epoch: 978 loss: [0.01589935]\n",
      "epoch: 979 loss: [0.01589962]\n",
      "epoch: 980 loss: [0.01589989]\n",
      "epoch: 981 loss: [0.01590016]\n",
      "epoch: 982 loss: [0.01590042]\n",
      "epoch: 983 loss: [0.01590069]\n",
      "epoch: 984 loss: [0.01590095]\n",
      "epoch: 985 loss: [0.01590121]\n",
      "epoch: 986 loss: [0.01590147]\n",
      "epoch: 987 loss: [0.01590173]\n",
      "epoch: 988 loss: [0.01590198]\n",
      "epoch: 989 loss: [0.01590224]\n",
      "epoch: 990 loss: [0.01590249]\n",
      "epoch: 991 loss: [0.01590274]\n",
      "epoch: 992 loss: [0.01590299]\n",
      "epoch: 993 loss: [0.01590324]\n",
      "epoch: 994 loss: [0.01590349]\n",
      "epoch: 995 loss: [0.01590373]\n",
      "epoch: 996 loss: [0.01590397]\n",
      "epoch: 997 loss: [0.01590421]\n",
      "epoch: 998 loss: [0.01590445]\n",
      "epoch: 999 loss: [0.01590469]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [0.25295813] err: [-0.25295813]\n",
      "[0. 1.] [1.] -> [0.74995601] err: [0.25004399]\n",
      "[1. 0.] [1.] -> [0.74766292] err: [0.25233708]\n",
      "[1. 1.] [1.] -> [1.2446608] err: [-0.2446608]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [1.] -> 1 err: [0.]\n",
      "[1. 0.] [1.] -> 1 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
