{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習\n",
    "\n",
    "ステップ１で作った関数をもう一度実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# numpyを導入\n",
    "import numpy as np\n",
    "\n",
    "# モデル作成\n",
    "def create_model():    \n",
    "    model = {\n",
    "        # 荷重を -5 ~ 5 の乱数で初期化\n",
    "        \"weights\" : np.random.uniform(-5, 5, 2),    \n",
    "        # バイアスも！\n",
    "        \"bias\"    : np.random.uniform(-5, 5, 1)}\n",
    "    return model\n",
    "\n",
    "# 推論\n",
    "def predict(model, activation, x):\n",
    "    \n",
    "    # 足し算を計算し…\n",
    "    y = model[\"weights\"][0] * x[0] + model[\"weights\"][1] * x[1] + model[\"bias\"]\n",
    "    \n",
    "    # 活性化で処理し、その結果を返す\n",
    "    y = activation(y)\n",
    "    return y\n",
    "\n",
    "# 線形活性化関数\n",
    "def linear_activation(x):\n",
    "    return x\n",
    "\n",
    "# ステップ活性化関数\n",
    "def step_activation(x):\n",
    "    if x >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差を計算\n",
    "\n",
    "正しい答え（ラベル）と推論した答えの差分を用い、学習させる。\n",
    "\n",
    "まず、「AND」のラベルを準備しよう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 入力\n",
    "x_list = np.array([\n",
    "    [0, 0], \n",
    "    [0, 1], \n",
    "    [1, 0], \n",
    "    [1, 1]\n",
    "], dtype = float)\n",
    "print(x_list.shape)\n",
    "\n",
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [0], \n",
    "    [0], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差（損失）関数を実装しよう。\n",
    "\n",
    "課題により、適切な関数を使うべきが、今回の入門課題はただの「差分」にしよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "def error(y_true, y_pred):\n",
    "    return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示する\n",
    "def print_results(model, activation, x_list, y_true):\n",
    "    \n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        err = error(y_t, y_p)\n",
    "        print(x, y_t, \"->\", y_p, \"err:\", err)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [-4.76588595] err: [4.76588595]\n",
      "[0. 1.] [0.] -> [-6.4132334] err: [6.4132334]\n",
      "[1. 0.] [0.] -> [-4.46310244] err: [4.46310244]\n",
      "[1. 1.] [1.] -> [-6.1104499] err: [7.1104499]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [0.] -> 0 err: [0.]\n",
      "[1. 0.] [0.] -> 0 err: [0.]\n",
      "[1. 1.] [1.] -> 0 err: [1.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "誤差にて、荷重を調整しよう。ただ、「入力」は「０」であると、出力に影響がないため、入力は「１」のときだけに荷重を調整する、つまり：\n",
    "\n",
    "$$ w_i' = w_i + x_i \\cdot error(y_{true}, y_{pred}) $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 荷重を更新する関数\n",
    "def update_weight(w, x, err):\n",
    "    return w + x * err\n",
    "\n",
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "    \n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "        \n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        \n",
    "        # 誤差を計算\n",
    "        err  = error(y_t, y_p)\n",
    "        mse += err * err\n",
    "            \n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "        \n",
    "        w0   = update_weight(w0, x[0], err)\n",
    "        w1   = update_weight(w1, x[1], err)\n",
    "        bias = update_weight(bias, 1 , err)\n",
    "        \n",
    "        model[\"weights\"][0] = w0[0]\n",
    "        model[\"weights\"][1] = w1[0]\n",
    "        model[\"bias\"] = bias[0]\n",
    "    \n",
    "    #誤差（損失）としては、平均値を返す\n",
    "    return mse / data_size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [9.48342647]\n"
     ]
    }
   ],
   "source": [
    "loss = fit_single_step(model, linear_activation, x_list, y_true)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 2.647347455187591 err: [-2.64734746]\n",
      "[0. 1.] [0.] -> 5.597478412325592 err: [-5.59747841]\n",
      "[1. 0.] [0.] -> 3.950130957138001 err: [-3.95013096]\n",
      "[1. 1.] [1.] -> 6.900261914276001 err: [-5.90026191]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, activation, x_list, y_true, epochs):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss = fit_single_step(model, activation, x_list, y_true)\n",
    "        print(\"epoch:\", i, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [4.71113339]\n",
      "epoch: 1 loss: [7.13221918]\n",
      "epoch: 2 loss: [4.71517769]\n",
      "epoch: 3 loss: [12.23374189]\n",
      "epoch: 4 loss: [6.71922199]\n",
      "epoch: 5 loss: [19.3352646]\n",
      "epoch: 6 loss: [10.72326628]\n",
      "epoch: 7 loss: [28.4367873]\n",
      "epoch: 8 loss: [16.72731058]\n",
      "epoch: 9 loss: [39.53831001]\n",
      "epoch: 10 loss: [24.73135487]\n",
      "epoch: 11 loss: [52.63983272]\n",
      "epoch: 12 loss: [34.73539917]\n",
      "epoch: 13 loss: [67.74135543]\n",
      "epoch: 14 loss: [46.73944347]\n",
      "epoch: 15 loss: [84.84287814]\n",
      "epoch: 16 loss: [60.74348776]\n",
      "epoch: 17 loss: [103.94440085]\n",
      "epoch: 18 loss: [76.74753206]\n",
      "epoch: 19 loss: [125.04592355]\n",
      "epoch: 20 loss: [94.75157635]\n",
      "epoch: 21 loss: [148.14744626]\n",
      "epoch: 22 loss: [114.75562065]\n",
      "epoch: 23 loss: [173.24896897]\n",
      "epoch: 24 loss: [136.75966494]\n",
      "epoch: 25 loss: [200.35049168]\n",
      "epoch: 26 loss: [160.76370924]\n",
      "epoch: 27 loss: [229.45201439]\n",
      "epoch: 28 loss: [186.76775354]\n",
      "epoch: 29 loss: [260.55353709]\n",
      "epoch: 30 loss: [214.77179783]\n",
      "epoch: 31 loss: [293.6550598]\n",
      "epoch: 32 loss: [244.77584213]\n",
      "epoch: 33 loss: [328.75658251]\n",
      "epoch: 34 loss: [276.77988642]\n",
      "epoch: 35 loss: [365.85810522]\n",
      "epoch: 36 loss: [310.78393072]\n",
      "epoch: 37 loss: [404.95962793]\n",
      "epoch: 38 loss: [346.78797501]\n",
      "epoch: 39 loss: [446.06115063]\n",
      "epoch: 40 loss: [384.79201931]\n",
      "epoch: 41 loss: [489.16267334]\n",
      "epoch: 42 loss: [424.79606361]\n",
      "epoch: 43 loss: [534.26419605]\n",
      "epoch: 44 loss: [466.8001079]\n",
      "epoch: 45 loss: [581.36571876]\n",
      "epoch: 46 loss: [510.8041522]\n",
      "epoch: 47 loss: [630.46724147]\n",
      "epoch: 48 loss: [556.80819649]\n",
      "epoch: 49 loss: [681.56876417]\n",
      "epoch: 50 loss: [604.81224079]\n",
      "epoch: 51 loss: [734.67028688]\n",
      "epoch: 52 loss: [654.81628509]\n",
      "epoch: 53 loss: [789.77180959]\n",
      "epoch: 54 loss: [706.82032938]\n",
      "epoch: 55 loss: [846.8733323]\n",
      "epoch: 56 loss: [760.82437368]\n",
      "epoch: 57 loss: [905.97485501]\n",
      "epoch: 58 loss: [816.82841797]\n",
      "epoch: 59 loss: [967.07637772]\n",
      "epoch: 60 loss: [874.83246227]\n",
      "epoch: 61 loss: [1030.17790042]\n",
      "epoch: 62 loss: [934.83650656]\n",
      "epoch: 63 loss: [1095.27942313]\n",
      "epoch: 64 loss: [996.84055086]\n",
      "epoch: 65 loss: [1162.38094584]\n",
      "epoch: 66 loss: [1060.84459516]\n",
      "epoch: 67 loss: [1231.48246855]\n",
      "epoch: 68 loss: [1126.84863945]\n",
      "epoch: 69 loss: [1302.58399126]\n",
      "epoch: 70 loss: [1194.85268375]\n",
      "epoch: 71 loss: [1375.68551396]\n",
      "epoch: 72 loss: [1264.85672804]\n",
      "epoch: 73 loss: [1450.78703667]\n",
      "epoch: 74 loss: [1336.86077234]\n",
      "epoch: 75 loss: [1527.88855938]\n",
      "epoch: 76 loss: [1410.86481663]\n",
      "epoch: 77 loss: [1606.99008209]\n",
      "epoch: 78 loss: [1486.86886093]\n",
      "epoch: 79 loss: [1688.0916048]\n",
      "epoch: 80 loss: [1564.87290523]\n",
      "epoch: 81 loss: [1771.1931275]\n",
      "epoch: 82 loss: [1644.87694952]\n",
      "epoch: 83 loss: [1856.29465021]\n",
      "epoch: 84 loss: [1726.88099382]\n",
      "epoch: 85 loss: [1943.39617292]\n",
      "epoch: 86 loss: [1810.88503811]\n",
      "epoch: 87 loss: [2032.49769563]\n",
      "epoch: 88 loss: [1896.88908241]\n",
      "epoch: 89 loss: [2123.59921834]\n",
      "epoch: 90 loss: [1984.89312671]\n",
      "epoch: 91 loss: [2216.70074105]\n",
      "epoch: 92 loss: [2074.897171]\n",
      "epoch: 93 loss: [2311.80226375]\n",
      "epoch: 94 loss: [2166.9012153]\n",
      "epoch: 95 loss: [2408.90378646]\n",
      "epoch: 96 loss: [2260.90525959]\n",
      "epoch: 97 loss: [2508.00530917]\n",
      "epoch: 98 loss: [2356.90930389]\n",
      "epoch: 99 loss: [2609.10683188]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習率とは\n",
    "\n",
    "上記のように、差分だけを直そうとすると、平均的な誤差がお大きくなってしまう。その理由は、確か、勾配の方向が正しいが、ステップが大きいすぎる。つまり、最適な数値から大幅に超えてしまい、段々離れてしまう。\n",
    "\n",
    "「学習率」という係数で、ステップの大きさを小さくし、少しずつ最適な数値に近づくようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "    \n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "        \n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        \n",
    "        # 誤差を計算\n",
    "        err = error(y_t, y_p)\n",
    "        mse = err * err\n",
    "        \n",
    "        # 学習率\n",
    "        learning_rate = 0.01\n",
    "\n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "        \n",
    "        w0   = update_weight(w0, x[0], err * learning_rate)\n",
    "        w1   = update_weight(w1, x[1], err * learning_rate)\n",
    "        bias = update_weight(bias, 1 , err * learning_rate)\n",
    "        \n",
    "        model[\"weights\"][0] = w0[0]\n",
    "        model[\"weights\"][1] = w1[0]\n",
    "        model[\"bias\"] = bias[0]\n",
    "        \n",
    "    return mse / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [2764.92505837]\n",
      "epoch: 1 loss: [2569.84868712]\n",
      "epoch: 2 loss: [2392.57119241]\n",
      "epoch: 3 loss: [2231.28874461]\n",
      "epoch: 4 loss: [2084.39244798]\n",
      "epoch: 5 loss: [1950.4463416]\n",
      "epoch: 6 loss: [1828.16795372]\n",
      "epoch: 7 loss: [1716.4111078]\n",
      "epoch: 8 loss: [1614.15071508]\n",
      "epoch: 9 loss: [1520.46931931]\n",
      "epoch: 10 loss: [1434.54518796]\n",
      "epoch: 11 loss: [1355.64176824]\n",
      "epoch: 12 loss: [1283.09834797]\n",
      "epoch: 13 loss: [1216.32178044]\n",
      "epoch: 14 loss: [1154.77914895]\n",
      "epoch: 15 loss: [1097.99126161]\n",
      "epoch: 16 loss: [1045.52687987]\n",
      "epoch: 17 loss: [996.99759551]\n",
      "epoch: 18 loss: [952.05328126]\n",
      "epoch: 19 loss: [910.37804859]\n",
      "epoch: 20 loss: [871.6866544]\n",
      "epoch: 21 loss: [835.72130498]\n",
      "epoch: 22 loss: [802.24881175]\n",
      "epoch: 23 loss: [771.05805861]\n",
      "epoch: 24 loss: [741.95774543]\n",
      "epoch: 25 loss: [714.77437627]\n",
      "epoch: 26 loss: [689.35046478]\n",
      "epoch: 27 loss: [665.54293215]\n",
      "epoch: 28 loss: [643.22167611]\n",
      "epoch: 29 loss: [622.26829177]\n",
      "epoch: 30 loss: [602.57492741]\n",
      "epoch: 31 loss: [584.04326027]\n",
      "epoch: 32 loss: [566.583579]\n",
      "epoch: 33 loss: [550.11396118]\n",
      "epoch: 34 loss: [534.5595353]\n",
      "epoch: 35 loss: [519.85181827]\n",
      "epoch: 36 loss: [505.92811997]\n",
      "epoch: 37 loss: [492.73100787]\n",
      "epoch: 38 loss: [480.20782516]\n",
      "epoch: 39 loss: [468.31025664]\n",
      "epoch: 40 loss: [456.99393743]\n",
      "epoch: 41 loss: [446.21809987]\n",
      "epoch: 42 loss: [435.94525472]\n",
      "epoch: 43 loss: [426.14090298]\n",
      "epoch: 44 loss: [416.77327523]\n",
      "epoch: 45 loss: [407.81309566]\n",
      "epoch: 46 loss: [399.23336826]\n",
      "epoch: 47 loss: [391.00918291]\n",
      "epoch: 48 loss: [383.11753939]\n",
      "epoch: 49 loss: [375.53718752]\n",
      "epoch: 50 loss: [368.24848182]\n",
      "epoch: 51 loss: [361.23324929]\n",
      "epoch: 52 loss: [354.47466894]\n",
      "epoch: 53 loss: [347.95716207]\n",
      "epoch: 54 loss: [341.66629219]\n",
      "epoch: 55 loss: [335.58867361]\n",
      "epoch: 56 loss: [329.71188799]\n",
      "epoch: 57 loss: [324.02440803]\n",
      "epoch: 58 loss: [318.51552764]\n",
      "epoch: 59 loss: [313.17529808]\n",
      "epoch: 60 loss: [307.99446931]\n",
      "epoch: 61 loss: [302.96443641]\n",
      "epoch: 62 loss: [298.07719023]\n",
      "epoch: 63 loss: [293.32527223]\n",
      "epoch: 64 loss: [288.70173289]\n",
      "epoch: 65 loss: [284.20009357]\n",
      "epoch: 66 loss: [279.81431136]\n",
      "epoch: 67 loss: [275.53874679]\n",
      "epoch: 68 loss: [271.36813399]\n",
      "epoch: 69 loss: [267.29755338]\n",
      "epoch: 70 loss: [263.32240627]\n",
      "epoch: 71 loss: [259.43839163]\n",
      "epoch: 72 loss: [255.64148455]\n",
      "epoch: 73 loss: [251.92791636]\n",
      "epoch: 74 loss: [248.29415629]\n",
      "epoch: 75 loss: [244.73689453]\n",
      "epoch: 76 loss: [241.25302657]\n",
      "epoch: 77 loss: [237.83963864]\n",
      "epoch: 78 loss: [234.49399438]\n",
      "epoch: 79 loss: [231.21352239]\n",
      "epoch: 80 loss: [227.99580473]\n",
      "epoch: 81 loss: [224.83856628]\n",
      "epoch: 82 loss: [221.7396649]\n",
      "epoch: 83 loss: [218.69708226]\n",
      "epoch: 84 loss: [215.70891538]\n",
      "epoch: 85 loss: [212.77336876]\n",
      "epoch: 86 loss: [209.88874712]\n",
      "epoch: 87 loss: [207.05344856]\n",
      "epoch: 88 loss: [204.26595838]\n",
      "epoch: 89 loss: [201.52484316]\n",
      "epoch: 90 loss: [198.82874537]\n",
      "epoch: 91 loss: [196.17637839]\n",
      "epoch: 92 loss: [193.56652176]\n",
      "epoch: 93 loss: [190.99801687]\n",
      "epoch: 94 loss: [188.46976292]\n",
      "epoch: 95 loss: [185.98071319]\n",
      "epoch: 96 loss: [183.52987148]\n",
      "epoch: 97 loss: [181.11628894]\n",
      "epoch: 98 loss: [178.73906099]\n",
      "epoch: 99 loss: [176.39732455]\n",
      "epoch: 100 loss: [174.09025537]\n",
      "epoch: 101 loss: [171.81706564]\n",
      "epoch: 102 loss: [169.57700173]\n",
      "epoch: 103 loss: [167.369342]\n",
      "epoch: 104 loss: [165.19339493]\n",
      "epoch: 105 loss: [163.04849721]\n",
      "epoch: 106 loss: [160.93401208]\n",
      "epoch: 107 loss: [158.8493277]\n",
      "epoch: 108 loss: [156.7938557]\n",
      "epoch: 109 loss: [154.76702977]\n",
      "epoch: 110 loss: [152.76830436]\n",
      "epoch: 111 loss: [150.79715353]\n",
      "epoch: 112 loss: [148.85306975]\n",
      "epoch: 113 loss: [146.93556292]\n",
      "epoch: 114 loss: [145.04415936]\n",
      "epoch: 115 loss: [143.17840087]\n",
      "epoch: 116 loss: [141.33784393]\n",
      "epoch: 117 loss: [139.52205887]\n",
      "epoch: 118 loss: [137.73062915]\n",
      "epoch: 119 loss: [135.96315063]\n",
      "epoch: 120 loss: [134.21923094]\n",
      "epoch: 121 loss: [132.49848888]\n",
      "epoch: 122 loss: [130.80055383]\n",
      "epoch: 123 loss: [129.12506524]\n",
      "epoch: 124 loss: [127.47167212]\n",
      "epoch: 125 loss: [125.84003259]\n",
      "epoch: 126 loss: [124.22981339]\n",
      "epoch: 127 loss: [122.64068957]\n",
      "epoch: 128 loss: [121.072344]\n",
      "epoch: 129 loss: [119.52446709]\n",
      "epoch: 130 loss: [117.99675642]\n",
      "epoch: 131 loss: [116.48891642]\n",
      "epoch: 132 loss: [115.0006581]\n",
      "epoch: 133 loss: [113.53169874]\n",
      "epoch: 134 loss: [112.08176165]\n",
      "epoch: 135 loss: [110.65057592]\n",
      "epoch: 136 loss: [109.23787621]\n",
      "epoch: 137 loss: [107.84340247]\n",
      "epoch: 138 loss: [106.46689981]\n",
      "epoch: 139 loss: [105.10811828]\n",
      "epoch: 140 loss: [103.76681267]\n",
      "epoch: 141 loss: [102.44274236]\n",
      "epoch: 142 loss: [101.13567116]\n",
      "epoch: 143 loss: [99.84536716]\n",
      "epoch: 144 loss: [98.57160256]\n",
      "epoch: 145 loss: [97.31415358]\n",
      "epoch: 146 loss: [96.0728003]\n",
      "epoch: 147 loss: [94.84732653]\n",
      "epoch: 148 loss: [93.63751974]\n",
      "epoch: 149 loss: [92.44317091]\n",
      "epoch: 150 loss: [91.26407444]\n",
      "epoch: 151 loss: [90.10002807]\n",
      "epoch: 152 loss: [88.95083276]\n",
      "epoch: 153 loss: [87.81629261]\n",
      "epoch: 154 loss: [86.69621479]\n",
      "epoch: 155 loss: [85.59040945]\n",
      "epoch: 156 loss: [84.49868965]\n",
      "epoch: 157 loss: [83.42087128]\n",
      "epoch: 158 loss: [82.35677299]\n",
      "epoch: 159 loss: [81.30621614]\n",
      "epoch: 160 loss: [80.26902473]\n",
      "epoch: 161 loss: [79.24502533]\n",
      "epoch: 162 loss: [78.23404702]\n",
      "epoch: 163 loss: [77.23592137]\n",
      "epoch: 164 loss: [76.25048234]\n",
      "epoch: 165 loss: [75.27756628]\n",
      "epoch: 166 loss: [74.31701182]\n",
      "epoch: 167 loss: [73.36865988]\n",
      "epoch: 168 loss: [72.4323536]\n",
      "epoch: 169 loss: [71.50793828]\n",
      "epoch: 170 loss: [70.5952614]\n",
      "epoch: 171 loss: [69.6941725]\n",
      "epoch: 172 loss: [68.80452319]\n",
      "epoch: 173 loss: [67.92616711]\n",
      "epoch: 174 loss: [67.05895988]\n",
      "epoch: 175 loss: [66.20275907]\n",
      "epoch: 176 loss: [65.35742417]\n",
      "epoch: 177 loss: [64.52281655]\n",
      "epoch: 178 loss: [63.69879945]\n",
      "epoch: 179 loss: [62.88523792]\n",
      "epoch: 180 loss: [62.08199879]\n",
      "epoch: 181 loss: [61.28895067]\n",
      "epoch: 182 loss: [60.50596391]\n",
      "epoch: 183 loss: [59.73291055]\n",
      "epoch: 184 loss: [58.96966433]\n",
      "epoch: 185 loss: [58.21610064]\n",
      "epoch: 186 loss: [57.47209648]\n",
      "epoch: 187 loss: [56.73753049]\n",
      "epoch: 188 loss: [56.01228286]\n",
      "epoch: 189 loss: [55.29623536]\n",
      "epoch: 190 loss: [54.58927128]\n",
      "epoch: 191 loss: [53.89127543]\n",
      "epoch: 192 loss: [53.2021341]\n",
      "epoch: 193 loss: [52.52173507]\n",
      "epoch: 194 loss: [51.84996754]\n",
      "epoch: 195 loss: [51.18672217]\n",
      "epoch: 196 loss: [50.53189099]\n",
      "epoch: 197 loss: [49.88536743]\n",
      "epoch: 198 loss: [49.24704632]\n",
      "epoch: 199 loss: [48.61682378]\n",
      "epoch: 200 loss: [47.99459732]\n",
      "epoch: 201 loss: [47.38026571]\n",
      "epoch: 202 loss: [46.77372906]\n",
      "epoch: 203 loss: [46.17488871]\n",
      "epoch: 204 loss: [45.5836473]\n",
      "epoch: 205 loss: [44.99990869]\n",
      "epoch: 206 loss: [44.42357797]\n",
      "epoch: 207 loss: [43.85456143]\n",
      "epoch: 208 loss: [43.29276657]\n",
      "epoch: 209 loss: [42.73810206]\n",
      "epoch: 210 loss: [42.19047772]\n",
      "epoch: 211 loss: [41.64980454]\n",
      "epoch: 212 loss: [41.11599461]\n",
      "epoch: 213 loss: [40.58896116]\n",
      "epoch: 214 loss: [40.06861851]\n",
      "epoch: 215 loss: [39.55488208]\n",
      "epoch: 216 loss: [39.04766833]\n",
      "epoch: 217 loss: [38.54689482]\n",
      "epoch: 218 loss: [38.05248012]\n",
      "epoch: 219 loss: [37.56434386]\n",
      "epoch: 220 loss: [37.08240667]\n",
      "epoch: 221 loss: [36.60659018]\n",
      "epoch: 222 loss: [36.13681703]\n",
      "epoch: 223 loss: [35.67301083]\n",
      "epoch: 224 loss: [35.21509615]\n",
      "epoch: 225 loss: [34.76299853]\n",
      "epoch: 226 loss: [34.31664442]\n",
      "epoch: 227 loss: [33.87596124]\n",
      "epoch: 228 loss: [33.44087731]\n",
      "epoch: 229 loss: [33.01132183]\n",
      "epoch: 230 loss: [32.58722494]\n",
      "epoch: 231 loss: [32.16851763]\n",
      "epoch: 232 loss: [31.75513178]\n",
      "epoch: 233 loss: [31.34700012]\n",
      "epoch: 234 loss: [30.94405622]\n",
      "epoch: 235 loss: [30.54623452]\n",
      "epoch: 236 loss: [30.15347025]\n",
      "epoch: 237 loss: [29.76569949]\n",
      "epoch: 238 loss: [29.38285911]\n",
      "epoch: 239 loss: [29.00488678]\n",
      "epoch: 240 loss: [28.63172095]\n",
      "epoch: 241 loss: [28.26330085]\n",
      "epoch: 242 loss: [27.89956649]\n",
      "epoch: 243 loss: [27.54045862]\n",
      "epoch: 244 loss: [27.18591875]\n",
      "epoch: 245 loss: [26.83588912]\n",
      "epoch: 246 loss: [26.4903127]\n",
      "epoch: 247 loss: [26.14913317]\n",
      "epoch: 248 loss: [25.81229495]\n",
      "epoch: 249 loss: [25.47974313]\n",
      "epoch: 250 loss: [25.1514235]\n",
      "epoch: 251 loss: [24.82728254]\n",
      "epoch: 252 loss: [24.50726741]\n",
      "epoch: 253 loss: [24.19132593]\n",
      "epoch: 254 loss: [23.87940657]\n",
      "epoch: 255 loss: [23.57145846]\n",
      "epoch: 256 loss: [23.26743137]\n",
      "epoch: 257 loss: [22.96727571]\n",
      "epoch: 258 loss: [22.67094249]\n",
      "epoch: 259 loss: [22.37838337]\n",
      "epoch: 260 loss: [22.0895506]\n",
      "epoch: 261 loss: [21.80439704]\n",
      "epoch: 262 loss: [21.52287614]\n",
      "epoch: 263 loss: [21.24494194]\n",
      "epoch: 264 loss: [20.97054905]\n",
      "epoch: 265 loss: [20.69965268]\n",
      "epoch: 266 loss: [20.43220857]\n",
      "epoch: 267 loss: [20.16817304]\n",
      "epoch: 268 loss: [19.90750296]\n",
      "epoch: 269 loss: [19.65015572]\n",
      "epoch: 270 loss: [19.39608929]\n",
      "epoch: 271 loss: [19.14526213]\n",
      "epoch: 272 loss: [18.89763325]\n",
      "epoch: 273 loss: [18.65316216]\n",
      "epoch: 274 loss: [18.4118089]\n",
      "epoch: 275 loss: [18.17353398]\n",
      "epoch: 276 loss: [17.93829845]\n",
      "epoch: 277 loss: [17.70606382]\n",
      "epoch: 278 loss: [17.4767921]\n",
      "epoch: 279 loss: [17.25044578]\n",
      "epoch: 280 loss: [17.02698782]\n",
      "epoch: 281 loss: [16.80638163]\n",
      "epoch: 282 loss: [16.58859111]\n",
      "epoch: 283 loss: [16.37358061]\n",
      "epoch: 284 loss: [16.16131491]\n",
      "epoch: 285 loss: [15.95175925]\n",
      "epoch: 286 loss: [15.74487931]\n",
      "epoch: 287 loss: [15.54064119]\n",
      "epoch: 288 loss: [15.33901144]\n",
      "epoch: 289 loss: [15.139957]\n",
      "epoch: 290 loss: [14.94344526]\n",
      "epoch: 291 loss: [14.749444]\n",
      "epoch: 292 loss: [14.55792141]\n",
      "epoch: 293 loss: [14.3688461]\n",
      "epoch: 294 loss: [14.18218704]\n",
      "epoch: 295 loss: [13.99791362]\n",
      "epoch: 296 loss: [13.81599562]\n",
      "epoch: 297 loss: [13.63640317]\n",
      "epoch: 298 loss: [13.4591068]\n",
      "epoch: 299 loss: [13.28407742]\n",
      "epoch: 300 loss: [13.11128629]\n",
      "epoch: 301 loss: [12.94070504]\n",
      "epoch: 302 loss: [12.77230565]\n",
      "epoch: 303 loss: [12.60606046]\n",
      "epoch: 304 loss: [12.44194217]\n",
      "epoch: 305 loss: [12.27992379]\n",
      "epoch: 306 loss: [12.11997872]\n",
      "epoch: 307 loss: [11.96208066]\n",
      "epoch: 308 loss: [11.80620365]\n",
      "epoch: 309 loss: [11.65232205]\n",
      "epoch: 310 loss: [11.50041057]\n",
      "epoch: 311 loss: [11.35044422]\n",
      "epoch: 312 loss: [11.20239831]\n",
      "epoch: 313 loss: [11.0562485]\n",
      "epoch: 314 loss: [10.91197073]\n",
      "epoch: 315 loss: [10.76954125]\n",
      "epoch: 316 loss: [10.62893661]\n",
      "epoch: 317 loss: [10.49013365]\n",
      "epoch: 318 loss: [10.35310952]\n",
      "epoch: 319 loss: [10.21784164]\n",
      "epoch: 320 loss: [10.08430772]\n",
      "epoch: 321 loss: [9.95248576]\n",
      "epoch: 322 loss: [9.82235404]\n",
      "epoch: 323 loss: [9.69389108]\n",
      "epoch: 324 loss: [9.56707572]\n",
      "epoch: 325 loss: [9.44188704]\n",
      "epoch: 326 loss: [9.31830438]\n",
      "epoch: 327 loss: [9.19630736]\n",
      "epoch: 328 loss: [9.07587584]\n",
      "epoch: 329 loss: [8.95698995]\n",
      "epoch: 330 loss: [8.83963005]\n",
      "epoch: 331 loss: [8.72377677]\n",
      "epoch: 332 loss: [8.60941097]\n",
      "epoch: 333 loss: [8.49651375]\n",
      "epoch: 334 loss: [8.38506648]\n",
      "epoch: 335 loss: [8.27505071]\n",
      "epoch: 336 loss: [8.16644828]\n",
      "epoch: 337 loss: [8.05924121]\n",
      "epoch: 338 loss: [7.9534118]\n",
      "epoch: 339 loss: [7.84894251]\n",
      "epoch: 340 loss: [7.74581608]\n",
      "epoch: 341 loss: [7.64401544]\n",
      "epoch: 342 loss: [7.54352374]\n",
      "epoch: 343 loss: [7.44432434]\n",
      "epoch: 344 loss: [7.3464008]\n",
      "epoch: 345 loss: [7.24973693]\n",
      "epoch: 346 loss: [7.15431669]\n",
      "epoch: 347 loss: [7.06012428]\n",
      "epoch: 348 loss: [6.96714408]\n",
      "epoch: 349 loss: [6.87536068]\n",
      "epoch: 350 loss: [6.78475886]\n",
      "epoch: 351 loss: [6.6953236]\n",
      "epoch: 352 loss: [6.60704005]\n",
      "epoch: 353 loss: [6.51989356]\n",
      "epoch: 354 loss: [6.43386968]\n",
      "epoch: 355 loss: [6.34895411]\n",
      "epoch: 356 loss: [6.26513276]\n",
      "epoch: 357 loss: [6.1823917]\n",
      "epoch: 358 loss: [6.10071719]\n",
      "epoch: 359 loss: [6.02009565]\n",
      "epoch: 360 loss: [5.94051369]\n",
      "epoch: 361 loss: [5.86195806]\n",
      "epoch: 362 loss: [5.78441571]\n",
      "epoch: 363 loss: [5.70787373]\n",
      "epoch: 364 loss: [5.6323194]\n",
      "epoch: 365 loss: [5.55774012]\n",
      "epoch: 366 loss: [5.4841235]\n",
      "epoch: 367 loss: [5.41145726]\n",
      "epoch: 368 loss: [5.33972931]\n",
      "epoch: 369 loss: [5.26892769]\n",
      "epoch: 370 loss: [5.1990406]\n",
      "epoch: 371 loss: [5.1300564]\n",
      "epoch: 372 loss: [5.06196358]\n",
      "epoch: 373 loss: [4.99475078]\n",
      "epoch: 374 loss: [4.92840678]\n",
      "epoch: 375 loss: [4.86292053]\n",
      "epoch: 376 loss: [4.79828108]\n",
      "epoch: 377 loss: [4.73447764]\n",
      "epoch: 378 loss: [4.67149956]\n",
      "epoch: 379 loss: [4.60933631]\n",
      "epoch: 380 loss: [4.54797751]\n",
      "epoch: 381 loss: [4.48741289]\n",
      "epoch: 382 loss: [4.42763233]\n",
      "epoch: 383 loss: [4.36862583]\n",
      "epoch: 384 loss: [4.31038352]\n",
      "epoch: 385 loss: [4.25289565]\n",
      "epoch: 386 loss: [4.19615259]\n",
      "epoch: 387 loss: [4.14014485]\n",
      "epoch: 388 loss: [4.08486303]\n",
      "epoch: 389 loss: [4.03029789]\n",
      "epoch: 390 loss: [3.97644027]\n",
      "epoch: 391 loss: [3.92328113]\n",
      "epoch: 392 loss: [3.87081158]\n",
      "epoch: 393 loss: [3.81902279]\n",
      "epoch: 394 loss: [3.76790609]\n",
      "epoch: 395 loss: [3.71745289]\n",
      "epoch: 396 loss: [3.66765471]\n",
      "epoch: 397 loss: [3.6185032]\n",
      "epoch: 398 loss: [3.56999009]\n",
      "epoch: 399 loss: [3.52210723]\n",
      "epoch: 400 loss: [3.47484656]\n",
      "epoch: 401 loss: [3.42820014]\n",
      "epoch: 402 loss: [3.38216012]\n",
      "epoch: 403 loss: [3.33671875]\n",
      "epoch: 404 loss: [3.29186838]\n",
      "epoch: 405 loss: [3.24760145]\n",
      "epoch: 406 loss: [3.20391051]\n",
      "epoch: 407 loss: [3.16078819]\n",
      "epoch: 408 loss: [3.11822723]\n",
      "epoch: 409 loss: [3.07622043]\n",
      "epoch: 410 loss: [3.03476073]\n",
      "epoch: 411 loss: [2.99384111]\n",
      "epoch: 412 loss: [2.95345467]\n",
      "epoch: 413 loss: [2.9135946]\n",
      "epoch: 414 loss: [2.87425414]\n",
      "epoch: 415 loss: [2.83542667]\n",
      "epoch: 416 loss: [2.7971056]\n",
      "epoch: 417 loss: [2.75928446]\n",
      "epoch: 418 loss: [2.72195685]\n",
      "epoch: 419 loss: [2.68511646]\n",
      "epoch: 420 loss: [2.64875703]\n",
      "epoch: 421 loss: [2.61287242]\n",
      "epoch: 422 loss: [2.57745654]\n",
      "epoch: 423 loss: [2.5425034]\n",
      "epoch: 424 loss: [2.50800705]\n",
      "epoch: 425 loss: [2.47396165]\n",
      "epoch: 426 loss: [2.44036143]\n",
      "epoch: 427 loss: [2.40720067]\n",
      "epoch: 428 loss: [2.37447375]\n",
      "epoch: 429 loss: [2.3421751]\n",
      "epoch: 430 loss: [2.31029923]\n",
      "epoch: 431 loss: [2.27884072]\n",
      "epoch: 432 loss: [2.24779423]\n",
      "epoch: 433 loss: [2.21715446]\n",
      "epoch: 434 loss: [2.1869162]\n",
      "epoch: 435 loss: [2.1570743]\n",
      "epoch: 436 loss: [2.12762367]\n",
      "epoch: 437 loss: [2.0985593]\n",
      "epoch: 438 loss: [2.06987623]\n",
      "epoch: 439 loss: [2.04156955]\n",
      "epoch: 440 loss: [2.01363445]\n",
      "epoch: 441 loss: [1.98606615]\n",
      "epoch: 442 loss: [1.95885993]\n",
      "epoch: 443 loss: [1.93201116]\n",
      "epoch: 444 loss: [1.90551524]\n",
      "epoch: 445 loss: [1.87936763]\n",
      "epoch: 446 loss: [1.85356387]\n",
      "epoch: 447 loss: [1.82809953]\n",
      "epoch: 448 loss: [1.80297025]\n",
      "epoch: 449 loss: [1.77817173]\n",
      "epoch: 450 loss: [1.75369971]\n",
      "epoch: 451 loss: [1.72955]\n",
      "epoch: 452 loss: [1.70571845]\n",
      "epoch: 453 loss: [1.68220097]\n",
      "epoch: 454 loss: [1.65899351]\n",
      "epoch: 455 loss: [1.6360921]\n",
      "epoch: 456 loss: [1.61349279]\n",
      "epoch: 457 loss: [1.5911917]\n",
      "epoch: 458 loss: [1.56918499]\n",
      "epoch: 459 loss: [1.54746886]\n",
      "epoch: 460 loss: [1.52603959]\n",
      "epoch: 461 loss: [1.50489346]\n",
      "epoch: 462 loss: [1.48402684]\n",
      "epoch: 463 loss: [1.46343613]\n",
      "epoch: 464 loss: [1.44311778]\n",
      "epoch: 465 loss: [1.42306826]\n",
      "epoch: 466 loss: [1.40328413]\n",
      "epoch: 467 loss: [1.38376195]\n",
      "epoch: 468 loss: [1.36449836]\n",
      "epoch: 469 loss: [1.34549001]\n",
      "epoch: 470 loss: [1.32673362]\n",
      "epoch: 471 loss: [1.30822594]\n",
      "epoch: 472 loss: [1.28996375]\n",
      "epoch: 473 loss: [1.2719439]\n",
      "epoch: 474 loss: [1.25416325]\n",
      "epoch: 475 loss: [1.23661871]\n",
      "epoch: 476 loss: [1.21930725]\n",
      "epoch: 477 loss: [1.20222584]\n",
      "epoch: 478 loss: [1.18537152]\n",
      "epoch: 479 loss: [1.16874136]\n",
      "epoch: 480 loss: [1.15233246]\n",
      "epoch: 481 loss: [1.13614197]\n",
      "epoch: 482 loss: [1.12016705]\n",
      "epoch: 483 loss: [1.10440492]\n",
      "epoch: 484 loss: [1.08885284]\n",
      "epoch: 485 loss: [1.07350808]\n",
      "epoch: 486 loss: [1.05836797]\n",
      "epoch: 487 loss: [1.04342986]\n",
      "epoch: 488 loss: [1.02869113]\n",
      "epoch: 489 loss: [1.01414921]\n",
      "epoch: 490 loss: [0.99980155]\n",
      "epoch: 491 loss: [0.98564563]\n",
      "epoch: 492 loss: [0.97167898]\n",
      "epoch: 493 loss: [0.95789913]\n",
      "epoch: 494 loss: [0.94430368]\n",
      "epoch: 495 loss: [0.93089023]\n",
      "epoch: 496 loss: [0.91765642]\n",
      "epoch: 497 loss: [0.90459994]\n",
      "epoch: 498 loss: [0.89171846]\n",
      "epoch: 499 loss: [0.87900974]\n",
      "epoch: 500 loss: [0.86647153]\n",
      "epoch: 501 loss: [0.85410162]\n",
      "epoch: 502 loss: [0.84189781]\n",
      "epoch: 503 loss: [0.82985797]\n",
      "epoch: 504 loss: [0.81797996]\n",
      "epoch: 505 loss: [0.80626168]\n",
      "epoch: 506 loss: [0.79470106]\n",
      "epoch: 507 loss: [0.78329604]\n",
      "epoch: 508 loss: [0.77204462]\n",
      "epoch: 509 loss: [0.76094478]\n",
      "epoch: 510 loss: [0.74999457]\n",
      "epoch: 511 loss: [0.73919204]\n",
      "epoch: 512 loss: [0.72853526]\n",
      "epoch: 513 loss: [0.71802235]\n",
      "epoch: 514 loss: [0.70765143]\n",
      "epoch: 515 loss: [0.69742065]\n",
      "epoch: 516 loss: [0.6873282]\n",
      "epoch: 517 loss: [0.67737226]\n",
      "epoch: 518 loss: [0.66755107]\n",
      "epoch: 519 loss: [0.65786286]\n",
      "epoch: 520 loss: [0.64830592]\n",
      "epoch: 521 loss: [0.63887851]\n",
      "epoch: 522 loss: [0.62957897]\n",
      "epoch: 523 loss: [0.62040562]\n",
      "epoch: 524 loss: [0.61135682]\n",
      "epoch: 525 loss: [0.60243094]\n",
      "epoch: 526 loss: [0.59362637]\n",
      "epoch: 527 loss: [0.58494155]\n",
      "epoch: 528 loss: [0.5763749]\n",
      "epoch: 529 loss: [0.56792488]\n",
      "epoch: 530 loss: [0.55958997]\n",
      "epoch: 531 loss: [0.55136866]\n",
      "epoch: 532 loss: [0.54325948]\n",
      "epoch: 533 loss: [0.53526095]\n",
      "epoch: 534 loss: [0.52737164]\n",
      "epoch: 535 loss: [0.5195901]\n",
      "epoch: 536 loss: [0.51191494]\n",
      "epoch: 537 loss: [0.50434477]\n",
      "epoch: 538 loss: [0.4968782]\n",
      "epoch: 539 loss: [0.48951388]\n",
      "epoch: 540 loss: [0.48225047]\n",
      "epoch: 541 loss: [0.47508666]\n",
      "epoch: 542 loss: [0.46802114]\n",
      "epoch: 543 loss: [0.46105262]\n",
      "epoch: 544 loss: [0.45417983]\n",
      "epoch: 545 loss: [0.44740151]\n",
      "epoch: 546 loss: [0.44071642]\n",
      "epoch: 547 loss: [0.43412335]\n",
      "epoch: 548 loss: [0.42762109]\n",
      "epoch: 549 loss: [0.42120844]\n",
      "epoch: 550 loss: [0.41488423]\n",
      "epoch: 551 loss: [0.40864729]\n",
      "epoch: 552 loss: [0.40249648]\n",
      "epoch: 553 loss: [0.39643068]\n",
      "epoch: 554 loss: [0.39044875]\n",
      "epoch: 555 loss: [0.3845496]\n",
      "epoch: 556 loss: [0.37873214]\n",
      "epoch: 557 loss: [0.3729953]\n",
      "epoch: 558 loss: [0.36733801]\n",
      "epoch: 559 loss: [0.36175923]\n",
      "epoch: 560 loss: [0.35625792]\n",
      "epoch: 561 loss: [0.35083307]\n",
      "epoch: 562 loss: [0.34548366]\n",
      "epoch: 563 loss: [0.3402087]\n",
      "epoch: 564 loss: [0.33500722]\n",
      "epoch: 565 loss: [0.32987823]\n",
      "epoch: 566 loss: [0.32482079]\n",
      "epoch: 567 loss: [0.31983395]\n",
      "epoch: 568 loss: [0.31491678]\n",
      "epoch: 569 loss: [0.31006836]\n",
      "epoch: 570 loss: [0.30528778]\n",
      "epoch: 571 loss: [0.30057414]\n",
      "epoch: 572 loss: [0.29592656]\n",
      "epoch: 573 loss: [0.29134417]\n",
      "epoch: 574 loss: [0.2868261]\n",
      "epoch: 575 loss: [0.2823715]\n",
      "epoch: 576 loss: [0.27797954]\n",
      "epoch: 577 loss: [0.27364938]\n",
      "epoch: 578 loss: [0.2693802]\n",
      "epoch: 579 loss: [0.2651712]\n",
      "epoch: 580 loss: [0.26102158]\n",
      "epoch: 581 loss: [0.25693055]\n",
      "epoch: 582 loss: [0.25289733]\n",
      "epoch: 583 loss: [0.24892116]\n",
      "epoch: 584 loss: [0.24500128]\n",
      "epoch: 585 loss: [0.24113694]\n",
      "epoch: 586 loss: [0.23732741]\n",
      "epoch: 587 loss: [0.23357195]\n",
      "epoch: 588 loss: [0.22986984]\n",
      "epoch: 589 loss: [0.22622039]\n",
      "epoch: 590 loss: [0.22262288]\n",
      "epoch: 591 loss: [0.21907663]\n",
      "epoch: 592 loss: [0.21558095]\n",
      "epoch: 593 loss: [0.21213516]\n",
      "epoch: 594 loss: [0.20873862]\n",
      "epoch: 595 loss: [0.20539065]\n",
      "epoch: 596 loss: [0.20209061]\n",
      "epoch: 597 loss: [0.19883786]\n",
      "epoch: 598 loss: [0.19563177]\n",
      "epoch: 599 loss: [0.19247171]\n",
      "epoch: 600 loss: [0.18935708]\n",
      "epoch: 601 loss: [0.18628726]\n",
      "epoch: 602 loss: [0.18326166]\n",
      "epoch: 603 loss: [0.18027968]\n",
      "epoch: 604 loss: [0.17734074]\n",
      "epoch: 605 loss: [0.17444426]\n",
      "epoch: 606 loss: [0.17158968]\n",
      "epoch: 607 loss: [0.16877643]\n",
      "epoch: 608 loss: [0.16600396]\n",
      "epoch: 609 loss: [0.16327172]\n",
      "epoch: 610 loss: [0.16057917]\n",
      "epoch: 611 loss: [0.15792579]\n",
      "epoch: 612 loss: [0.15531104]\n",
      "epoch: 613 loss: [0.1527344]\n",
      "epoch: 614 loss: [0.15019537]\n",
      "epoch: 615 loss: [0.14769343]\n",
      "epoch: 616 loss: [0.14522809]\n",
      "epoch: 617 loss: [0.14279886]\n",
      "epoch: 618 loss: [0.14040525]\n",
      "epoch: 619 loss: [0.13804677]\n",
      "epoch: 620 loss: [0.13572296]\n",
      "epoch: 621 loss: [0.13343335]\n",
      "epoch: 622 loss: [0.13117747]\n",
      "epoch: 623 loss: [0.12895488]\n",
      "epoch: 624 loss: [0.12676511]\n",
      "epoch: 625 loss: [0.12460774]\n",
      "epoch: 626 loss: [0.12248231]\n",
      "epoch: 627 loss: [0.1203884]\n",
      "epoch: 628 loss: [0.11832559]\n",
      "epoch: 629 loss: [0.11629344]\n",
      "epoch: 630 loss: [0.11429155]\n",
      "epoch: 631 loss: [0.1123195]\n",
      "epoch: 632 loss: [0.11037689]\n",
      "epoch: 633 loss: [0.10846332]\n",
      "epoch: 634 loss: [0.1065784]\n",
      "epoch: 635 loss: [0.10472173]\n",
      "epoch: 636 loss: [0.10289294]\n",
      "epoch: 637 loss: [0.10109164]\n",
      "epoch: 638 loss: [0.09931746]\n",
      "epoch: 639 loss: [0.09757003]\n",
      "epoch: 640 loss: [0.09584899]\n",
      "epoch: 641 loss: [0.09415397]\n",
      "epoch: 642 loss: [0.09248463]\n",
      "epoch: 643 loss: [0.0908406]\n",
      "epoch: 644 loss: [0.08922155]\n",
      "epoch: 645 loss: [0.08762714]\n",
      "epoch: 646 loss: [0.08605702]\n",
      "epoch: 647 loss: [0.08451086]\n",
      "epoch: 648 loss: [0.08298835]\n",
      "epoch: 649 loss: [0.08148914]\n",
      "epoch: 650 loss: [0.08001293]\n",
      "epoch: 651 loss: [0.0785594]\n",
      "epoch: 652 loss: [0.07712824]\n",
      "epoch: 653 loss: [0.07571913]\n",
      "epoch: 654 loss: [0.07433179]\n",
      "epoch: 655 loss: [0.0729659]\n",
      "epoch: 656 loss: [0.07162117]\n",
      "epoch: 657 loss: [0.07029732]\n",
      "epoch: 658 loss: [0.06899405]\n",
      "epoch: 659 loss: [0.06771108]\n",
      "epoch: 660 loss: [0.06644813]\n",
      "epoch: 661 loss: [0.06520493]\n",
      "epoch: 662 loss: [0.06398119]\n",
      "epoch: 663 loss: [0.06277666]\n",
      "epoch: 664 loss: [0.06159107]\n",
      "epoch: 665 loss: [0.06042415]\n",
      "epoch: 666 loss: [0.05927565]\n",
      "epoch: 667 loss: [0.0581453]\n",
      "epoch: 668 loss: [0.05703287]\n",
      "epoch: 669 loss: [0.0559381]\n",
      "epoch: 670 loss: [0.05486074]\n",
      "epoch: 671 loss: [0.05380056]\n",
      "epoch: 672 loss: [0.05275731]\n",
      "epoch: 673 loss: [0.05173076]\n",
      "epoch: 674 loss: [0.05072068]\n",
      "epoch: 675 loss: [0.04972683]\n",
      "epoch: 676 loss: [0.04874899]\n",
      "epoch: 677 loss: [0.04778694]\n",
      "epoch: 678 loss: [0.04684046]\n",
      "epoch: 679 loss: [0.04590932]\n",
      "epoch: 680 loss: [0.04499332]\n",
      "epoch: 681 loss: [0.04409224]\n",
      "epoch: 682 loss: [0.04320588]\n",
      "epoch: 683 loss: [0.04233402]\n",
      "epoch: 684 loss: [0.04147646]\n",
      "epoch: 685 loss: [0.040633]\n",
      "epoch: 686 loss: [0.03980345]\n",
      "epoch: 687 loss: [0.0389876]\n",
      "epoch: 688 loss: [0.03818527]\n",
      "epoch: 689 loss: [0.03739627]\n",
      "epoch: 690 loss: [0.0366204]\n",
      "epoch: 691 loss: [0.03585748]\n",
      "epoch: 692 loss: [0.03510733]\n",
      "epoch: 693 loss: [0.03436977]\n",
      "epoch: 694 loss: [0.03364461]\n",
      "epoch: 695 loss: [0.03293169]\n",
      "epoch: 696 loss: [0.03223083]\n",
      "epoch: 697 loss: [0.03154186]\n",
      "epoch: 698 loss: [0.03086461]\n",
      "epoch: 699 loss: [0.03019891]\n",
      "epoch: 700 loss: [0.0295446]\n",
      "epoch: 701 loss: [0.02890151]\n",
      "epoch: 702 loss: [0.02826949]\n",
      "epoch: 703 loss: [0.02764838]\n",
      "epoch: 704 loss: [0.02703802]\n",
      "epoch: 705 loss: [0.02643826]\n",
      "epoch: 706 loss: [0.02584895]\n",
      "epoch: 707 loss: [0.02526993]\n",
      "epoch: 708 loss: [0.02470107]\n",
      "epoch: 709 loss: [0.0241422]\n",
      "epoch: 710 loss: [0.0235932]\n",
      "epoch: 711 loss: [0.02305392]\n",
      "epoch: 712 loss: [0.02252421]\n",
      "epoch: 713 loss: [0.02200394]\n",
      "epoch: 714 loss: [0.02149298]\n",
      "epoch: 715 loss: [0.02099119]\n",
      "epoch: 716 loss: [0.02049843]\n",
      "epoch: 717 loss: [0.02001458]\n",
      "epoch: 718 loss: [0.0195395]\n",
      "epoch: 719 loss: [0.01907308]\n",
      "epoch: 720 loss: [0.01861517]\n",
      "epoch: 721 loss: [0.01816567]\n",
      "epoch: 722 loss: [0.01772445]\n",
      "epoch: 723 loss: [0.01729138]\n",
      "epoch: 724 loss: [0.01686635]\n",
      "epoch: 725 loss: [0.01644924]\n",
      "epoch: 726 loss: [0.01603994]\n",
      "epoch: 727 loss: [0.01563832]\n",
      "epoch: 728 loss: [0.01524428]\n",
      "epoch: 729 loss: [0.01485771]\n",
      "epoch: 730 loss: [0.0144785]\n",
      "epoch: 731 loss: [0.01410653]\n",
      "epoch: 732 loss: [0.0137417]\n",
      "epoch: 733 loss: [0.01338391]\n",
      "epoch: 734 loss: [0.01303306]\n",
      "epoch: 735 loss: [0.01268903]\n",
      "epoch: 736 loss: [0.01235173]\n",
      "epoch: 737 loss: [0.01202106]\n",
      "epoch: 738 loss: [0.01169692]\n",
      "epoch: 739 loss: [0.01137921]\n",
      "epoch: 740 loss: [0.01106784]\n",
      "epoch: 741 loss: [0.01076271]\n",
      "epoch: 742 loss: [0.01046373]\n",
      "epoch: 743 loss: [0.01017081]\n",
      "epoch: 744 loss: [0.00988386]\n",
      "epoch: 745 loss: [0.00960278]\n",
      "epoch: 746 loss: [0.0093275]\n",
      "epoch: 747 loss: [0.00905791]\n",
      "epoch: 748 loss: [0.00879395]\n",
      "epoch: 749 loss: [0.00853551]\n",
      "epoch: 750 loss: [0.00828252]\n",
      "epoch: 751 loss: [0.0080349]\n",
      "epoch: 752 loss: [0.00779257]\n",
      "epoch: 753 loss: [0.00755543]\n",
      "epoch: 754 loss: [0.00732343]\n",
      "epoch: 755 loss: [0.00709647]\n",
      "epoch: 756 loss: [0.00687448]\n",
      "epoch: 757 loss: [0.00665738]\n",
      "epoch: 758 loss: [0.00644511]\n",
      "epoch: 759 loss: [0.00623758]\n",
      "epoch: 760 loss: [0.00603472]\n",
      "epoch: 761 loss: [0.00583647]\n",
      "epoch: 762 loss: [0.00564275]\n",
      "epoch: 763 loss: [0.00545348]\n",
      "epoch: 764 loss: [0.00526861]\n",
      "epoch: 765 loss: [0.00508807]\n",
      "epoch: 766 loss: [0.00491178]\n",
      "epoch: 767 loss: [0.00473968]\n",
      "epoch: 768 loss: [0.0045717]\n",
      "epoch: 769 loss: [0.00440779]\n",
      "epoch: 770 loss: [0.00424788]\n",
      "epoch: 771 loss: [0.0040919]\n",
      "epoch: 772 loss: [0.0039398]\n",
      "epoch: 773 loss: [0.00379151]\n",
      "epoch: 774 loss: [0.00364697]\n",
      "epoch: 775 loss: [0.00350613]\n",
      "epoch: 776 loss: [0.00336892]\n",
      "epoch: 777 loss: [0.00323529]\n",
      "epoch: 778 loss: [0.00310518]\n",
      "epoch: 779 loss: [0.00297854]\n",
      "epoch: 780 loss: [0.00285531]\n",
      "epoch: 781 loss: [0.00273543]\n",
      "epoch: 782 loss: [0.00261886]\n",
      "epoch: 783 loss: [0.00250554]\n",
      "epoch: 784 loss: [0.00239541]\n",
      "epoch: 785 loss: [0.00228843]\n",
      "epoch: 786 loss: [0.00218455]\n",
      "epoch: 787 loss: [0.00208371]\n",
      "epoch: 788 loss: [0.00198587]\n",
      "epoch: 789 loss: [0.00189098]\n",
      "epoch: 790 loss: [0.00179898]\n",
      "epoch: 791 loss: [0.00170984]\n",
      "epoch: 792 loss: [0.0016235]\n",
      "epoch: 793 loss: [0.00153993]\n",
      "epoch: 794 loss: [0.00145907]\n",
      "epoch: 795 loss: [0.00138088]\n",
      "epoch: 796 loss: [0.00130531]\n",
      "epoch: 797 loss: [0.00123233]\n",
      "epoch: 798 loss: [0.00116189]\n",
      "epoch: 799 loss: [0.00109395]\n",
      "epoch: 800 loss: [0.00102846]\n",
      "epoch: 801 loss: [0.00096539]\n",
      "epoch: 802 loss: [0.00090469]\n",
      "epoch: 803 loss: [0.00084633]\n",
      "epoch: 804 loss: [0.00079026]\n",
      "epoch: 805 loss: [0.00073645]\n",
      "epoch: 806 loss: [0.00068486]\n",
      "epoch: 807 loss: [0.00063545]\n",
      "epoch: 808 loss: [0.00058818]\n",
      "epoch: 809 loss: [0.00054302]\n",
      "epoch: 810 loss: [0.00049993]\n",
      "epoch: 811 loss: [0.00045888]\n",
      "epoch: 812 loss: [0.00041983]\n",
      "epoch: 813 loss: [0.00038275]\n",
      "epoch: 814 loss: [0.00034759]\n",
      "epoch: 815 loss: [0.00031434]\n",
      "epoch: 816 loss: [0.00028295]\n",
      "epoch: 817 loss: [0.0002534]\n",
      "epoch: 818 loss: [0.00022564]\n",
      "epoch: 819 loss: [0.00019966]\n",
      "epoch: 820 loss: [0.00017541]\n",
      "epoch: 821 loss: [0.00015287]\n",
      "epoch: 822 loss: [0.00013201]\n",
      "epoch: 823 loss: [0.00011279]\n",
      "epoch: 824 loss: [9.51955597e-05]\n",
      "epoch: 825 loss: [7.91861673e-05]\n",
      "epoch: 826 loss: [6.47367447e-05]\n",
      "epoch: 827 loss: [5.18190996e-05]\n",
      "epoch: 828 loss: [4.04054483e-05]\n",
      "epoch: 829 loss: [3.04684102e-05]\n",
      "epoch: 830 loss: [2.19810027e-05]\n",
      "epoch: 831 loss: [1.49166353e-05]\n",
      "epoch: 832 loss: [9.24910493e-06]\n",
      "epoch: 833 loss: [4.9525904e-06]\n",
      "epoch: 834 loss: [2.00164731e-06]\n",
      "epoch: 835 loss: [3.71203054e-07]\n",
      "epoch: 836 loss: [3.655176e-08]\n",
      "epoch: 837 loss: [9.73349383e-07]\n",
      "epoch: 838 loss: [3.15760882e-06]\n",
      "epoch: 839 loss: [6.56569511e-06]\n",
      "epoch: 840 loss: [1.11743207e-05]\n",
      "epoch: 841 loss: [1.69605406e-05]\n",
      "epoch: 842 loss: [2.39017482e-05]\n",
      "epoch: 843 loss: [3.19756701e-05]\n",
      "epoch: 844 loss: [4.1160362e-05]\n",
      "epoch: 845 loss: [5.14342043e-05]\n",
      "epoch: 846 loss: [6.27758974e-05]\n",
      "epoch: 847 loss: [7.51644577e-05]\n",
      "epoch: 848 loss: [8.85792129e-05]\n",
      "epoch: 849 loss: [0.000103]\n",
      "epoch: 850 loss: [0.00011841]\n",
      "epoch: 851 loss: [0.00013478]\n",
      "epoch: 852 loss: [0.0001521]\n",
      "epoch: 853 loss: [0.00017034]\n",
      "epoch: 854 loss: [0.0001895]\n",
      "epoch: 855 loss: [0.00020954]\n",
      "epoch: 856 loss: [0.00023046]\n",
      "epoch: 857 loss: [0.00025223]\n",
      "epoch: 858 loss: [0.00027483]\n",
      "epoch: 859 loss: [0.00029826]\n",
      "epoch: 860 loss: [0.00032248]\n",
      "epoch: 861 loss: [0.00034749]\n",
      "epoch: 862 loss: [0.00037327]\n",
      "epoch: 863 loss: [0.00039979]\n",
      "epoch: 864 loss: [0.00042705]\n",
      "epoch: 865 loss: [0.00045503]\n",
      "epoch: 866 loss: [0.00048372]\n",
      "epoch: 867 loss: [0.00051309]\n",
      "epoch: 868 loss: [0.00054313]\n",
      "epoch: 869 loss: [0.00057383]\n",
      "epoch: 870 loss: [0.00060517]\n",
      "epoch: 871 loss: [0.00063714]\n",
      "epoch: 872 loss: [0.00066972]\n",
      "epoch: 873 loss: [0.0007029]\n",
      "epoch: 874 loss: [0.00073667]\n",
      "epoch: 875 loss: [0.000771]\n",
      "epoch: 876 loss: [0.0008059]\n",
      "epoch: 877 loss: [0.00084134]\n",
      "epoch: 878 loss: [0.00087731]\n",
      "epoch: 879 loss: [0.00091381]\n",
      "epoch: 880 loss: [0.0009508]\n",
      "epoch: 881 loss: [0.0009883]\n",
      "epoch: 882 loss: [0.00102627]\n",
      "epoch: 883 loss: [0.00106471]\n",
      "epoch: 884 loss: [0.00110362]\n",
      "epoch: 885 loss: [0.00114296]\n",
      "epoch: 886 loss: [0.00118274]\n",
      "epoch: 887 loss: [0.00122295]\n",
      "epoch: 888 loss: [0.00126356]\n",
      "epoch: 889 loss: [0.00130458]\n",
      "epoch: 890 loss: [0.00134599]\n",
      "epoch: 891 loss: [0.00138778]\n",
      "epoch: 892 loss: [0.00142993]\n",
      "epoch: 893 loss: [0.00147245]\n",
      "epoch: 894 loss: [0.00151531]\n",
      "epoch: 895 loss: [0.00155851]\n",
      "epoch: 896 loss: [0.00160205]\n",
      "epoch: 897 loss: [0.0016459]\n",
      "epoch: 898 loss: [0.00169006]\n",
      "epoch: 899 loss: [0.00173452]\n",
      "epoch: 900 loss: [0.00177927]\n",
      "epoch: 901 loss: [0.00182431]\n",
      "epoch: 902 loss: [0.00186961]\n",
      "epoch: 903 loss: [0.00191519]\n",
      "epoch: 904 loss: [0.00196102]\n",
      "epoch: 905 loss: [0.00200709]\n",
      "epoch: 906 loss: [0.00205341]\n",
      "epoch: 907 loss: [0.00209995]\n",
      "epoch: 908 loss: [0.00214672]\n",
      "epoch: 909 loss: [0.00219371]\n",
      "epoch: 910 loss: [0.0022409]\n",
      "epoch: 911 loss: [0.00228829]\n",
      "epoch: 912 loss: [0.00233588]\n",
      "epoch: 913 loss: [0.00238365]\n",
      "epoch: 914 loss: [0.00243159]\n",
      "epoch: 915 loss: [0.00247971]\n",
      "epoch: 916 loss: [0.00252799]\n",
      "epoch: 917 loss: [0.00257643]\n",
      "epoch: 918 loss: [0.00262502]\n",
      "epoch: 919 loss: [0.00267375]\n",
      "epoch: 920 loss: [0.00272261]\n",
      "epoch: 921 loss: [0.00277161]\n",
      "epoch: 922 loss: [0.00282073]\n",
      "epoch: 923 loss: [0.00286997]\n",
      "epoch: 924 loss: [0.00291931]\n",
      "epoch: 925 loss: [0.00296877]\n",
      "epoch: 926 loss: [0.00301832]\n",
      "epoch: 927 loss: [0.00306797]\n",
      "epoch: 928 loss: [0.00311771]\n",
      "epoch: 929 loss: [0.00316752]\n",
      "epoch: 930 loss: [0.00321742]\n",
      "epoch: 931 loss: [0.00326738]\n",
      "epoch: 932 loss: [0.00331742]\n",
      "epoch: 933 loss: [0.00336751]\n",
      "epoch: 934 loss: [0.00341766]\n",
      "epoch: 935 loss: [0.00346786]\n",
      "epoch: 936 loss: [0.0035181]\n",
      "epoch: 937 loss: [0.00356839]\n",
      "epoch: 938 loss: [0.00361871]\n",
      "epoch: 939 loss: [0.00366906]\n",
      "epoch: 940 loss: [0.00371945]\n",
      "epoch: 941 loss: [0.00376985]\n",
      "epoch: 942 loss: [0.00382027]\n",
      "epoch: 943 loss: [0.00387071]\n",
      "epoch: 944 loss: [0.00392115]\n",
      "epoch: 945 loss: [0.0039716]\n",
      "epoch: 946 loss: [0.00402205]\n",
      "epoch: 947 loss: [0.0040725]\n",
      "epoch: 948 loss: [0.00412294]\n",
      "epoch: 949 loss: [0.00417337]\n",
      "epoch: 950 loss: [0.00422379]\n",
      "epoch: 951 loss: [0.00427418]\n",
      "epoch: 952 loss: [0.00432456]\n",
      "epoch: 953 loss: [0.00437491]\n",
      "epoch: 954 loss: [0.00442523]\n",
      "epoch: 955 loss: [0.00447552]\n",
      "epoch: 956 loss: [0.00452577]\n",
      "epoch: 957 loss: [0.00457599]\n",
      "epoch: 958 loss: [0.00462616]\n",
      "epoch: 959 loss: [0.00467628]\n",
      "epoch: 960 loss: [0.00472636]\n",
      "epoch: 961 loss: [0.00477639]\n",
      "epoch: 962 loss: [0.00482636]\n",
      "epoch: 963 loss: [0.00487627]\n",
      "epoch: 964 loss: [0.00492612]\n",
      "epoch: 965 loss: [0.00497591]\n",
      "epoch: 966 loss: [0.00502564]\n",
      "epoch: 967 loss: [0.00507529]\n",
      "epoch: 968 loss: [0.00512487]\n",
      "epoch: 969 loss: [0.00517438]\n",
      "epoch: 970 loss: [0.00522381]\n",
      "epoch: 971 loss: [0.00527317]\n",
      "epoch: 972 loss: [0.00532244]\n",
      "epoch: 973 loss: [0.00537162]\n",
      "epoch: 974 loss: [0.00542073]\n",
      "epoch: 975 loss: [0.00546974]\n",
      "epoch: 976 loss: [0.00551866]\n",
      "epoch: 977 loss: [0.00556749]\n",
      "epoch: 978 loss: [0.00561622]\n",
      "epoch: 979 loss: [0.00566486]\n",
      "epoch: 980 loss: [0.0057134]\n",
      "epoch: 981 loss: [0.00576183]\n",
      "epoch: 982 loss: [0.00581017]\n",
      "epoch: 983 loss: [0.0058584]\n",
      "epoch: 984 loss: [0.00590652]\n",
      "epoch: 985 loss: [0.00595453]\n",
      "epoch: 986 loss: [0.00600243]\n",
      "epoch: 987 loss: [0.00605022]\n",
      "epoch: 988 loss: [0.0060979]\n",
      "epoch: 989 loss: [0.00614546]\n",
      "epoch: 990 loss: [0.00619291]\n",
      "epoch: 991 loss: [0.00624023]\n",
      "epoch: 992 loss: [0.00628744]\n",
      "epoch: 993 loss: [0.00633452]\n",
      "epoch: 994 loss: [0.00638148]\n",
      "epoch: 995 loss: [0.00642832]\n",
      "epoch: 996 loss: [0.00647503]\n",
      "epoch: 997 loss: [0.00652161]\n",
      "epoch: 998 loss: [0.00656807]\n",
      "epoch: 999 loss: [0.00661439]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> -0.38349529449329667 err: [0.38349529]\n",
      "[0. 1.] [0.] -> 0.2273757695265377 err: [-0.22737577]\n",
      "[1. 0.] [0.] -> 0.23135084497067016 err: [-0.23135084]\n",
      "[1. 1.] [1.] -> 0.8422219089905046 err: [0.15777809]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [0.] -> 0 err: [0.]\n",
      "[1. 0.] [0.] -> 0 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 練習:「OR」を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [1], \n",
    "    [1], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [0.06263027]\n",
      "epoch: 1 loss: [0.04721831]\n",
      "epoch: 2 loss: [0.03480223]\n",
      "epoch: 3 loss: [0.02492871]\n",
      "epoch: 4 loss: [0.01720585]\n",
      "epoch: 5 loss: [0.01129529]\n",
      "epoch: 6 loss: [0.00690527]\n",
      "epoch: 7 loss: [0.00378461]\n",
      "epoch: 8 loss: [0.00171744]\n",
      "epoch: 9 loss: [0.00051854]\n",
      "epoch: 10 loss: [2.9366584e-05]\n",
      "epoch: 11 loss: [0.00011446]\n",
      "epoch: 12 loss: [0.00065843]\n",
      "epoch: 13 loss: [0.00156321]\n",
      "epoch: 14 loss: [0.0027458]\n",
      "epoch: 15 loss: [0.00413612]\n",
      "epoch: 16 loss: [0.00567533]\n",
      "epoch: 17 loss: [0.00731422]\n",
      "epoch: 18 loss: [0.00901189]\n",
      "epoch: 19 loss: [0.01073454]\n",
      "epoch: 20 loss: [0.01245453]\n",
      "epoch: 21 loss: [0.0141494]\n",
      "epoch: 22 loss: [0.01580118]\n",
      "epoch: 23 loss: [0.01739569]\n",
      "epoch: 24 loss: [0.01892196]\n",
      "epoch: 25 loss: [0.02037175]\n",
      "epoch: 26 loss: [0.0217391]\n",
      "epoch: 27 loss: [0.02302]\n",
      "epoch: 28 loss: [0.02421202]\n",
      "epoch: 29 loss: [0.02531405]\n",
      "epoch: 30 loss: [0.0263261]\n",
      "epoch: 31 loss: [0.02724906]\n",
      "epoch: 32 loss: [0.02808453]\n",
      "epoch: 33 loss: [0.02883468]\n",
      "epoch: 34 loss: [0.02950214]\n",
      "epoch: 35 loss: [0.03008988]\n",
      "epoch: 36 loss: [0.03060111]\n",
      "epoch: 37 loss: [0.03103922]\n",
      "epoch: 38 loss: [0.03140772]\n",
      "epoch: 39 loss: [0.03171017]\n",
      "epoch: 40 loss: [0.03195015]\n",
      "epoch: 41 loss: [0.03213123]\n",
      "epoch: 42 loss: [0.03225694]\n",
      "epoch: 43 loss: [0.03233071]\n",
      "epoch: 44 loss: [0.03235591]\n",
      "epoch: 45 loss: [0.03233581]\n",
      "epoch: 46 loss: [0.03227354]\n",
      "epoch: 47 loss: [0.03217213]\n",
      "epoch: 48 loss: [0.03203448]\n",
      "epoch: 49 loss: [0.03186336]\n",
      "epoch: 50 loss: [0.03166142]\n",
      "epoch: 51 loss: [0.03143116]\n",
      "epoch: 52 loss: [0.03117497]\n",
      "epoch: 53 loss: [0.03089509]\n",
      "epoch: 54 loss: [0.03059366]\n",
      "epoch: 55 loss: [0.03027268]\n",
      "epoch: 56 loss: [0.02993405]\n",
      "epoch: 57 loss: [0.02957953]\n",
      "epoch: 58 loss: [0.02921081]\n",
      "epoch: 59 loss: [0.02882943]\n",
      "epoch: 60 loss: [0.02843686]\n",
      "epoch: 61 loss: [0.02803447]\n",
      "epoch: 62 loss: [0.02762353]\n",
      "epoch: 63 loss: [0.02720523]\n",
      "epoch: 64 loss: [0.02678069]\n",
      "epoch: 65 loss: [0.02635093]\n",
      "epoch: 66 loss: [0.0259169]\n",
      "epoch: 67 loss: [0.0254795]\n",
      "epoch: 68 loss: [0.02503956]\n",
      "epoch: 69 loss: [0.02459782]\n",
      "epoch: 70 loss: [0.024155]\n",
      "epoch: 71 loss: [0.02371175]\n",
      "epoch: 72 loss: [0.02326867]\n",
      "epoch: 73 loss: [0.0228263]\n",
      "epoch: 74 loss: [0.02238516]\n",
      "epoch: 75 loss: [0.02194571]\n",
      "epoch: 76 loss: [0.02150838]\n",
      "epoch: 77 loss: [0.02107356]\n",
      "epoch: 78 loss: [0.0206416]\n",
      "epoch: 79 loss: [0.02021284]\n",
      "epoch: 80 loss: [0.01978755]\n",
      "epoch: 81 loss: [0.01936602]\n",
      "epoch: 82 loss: [0.01894848]\n",
      "epoch: 83 loss: [0.01853515]\n",
      "epoch: 84 loss: [0.01812623]\n",
      "epoch: 85 loss: [0.01772189]\n",
      "epoch: 86 loss: [0.01732228]\n",
      "epoch: 87 loss: [0.01692755]\n",
      "epoch: 88 loss: [0.01653782]\n",
      "epoch: 89 loss: [0.01615318]\n",
      "epoch: 90 loss: [0.01577374]\n",
      "epoch: 91 loss: [0.01539957]\n",
      "epoch: 92 loss: [0.01503074]\n",
      "epoch: 93 loss: [0.01466731]\n",
      "epoch: 94 loss: [0.01430931]\n",
      "epoch: 95 loss: [0.01395679]\n",
      "epoch: 96 loss: [0.01360976]\n",
      "epoch: 97 loss: [0.01326826]\n",
      "epoch: 98 loss: [0.01293229]\n",
      "epoch: 99 loss: [0.01260185]\n",
      "epoch: 100 loss: [0.01227694]\n",
      "epoch: 101 loss: [0.01195756]\n",
      "epoch: 102 loss: [0.01164369]\n",
      "epoch: 103 loss: [0.01133531]\n",
      "epoch: 104 loss: [0.0110324]\n",
      "epoch: 105 loss: [0.01073493]\n",
      "epoch: 106 loss: [0.01044288]\n",
      "epoch: 107 loss: [0.0101562]\n",
      "epoch: 108 loss: [0.00987487]\n",
      "epoch: 109 loss: [0.00959884]\n",
      "epoch: 110 loss: [0.00932808]\n",
      "epoch: 111 loss: [0.00906252]\n",
      "epoch: 112 loss: [0.00880214]\n",
      "epoch: 113 loss: [0.00854689]\n",
      "epoch: 114 loss: [0.0082967]\n",
      "epoch: 115 loss: [0.00805154]\n",
      "epoch: 116 loss: [0.00781135]\n",
      "epoch: 117 loss: [0.00757607]\n",
      "epoch: 118 loss: [0.00734566]\n",
      "epoch: 119 loss: [0.00712005]\n",
      "epoch: 120 loss: [0.0068992]\n",
      "epoch: 121 loss: [0.00668304]\n",
      "epoch: 122 loss: [0.00647152]\n",
      "epoch: 123 loss: [0.00626458]\n",
      "epoch: 124 loss: [0.00606216]\n",
      "epoch: 125 loss: [0.00586421]\n",
      "epoch: 126 loss: [0.00567067]\n",
      "epoch: 127 loss: [0.00548148]\n",
      "epoch: 128 loss: [0.00529659]\n",
      "epoch: 129 loss: [0.00511593]\n",
      "epoch: 130 loss: [0.00493944]\n",
      "epoch: 131 loss: [0.00476708]\n",
      "epoch: 132 loss: [0.00459878]\n",
      "epoch: 133 loss: [0.00443449]\n",
      "epoch: 134 loss: [0.00427415]\n",
      "epoch: 135 loss: [0.0041177]\n",
      "epoch: 136 loss: [0.00396508]\n",
      "epoch: 137 loss: [0.00381625]\n",
      "epoch: 138 loss: [0.00367115]\n",
      "epoch: 139 loss: [0.00352971]\n",
      "epoch: 140 loss: [0.0033919]\n",
      "epoch: 141 loss: [0.00325764]\n",
      "epoch: 142 loss: [0.0031269]\n",
      "epoch: 143 loss: [0.00299961]\n",
      "epoch: 144 loss: [0.00287573]\n",
      "epoch: 145 loss: [0.0027552]\n",
      "epoch: 146 loss: [0.00263797]\n",
      "epoch: 147 loss: [0.00252399]\n",
      "epoch: 148 loss: [0.00241321]\n",
      "epoch: 149 loss: [0.00230558]\n",
      "epoch: 150 loss: [0.00220106]\n",
      "epoch: 151 loss: [0.00209958]\n",
      "epoch: 152 loss: [0.00200111]\n",
      "epoch: 153 loss: [0.0019056]\n",
      "epoch: 154 loss: [0.00181299]\n",
      "epoch: 155 loss: [0.00172325]\n",
      "epoch: 156 loss: [0.00163633]\n",
      "epoch: 157 loss: [0.00155217]\n",
      "epoch: 158 loss: [0.00147075]\n",
      "epoch: 159 loss: [0.00139201]\n",
      "epoch: 160 loss: [0.00131591]\n",
      "epoch: 161 loss: [0.0012424]\n",
      "epoch: 162 loss: [0.00117145]\n",
      "epoch: 163 loss: [0.00110301]\n",
      "epoch: 164 loss: [0.00103704]\n",
      "epoch: 165 loss: [0.0009735]\n",
      "epoch: 166 loss: [0.00091235]\n",
      "epoch: 167 loss: [0.00085355]\n",
      "epoch: 168 loss: [0.00079706]\n",
      "epoch: 169 loss: [0.00074284]\n",
      "epoch: 170 loss: [0.00069086]\n",
      "epoch: 171 loss: [0.00064106]\n",
      "epoch: 172 loss: [0.00059343]\n",
      "epoch: 173 loss: [0.00054792]\n",
      "epoch: 174 loss: [0.0005045]\n",
      "epoch: 175 loss: [0.00046312]\n",
      "epoch: 176 loss: [0.00042376]\n",
      "epoch: 177 loss: [0.00038638]\n",
      "epoch: 178 loss: [0.00035094]\n",
      "epoch: 179 loss: [0.00031742]\n",
      "epoch: 180 loss: [0.00028577]\n",
      "epoch: 181 loss: [0.00025597]\n",
      "epoch: 182 loss: [0.00022798]\n",
      "epoch: 183 loss: [0.00020178]\n",
      "epoch: 184 loss: [0.00017732]\n",
      "epoch: 185 loss: [0.00015459]\n",
      "epoch: 186 loss: [0.00013354]\n",
      "epoch: 187 loss: [0.00011415]\n",
      "epoch: 188 loss: [9.63867752e-05]\n",
      "epoch: 189 loss: [8.02243908e-05]\n",
      "epoch: 190 loss: [6.56323991e-05]\n",
      "epoch: 191 loss: [5.258236e-05]\n",
      "epoch: 192 loss: [4.10462424e-05]\n",
      "epoch: 193 loss: [3.09964196e-05]\n",
      "epoch: 194 loss: [2.24056642e-05]\n",
      "epoch: 195 loss: [1.52471428e-05]\n",
      "epoch: 196 loss: [9.49441101e-06]\n",
      "epoch: 197 loss: [5.12140884e-06]\n",
      "epoch: 198 loss: [2.10245548e-06]\n",
      "epoch: 199 loss: [4.12244491e-07]\n",
      "epoch: 200 loss: [2.58390234e-08]\n",
      "epoch: 201 loss: [9.18666996e-07]\n",
      "epoch: 202 loss: [3.06651636e-06]\n",
      "epoch: 203 loss: [6.44553042e-06]\n",
      "epoch: 204 loss: [1.10322032e-05]\n",
      "epoch: 205 loss: [1.68033746e-05]\n",
      "epoch: 206 loss: [2.37362265e-05]\n",
      "epoch: 207 loss: [3.18082773e-05]\n",
      "epoch: 208 loss: [4.09973784e-05]\n",
      "epoch: 209 loss: [5.1281709e-05]\n",
      "epoch: 210 loss: [6.26397724e-05]\n",
      "epoch: 211 loss: [7.50503912e-05]\n",
      "epoch: 212 loss: [8.84927033e-05]\n",
      "epoch: 213 loss: [0.00010295]\n",
      "epoch: 214 loss: [0.00011839]\n",
      "epoch: 215 loss: [0.00013481]\n",
      "epoch: 216 loss: [0.00015217]\n",
      "epoch: 217 loss: [0.00017047]\n",
      "epoch: 218 loss: [0.00018968]\n",
      "epoch: 219 loss: [0.00020979]\n",
      "epoch: 220 loss: [0.00023077]\n",
      "epoch: 221 loss: [0.00025261]\n",
      "epoch: 222 loss: [0.00027529]\n",
      "epoch: 223 loss: [0.00029879]\n",
      "epoch: 224 loss: [0.00032309]\n",
      "epoch: 225 loss: [0.00034818]\n",
      "epoch: 226 loss: [0.00037405]\n",
      "epoch: 227 loss: [0.00040066]\n",
      "epoch: 228 loss: [0.00042801]\n",
      "epoch: 229 loss: [0.00045609]\n",
      "epoch: 230 loss: [0.00048487]\n",
      "epoch: 231 loss: [0.00051434]\n",
      "epoch: 232 loss: [0.00054449]\n",
      "epoch: 233 loss: [0.00057529]\n",
      "epoch: 234 loss: [0.00060674]\n",
      "epoch: 235 loss: [0.00063882]\n",
      "epoch: 236 loss: [0.00067151]\n",
      "epoch: 237 loss: [0.00070481]\n",
      "epoch: 238 loss: [0.00073869]\n",
      "epoch: 239 loss: [0.00077314]\n",
      "epoch: 240 loss: [0.00080816]\n",
      "epoch: 241 loss: [0.00084372]\n",
      "epoch: 242 loss: [0.00087981]\n",
      "epoch: 243 loss: [0.00091643]\n",
      "epoch: 244 loss: [0.00095355]\n",
      "epoch: 245 loss: [0.00099117]\n",
      "epoch: 246 loss: [0.00102927]\n",
      "epoch: 247 loss: [0.00106784]\n",
      "epoch: 248 loss: [0.00110687]\n",
      "epoch: 249 loss: [0.00114634]\n",
      "epoch: 250 loss: [0.00118625]\n",
      "epoch: 251 loss: [0.00122659]\n",
      "epoch: 252 loss: [0.00126733]\n",
      "epoch: 253 loss: [0.00130848]\n",
      "epoch: 254 loss: [0.00135002]\n",
      "epoch: 255 loss: [0.00139194]\n",
      "epoch: 256 loss: [0.00143423]\n",
      "epoch: 257 loss: [0.00147688]\n",
      "epoch: 258 loss: [0.00151987]\n",
      "epoch: 259 loss: [0.00156321]\n",
      "epoch: 260 loss: [0.00160687]\n",
      "epoch: 261 loss: [0.00165086]\n",
      "epoch: 262 loss: [0.00169515]\n",
      "epoch: 263 loss: [0.00173974]\n",
      "epoch: 264 loss: [0.00178463]\n",
      "epoch: 265 loss: [0.0018298]\n",
      "epoch: 266 loss: [0.00187524]\n",
      "epoch: 267 loss: [0.00192094]\n",
      "epoch: 268 loss: [0.0019669]\n",
      "epoch: 269 loss: [0.00201311]\n",
      "epoch: 270 loss: [0.00205955]\n",
      "epoch: 271 loss: [0.00210623]\n",
      "epoch: 272 loss: [0.00215313]\n",
      "epoch: 273 loss: [0.00220024]\n",
      "epoch: 274 loss: [0.00224756]\n",
      "epoch: 275 loss: [0.00229508]\n",
      "epoch: 276 loss: [0.0023428]\n",
      "epoch: 277 loss: [0.00239069]\n",
      "epoch: 278 loss: [0.00243877]\n",
      "epoch: 279 loss: [0.00248701]\n",
      "epoch: 280 loss: [0.00253541]\n",
      "epoch: 281 loss: [0.00258397]\n",
      "epoch: 282 loss: [0.00263268]\n",
      "epoch: 283 loss: [0.00268154]\n",
      "epoch: 284 loss: [0.00273052]\n",
      "epoch: 285 loss: [0.00277964]\n",
      "epoch: 286 loss: [0.00282888]\n",
      "epoch: 287 loss: [0.00287824]\n",
      "epoch: 288 loss: [0.0029277]\n",
      "epoch: 289 loss: [0.00297727]\n",
      "epoch: 290 loss: [0.00302694]\n",
      "epoch: 291 loss: [0.0030767]\n",
      "epoch: 292 loss: [0.00312655]\n",
      "epoch: 293 loss: [0.00317648]\n",
      "epoch: 294 loss: [0.00322649]\n",
      "epoch: 295 loss: [0.00327657]\n",
      "epoch: 296 loss: [0.00332671]\n",
      "epoch: 297 loss: [0.00337691]\n",
      "epoch: 298 loss: [0.00342716]\n",
      "epoch: 299 loss: [0.00347747]\n",
      "epoch: 300 loss: [0.00352782]\n",
      "epoch: 301 loss: [0.00357821]\n",
      "epoch: 302 loss: [0.00362864]\n",
      "epoch: 303 loss: [0.00367909]\n",
      "epoch: 304 loss: [0.00372957]\n",
      "epoch: 305 loss: [0.00378008]\n",
      "epoch: 306 loss: [0.00383059]\n",
      "epoch: 307 loss: [0.00388113]\n",
      "epoch: 308 loss: [0.00393167]\n",
      "epoch: 309 loss: [0.00398221]\n",
      "epoch: 310 loss: [0.00403276]\n",
      "epoch: 311 loss: [0.0040833]\n",
      "epoch: 312 loss: [0.00413383]\n",
      "epoch: 313 loss: [0.00418435]\n",
      "epoch: 314 loss: [0.00423485]\n",
      "epoch: 315 loss: [0.00428534]\n",
      "epoch: 316 loss: [0.0043358]\n",
      "epoch: 317 loss: [0.00438623]\n",
      "epoch: 318 loss: [0.00443664]\n",
      "epoch: 319 loss: [0.00448701]\n",
      "epoch: 320 loss: [0.00453734]\n",
      "epoch: 321 loss: [0.00458764]\n",
      "epoch: 322 loss: [0.00463789]\n",
      "epoch: 323 loss: [0.00468809]\n",
      "epoch: 324 loss: [0.00473824]\n",
      "epoch: 325 loss: [0.00478834]\n",
      "epoch: 326 loss: [0.00483839]\n",
      "epoch: 327 loss: [0.00488837]\n",
      "epoch: 328 loss: [0.0049383]\n",
      "epoch: 329 loss: [0.00498816]\n",
      "epoch: 330 loss: [0.00503795]\n",
      "epoch: 331 loss: [0.00508767]\n",
      "epoch: 332 loss: [0.00513732]\n",
      "epoch: 333 loss: [0.00518689]\n",
      "epoch: 334 loss: [0.00523639]\n",
      "epoch: 335 loss: [0.0052858]\n",
      "epoch: 336 loss: [0.00533514]\n",
      "epoch: 337 loss: [0.00538438]\n",
      "epoch: 338 loss: [0.00543354]\n",
      "epoch: 339 loss: [0.00548261]\n",
      "epoch: 340 loss: [0.00553159]\n",
      "epoch: 341 loss: [0.00558048]\n",
      "epoch: 342 loss: [0.00562927]\n",
      "epoch: 343 loss: [0.00567796]\n",
      "epoch: 344 loss: [0.00572655]\n",
      "epoch: 345 loss: [0.00577503]\n",
      "epoch: 346 loss: [0.00582342]\n",
      "epoch: 347 loss: [0.00587169]\n",
      "epoch: 348 loss: [0.00591986]\n",
      "epoch: 349 loss: [0.00596792]\n",
      "epoch: 350 loss: [0.00601587]\n",
      "epoch: 351 loss: [0.0060637]\n",
      "epoch: 352 loss: [0.00611142]\n",
      "epoch: 353 loss: [0.00615903]\n",
      "epoch: 354 loss: [0.00620651]\n",
      "epoch: 355 loss: [0.00625388]\n",
      "epoch: 356 loss: [0.00630112]\n",
      "epoch: 357 loss: [0.00634824]\n",
      "epoch: 358 loss: [0.00639524]\n",
      "epoch: 359 loss: [0.00644211]\n",
      "epoch: 360 loss: [0.00648886]\n",
      "epoch: 361 loss: [0.00653547]\n",
      "epoch: 362 loss: [0.00658196]\n",
      "epoch: 363 loss: [0.00662832]\n",
      "epoch: 364 loss: [0.00667454]\n",
      "epoch: 365 loss: [0.00672063]\n",
      "epoch: 366 loss: [0.00676659]\n",
      "epoch: 367 loss: [0.00681241]\n",
      "epoch: 368 loss: [0.00685809]\n",
      "epoch: 369 loss: [0.00690364]\n",
      "epoch: 370 loss: [0.00694905]\n",
      "epoch: 371 loss: [0.00699431]\n",
      "epoch: 372 loss: [0.00703944]\n",
      "epoch: 373 loss: [0.00708443]\n",
      "epoch: 374 loss: [0.00712927]\n",
      "epoch: 375 loss: [0.00717397]\n",
      "epoch: 376 loss: [0.00721853]\n",
      "epoch: 377 loss: [0.00726294]\n",
      "epoch: 378 loss: [0.0073072]\n",
      "epoch: 379 loss: [0.00735132]\n",
      "epoch: 380 loss: [0.00739529]\n",
      "epoch: 381 loss: [0.00743911]\n",
      "epoch: 382 loss: [0.00748278]\n",
      "epoch: 383 loss: [0.00752631]\n",
      "epoch: 384 loss: [0.00756968]\n",
      "epoch: 385 loss: [0.0076129]\n",
      "epoch: 386 loss: [0.00765597]\n",
      "epoch: 387 loss: [0.00769889]\n",
      "epoch: 388 loss: [0.00774166]\n",
      "epoch: 389 loss: [0.00778427]\n",
      "epoch: 390 loss: [0.00782673]\n",
      "epoch: 391 loss: [0.00786904]\n",
      "epoch: 392 loss: [0.00791119]\n",
      "epoch: 393 loss: [0.00795319]\n",
      "epoch: 394 loss: [0.00799503]\n",
      "epoch: 395 loss: [0.00803671]\n",
      "epoch: 396 loss: [0.00807824]\n",
      "epoch: 397 loss: [0.00811961]\n",
      "epoch: 398 loss: [0.00816083]\n",
      "epoch: 399 loss: [0.00820189]\n",
      "epoch: 400 loss: [0.00824279]\n",
      "epoch: 401 loss: [0.00828353]\n",
      "epoch: 402 loss: [0.00832412]\n",
      "epoch: 403 loss: [0.00836454]\n",
      "epoch: 404 loss: [0.00840481]\n",
      "epoch: 405 loss: [0.00844492]\n",
      "epoch: 406 loss: [0.00848487]\n",
      "epoch: 407 loss: [0.00852466]\n",
      "epoch: 408 loss: [0.00856429]\n",
      "epoch: 409 loss: [0.00860376]\n",
      "epoch: 410 loss: [0.00864308]\n",
      "epoch: 411 loss: [0.00868223]\n",
      "epoch: 412 loss: [0.00872122]\n",
      "epoch: 413 loss: [0.00876005]\n",
      "epoch: 414 loss: [0.00879873]\n",
      "epoch: 415 loss: [0.00883724]\n",
      "epoch: 416 loss: [0.00887559]\n",
      "epoch: 417 loss: [0.00891378]\n",
      "epoch: 418 loss: [0.00895181]\n",
      "epoch: 419 loss: [0.00898969]\n",
      "epoch: 420 loss: [0.0090274]\n",
      "epoch: 421 loss: [0.00906495]\n",
      "epoch: 422 loss: [0.00910234]\n",
      "epoch: 423 loss: [0.00913957]\n",
      "epoch: 424 loss: [0.00917664]\n",
      "epoch: 425 loss: [0.00921355]\n",
      "epoch: 426 loss: [0.0092503]\n",
      "epoch: 427 loss: [0.0092869]\n",
      "epoch: 428 loss: [0.00932333]\n",
      "epoch: 429 loss: [0.0093596]\n",
      "epoch: 430 loss: [0.00939571]\n",
      "epoch: 431 loss: [0.00943166]\n",
      "epoch: 432 loss: [0.00946746]\n",
      "epoch: 433 loss: [0.00950309]\n",
      "epoch: 434 loss: [0.00953857]\n",
      "epoch: 435 loss: [0.00957388]\n",
      "epoch: 436 loss: [0.00960904]\n",
      "epoch: 437 loss: [0.00964404]\n",
      "epoch: 438 loss: [0.00967888]\n",
      "epoch: 439 loss: [0.00971357]\n",
      "epoch: 440 loss: [0.00974809]\n",
      "epoch: 441 loss: [0.00978246]\n",
      "epoch: 442 loss: [0.00981667]\n",
      "epoch: 443 loss: [0.00985072]\n",
      "epoch: 444 loss: [0.00988462]\n",
      "epoch: 445 loss: [0.00991836]\n",
      "epoch: 446 loss: [0.00995195]\n",
      "epoch: 447 loss: [0.00998537]\n",
      "epoch: 448 loss: [0.01001865]\n",
      "epoch: 449 loss: [0.01005176]\n",
      "epoch: 450 loss: [0.01008472]\n",
      "epoch: 451 loss: [0.01011753]\n",
      "epoch: 452 loss: [0.01015018]\n",
      "epoch: 453 loss: [0.01018268]\n",
      "epoch: 454 loss: [0.01021503]\n",
      "epoch: 455 loss: [0.01024722]\n",
      "epoch: 456 loss: [0.01027925]\n",
      "epoch: 457 loss: [0.01031114]\n",
      "epoch: 458 loss: [0.01034287]\n",
      "epoch: 459 loss: [0.01037445]\n",
      "epoch: 460 loss: [0.01040587]\n",
      "epoch: 461 loss: [0.01043715]\n",
      "epoch: 462 loss: [0.01046828]\n",
      "epoch: 463 loss: [0.01049925]\n",
      "epoch: 464 loss: [0.01053007]\n",
      "epoch: 465 loss: [0.01056075]\n",
      "epoch: 466 loss: [0.01059127]\n",
      "epoch: 467 loss: [0.01062164]\n",
      "epoch: 468 loss: [0.01065187]\n",
      "epoch: 469 loss: [0.01068195]\n",
      "epoch: 470 loss: [0.01071188]\n",
      "epoch: 471 loss: [0.01074166]\n",
      "epoch: 472 loss: [0.01077129]\n",
      "epoch: 473 loss: [0.01080078]\n",
      "epoch: 474 loss: [0.01083012]\n",
      "epoch: 475 loss: [0.01085931]\n",
      "epoch: 476 loss: [0.01088836]\n",
      "epoch: 477 loss: [0.01091727]\n",
      "epoch: 478 loss: [0.01094603]\n",
      "epoch: 479 loss: [0.01097464]\n",
      "epoch: 480 loss: [0.01100311]\n",
      "epoch: 481 loss: [0.01103144]\n",
      "epoch: 482 loss: [0.01105963]\n",
      "epoch: 483 loss: [0.01108767]\n",
      "epoch: 484 loss: [0.01111557]\n",
      "epoch: 485 loss: [0.01114333]\n",
      "epoch: 486 loss: [0.01117095]\n",
      "epoch: 487 loss: [0.01119842]\n",
      "epoch: 488 loss: [0.01122576]\n",
      "epoch: 489 loss: [0.01125296]\n",
      "epoch: 490 loss: [0.01128002]\n",
      "epoch: 491 loss: [0.01130694]\n",
      "epoch: 492 loss: [0.01133372]\n",
      "epoch: 493 loss: [0.01136036]\n",
      "epoch: 494 loss: [0.01138687]\n",
      "epoch: 495 loss: [0.01141324]\n",
      "epoch: 496 loss: [0.01143947]\n",
      "epoch: 497 loss: [0.01146556]\n",
      "epoch: 498 loss: [0.01149153]\n",
      "epoch: 499 loss: [0.01151735]\n",
      "epoch: 500 loss: [0.01154304]\n",
      "epoch: 501 loss: [0.0115686]\n",
      "epoch: 502 loss: [0.01159403]\n",
      "epoch: 503 loss: [0.01161932]\n",
      "epoch: 504 loss: [0.01164448]\n",
      "epoch: 505 loss: [0.0116695]\n",
      "epoch: 506 loss: [0.0116944]\n",
      "epoch: 507 loss: [0.01171916]\n",
      "epoch: 508 loss: [0.01174379]\n",
      "epoch: 509 loss: [0.0117683]\n",
      "epoch: 510 loss: [0.01179267]\n",
      "epoch: 511 loss: [0.01181692]\n",
      "epoch: 512 loss: [0.01184103]\n",
      "epoch: 513 loss: [0.01186502]\n",
      "epoch: 514 loss: [0.01188888]\n",
      "epoch: 515 loss: [0.01191262]\n",
      "epoch: 516 loss: [0.01193623]\n",
      "epoch: 517 loss: [0.01195971]\n",
      "epoch: 518 loss: [0.01198306]\n",
      "epoch: 519 loss: [0.01200629]\n",
      "epoch: 520 loss: [0.0120294]\n",
      "epoch: 521 loss: [0.01205238]\n",
      "epoch: 522 loss: [0.01207524]\n",
      "epoch: 523 loss: [0.01209798]\n",
      "epoch: 524 loss: [0.01212059]\n",
      "epoch: 525 loss: [0.01214308]\n",
      "epoch: 526 loss: [0.01216545]\n",
      "epoch: 527 loss: [0.0121877]\n",
      "epoch: 528 loss: [0.01220983]\n",
      "epoch: 529 loss: [0.01223184]\n",
      "epoch: 530 loss: [0.01225373]\n",
      "epoch: 531 loss: [0.0122755]\n",
      "epoch: 532 loss: [0.01229715]\n",
      "epoch: 533 loss: [0.01231869]\n",
      "epoch: 534 loss: [0.01234011]\n",
      "epoch: 535 loss: [0.01236141]\n",
      "epoch: 536 loss: [0.01238259]\n",
      "epoch: 537 loss: [0.01240366]\n",
      "epoch: 538 loss: [0.01242461]\n",
      "epoch: 539 loss: [0.01244545]\n",
      "epoch: 540 loss: [0.01246617]\n",
      "epoch: 541 loss: [0.01248678]\n",
      "epoch: 542 loss: [0.01250728]\n",
      "epoch: 543 loss: [0.01252766]\n",
      "epoch: 544 loss: [0.01254793]\n",
      "epoch: 545 loss: [0.01256809]\n",
      "epoch: 546 loss: [0.01258814]\n",
      "epoch: 547 loss: [0.01260808]\n",
      "epoch: 548 loss: [0.0126279]\n",
      "epoch: 549 loss: [0.01264762]\n",
      "epoch: 550 loss: [0.01266723]\n",
      "epoch: 551 loss: [0.01268673]\n",
      "epoch: 552 loss: [0.01270612]\n",
      "epoch: 553 loss: [0.0127254]\n",
      "epoch: 554 loss: [0.01274458]\n",
      "epoch: 555 loss: [0.01276364]\n",
      "epoch: 556 loss: [0.01278261]\n",
      "epoch: 557 loss: [0.01280146]\n",
      "epoch: 558 loss: [0.01282021]\n",
      "epoch: 559 loss: [0.01283886]\n",
      "epoch: 560 loss: [0.0128574]\n",
      "epoch: 561 loss: [0.01287584]\n",
      "epoch: 562 loss: [0.01289417]\n",
      "epoch: 563 loss: [0.01291241]\n",
      "epoch: 564 loss: [0.01293054]\n",
      "epoch: 565 loss: [0.01294856]\n",
      "epoch: 566 loss: [0.01296649]\n",
      "epoch: 567 loss: [0.01298431]\n",
      "epoch: 568 loss: [0.01300204]\n",
      "epoch: 569 loss: [0.01301966]\n",
      "epoch: 570 loss: [0.01303719]\n",
      "epoch: 571 loss: [0.01305461]\n",
      "epoch: 572 loss: [0.01307194]\n",
      "epoch: 573 loss: [0.01308917]\n",
      "epoch: 574 loss: [0.0131063]\n",
      "epoch: 575 loss: [0.01312334]\n",
      "epoch: 576 loss: [0.01314028]\n",
      "epoch: 577 loss: [0.01315712]\n",
      "epoch: 578 loss: [0.01317386]\n",
      "epoch: 579 loss: [0.01319052]\n",
      "epoch: 580 loss: [0.01320707]\n",
      "epoch: 581 loss: [0.01322353]\n",
      "epoch: 582 loss: [0.0132399]\n",
      "epoch: 583 loss: [0.01325618]\n",
      "epoch: 584 loss: [0.01327236]\n",
      "epoch: 585 loss: [0.01328845]\n",
      "epoch: 586 loss: [0.01330445]\n",
      "epoch: 587 loss: [0.01332035]\n",
      "epoch: 588 loss: [0.01333617]\n",
      "epoch: 589 loss: [0.01335189]\n",
      "epoch: 590 loss: [0.01336753]\n",
      "epoch: 591 loss: [0.01338307]\n",
      "epoch: 592 loss: [0.01339853]\n",
      "epoch: 593 loss: [0.01341389]\n",
      "epoch: 594 loss: [0.01342917]\n",
      "epoch: 595 loss: [0.01344436]\n",
      "epoch: 596 loss: [0.01345946]\n",
      "epoch: 597 loss: [0.01347448]\n",
      "epoch: 598 loss: [0.01348941]\n",
      "epoch: 599 loss: [0.01350425]\n",
      "epoch: 600 loss: [0.01351901]\n",
      "epoch: 601 loss: [0.01353368]\n",
      "epoch: 602 loss: [0.01354827]\n",
      "epoch: 603 loss: [0.01356277]\n",
      "epoch: 604 loss: [0.01357719]\n",
      "epoch: 605 loss: [0.01359153]\n",
      "epoch: 606 loss: [0.01360578]\n",
      "epoch: 607 loss: [0.01361995]\n",
      "epoch: 608 loss: [0.01363404]\n",
      "epoch: 609 loss: [0.01364805]\n",
      "epoch: 610 loss: [0.01366197]\n",
      "epoch: 611 loss: [0.01367582]\n",
      "epoch: 612 loss: [0.01368958]\n",
      "epoch: 613 loss: [0.01370326]\n",
      "epoch: 614 loss: [0.01371687]\n",
      "epoch: 615 loss: [0.01373039]\n",
      "epoch: 616 loss: [0.01374384]\n",
      "epoch: 617 loss: [0.0137572]\n",
      "epoch: 618 loss: [0.01377049]\n",
      "epoch: 619 loss: [0.01378371]\n",
      "epoch: 620 loss: [0.01379684]\n",
      "epoch: 621 loss: [0.0138099]\n",
      "epoch: 622 loss: [0.01382288]\n",
      "epoch: 623 loss: [0.01383579]\n",
      "epoch: 624 loss: [0.01384862]\n",
      "epoch: 625 loss: [0.01386137]\n",
      "epoch: 626 loss: [0.01387405]\n",
      "epoch: 627 loss: [0.01388666]\n",
      "epoch: 628 loss: [0.01389919]\n",
      "epoch: 629 loss: [0.01391165]\n",
      "epoch: 630 loss: [0.01392403]\n",
      "epoch: 631 loss: [0.01393635]\n",
      "epoch: 632 loss: [0.01394859]\n",
      "epoch: 633 loss: [0.01396075]\n",
      "epoch: 634 loss: [0.01397285]\n",
      "epoch: 635 loss: [0.01398488]\n",
      "epoch: 636 loss: [0.01399683]\n",
      "epoch: 637 loss: [0.01400872]\n",
      "epoch: 638 loss: [0.01402053]\n",
      "epoch: 639 loss: [0.01403227]\n",
      "epoch: 640 loss: [0.01404395]\n",
      "epoch: 641 loss: [0.01405556]\n",
      "epoch: 642 loss: [0.01406709]\n",
      "epoch: 643 loss: [0.01407856]\n",
      "epoch: 644 loss: [0.01408996]\n",
      "epoch: 645 loss: [0.0141013]\n",
      "epoch: 646 loss: [0.01411257]\n",
      "epoch: 647 loss: [0.01412377]\n",
      "epoch: 648 loss: [0.0141349]\n",
      "epoch: 649 loss: [0.01414597]\n",
      "epoch: 650 loss: [0.01415697]\n",
      "epoch: 651 loss: [0.01416791]\n",
      "epoch: 652 loss: [0.01417878]\n",
      "epoch: 653 loss: [0.01418959]\n",
      "epoch: 654 loss: [0.01420033]\n",
      "epoch: 655 loss: [0.01421101]\n",
      "epoch: 656 loss: [0.01422163]\n",
      "epoch: 657 loss: [0.01423218]\n",
      "epoch: 658 loss: [0.01424268]\n",
      "epoch: 659 loss: [0.0142531]\n",
      "epoch: 660 loss: [0.01426347]\n",
      "epoch: 661 loss: [0.01427377]\n",
      "epoch: 662 loss: [0.01428402]\n",
      "epoch: 663 loss: [0.0142942]\n",
      "epoch: 664 loss: [0.01430432]\n",
      "epoch: 665 loss: [0.01431438]\n",
      "epoch: 666 loss: [0.01432438]\n",
      "epoch: 667 loss: [0.01433432]\n",
      "epoch: 668 loss: [0.01434421]\n",
      "epoch: 669 loss: [0.01435403]\n",
      "epoch: 670 loss: [0.01436379]\n",
      "epoch: 671 loss: [0.0143735]\n",
      "epoch: 672 loss: [0.01438315]\n",
      "epoch: 673 loss: [0.01439274]\n",
      "epoch: 674 loss: [0.01440227]\n",
      "epoch: 675 loss: [0.01441175]\n",
      "epoch: 676 loss: [0.01442117]\n",
      "epoch: 677 loss: [0.01443053]\n",
      "epoch: 678 loss: [0.01443983]\n",
      "epoch: 679 loss: [0.01444908]\n",
      "epoch: 680 loss: [0.01445828]\n",
      "epoch: 681 loss: [0.01446742]\n",
      "epoch: 682 loss: [0.01447651]\n",
      "epoch: 683 loss: [0.01448554]\n",
      "epoch: 684 loss: [0.01449451]\n",
      "epoch: 685 loss: [0.01450343]\n",
      "epoch: 686 loss: [0.0145123]\n",
      "epoch: 687 loss: [0.01452112]\n",
      "epoch: 688 loss: [0.01452988]\n",
      "epoch: 689 loss: [0.01453859]\n",
      "epoch: 690 loss: [0.01454725]\n",
      "epoch: 691 loss: [0.01455585]\n",
      "epoch: 692 loss: [0.01456441]\n",
      "epoch: 693 loss: [0.01457291]\n",
      "epoch: 694 loss: [0.01458136]\n",
      "epoch: 695 loss: [0.01458976]\n",
      "epoch: 696 loss: [0.01459811]\n",
      "epoch: 697 loss: [0.01460641]\n",
      "epoch: 698 loss: [0.01461466]\n",
      "epoch: 699 loss: [0.01462286]\n",
      "epoch: 700 loss: [0.01463101]\n",
      "epoch: 701 loss: [0.01463911]\n",
      "epoch: 702 loss: [0.01464716]\n",
      "epoch: 703 loss: [0.01465516]\n",
      "epoch: 704 loss: [0.01466311]\n",
      "epoch: 705 loss: [0.01467102]\n",
      "epoch: 706 loss: [0.01467888]\n",
      "epoch: 707 loss: [0.01468669]\n",
      "epoch: 708 loss: [0.01469445]\n",
      "epoch: 709 loss: [0.01470217]\n",
      "epoch: 710 loss: [0.01470984]\n",
      "epoch: 711 loss: [0.01471746]\n",
      "epoch: 712 loss: [0.01472504]\n",
      "epoch: 713 loss: [0.01473257]\n",
      "epoch: 714 loss: [0.01474006]\n",
      "epoch: 715 loss: [0.0147475]\n",
      "epoch: 716 loss: [0.01475489]\n",
      "epoch: 717 loss: [0.01476224]\n",
      "epoch: 718 loss: [0.01476955]\n",
      "epoch: 719 loss: [0.01477681]\n",
      "epoch: 720 loss: [0.01478403]\n",
      "epoch: 721 loss: [0.0147912]\n",
      "epoch: 722 loss: [0.01479833]\n",
      "epoch: 723 loss: [0.01480542]\n",
      "epoch: 724 loss: [0.01481246]\n",
      "epoch: 725 loss: [0.01481946]\n",
      "epoch: 726 loss: [0.01482642]\n",
      "epoch: 727 loss: [0.01483334]\n",
      "epoch: 728 loss: [0.01484021]\n",
      "epoch: 729 loss: [0.01484705]\n",
      "epoch: 730 loss: [0.01485384]\n",
      "epoch: 731 loss: [0.01486059]\n",
      "epoch: 732 loss: [0.01486729]\n",
      "epoch: 733 loss: [0.01487396]\n",
      "epoch: 734 loss: [0.01488059]\n",
      "epoch: 735 loss: [0.01488717]\n",
      "epoch: 736 loss: [0.01489372]\n",
      "epoch: 737 loss: [0.01490023]\n",
      "epoch: 738 loss: [0.01490669]\n",
      "epoch: 739 loss: [0.01491312]\n",
      "epoch: 740 loss: [0.01491951]\n",
      "epoch: 741 loss: [0.01492586]\n",
      "epoch: 742 loss: [0.01493217]\n",
      "epoch: 743 loss: [0.01493844]\n",
      "epoch: 744 loss: [0.01494467]\n",
      "epoch: 745 loss: [0.01495087]\n",
      "epoch: 746 loss: [0.01495703]\n",
      "epoch: 747 loss: [0.01496315]\n",
      "epoch: 748 loss: [0.01496923]\n",
      "epoch: 749 loss: [0.01497527]\n",
      "epoch: 750 loss: [0.01498128]\n",
      "epoch: 751 loss: [0.01498725]\n",
      "epoch: 752 loss: [0.01499319]\n",
      "epoch: 753 loss: [0.01499909]\n",
      "epoch: 754 loss: [0.01500495]\n",
      "epoch: 755 loss: [0.01501078]\n",
      "epoch: 756 loss: [0.01501657]\n",
      "epoch: 757 loss: [0.01502232]\n",
      "epoch: 758 loss: [0.01502804]\n",
      "epoch: 759 loss: [0.01503373]\n",
      "epoch: 760 loss: [0.01503938]\n",
      "epoch: 761 loss: [0.015045]\n",
      "epoch: 762 loss: [0.01505058]\n",
      "epoch: 763 loss: [0.01505613]\n",
      "epoch: 764 loss: [0.01506164]\n",
      "epoch: 765 loss: [0.01506712]\n",
      "epoch: 766 loss: [0.01507256]\n",
      "epoch: 767 loss: [0.01507798]\n",
      "epoch: 768 loss: [0.01508336]\n",
      "epoch: 769 loss: [0.0150887]\n",
      "epoch: 770 loss: [0.01509402]\n",
      "epoch: 771 loss: [0.0150993]\n",
      "epoch: 772 loss: [0.01510455]\n",
      "epoch: 773 loss: [0.01510976]\n",
      "epoch: 774 loss: [0.01511495]\n",
      "epoch: 775 loss: [0.0151201]\n",
      "epoch: 776 loss: [0.01512522]\n",
      "epoch: 777 loss: [0.01513031]\n",
      "epoch: 778 loss: [0.01513537]\n",
      "epoch: 779 loss: [0.01514039]\n",
      "epoch: 780 loss: [0.01514539]\n",
      "epoch: 781 loss: [0.01515035]\n",
      "epoch: 782 loss: [0.01515529]\n",
      "epoch: 783 loss: [0.01516019]\n",
      "epoch: 784 loss: [0.01516507]\n",
      "epoch: 785 loss: [0.01516991]\n",
      "epoch: 786 loss: [0.01517473]\n",
      "epoch: 787 loss: [0.01517951]\n",
      "epoch: 788 loss: [0.01518427]\n",
      "epoch: 789 loss: [0.01518899]\n",
      "epoch: 790 loss: [0.01519369]\n",
      "epoch: 791 loss: [0.01519836]\n",
      "epoch: 792 loss: [0.015203]\n",
      "epoch: 793 loss: [0.01520761]\n",
      "epoch: 794 loss: [0.01521219]\n",
      "epoch: 795 loss: [0.01521674]\n",
      "epoch: 796 loss: [0.01522127]\n",
      "epoch: 797 loss: [0.01522577]\n",
      "epoch: 798 loss: [0.01523024]\n",
      "epoch: 799 loss: [0.01523468]\n",
      "epoch: 800 loss: [0.0152391]\n",
      "epoch: 801 loss: [0.01524348]\n",
      "epoch: 802 loss: [0.01524785]\n",
      "epoch: 803 loss: [0.01525218]\n",
      "epoch: 804 loss: [0.01525649]\n",
      "epoch: 805 loss: [0.01526077]\n",
      "epoch: 806 loss: [0.01526502]\n",
      "epoch: 807 loss: [0.01526925]\n",
      "epoch: 808 loss: [0.01527345]\n",
      "epoch: 809 loss: [0.01527763]\n",
      "epoch: 810 loss: [0.01528178]\n",
      "epoch: 811 loss: [0.0152859]\n",
      "epoch: 812 loss: [0.01529]\n",
      "epoch: 813 loss: [0.01529408]\n",
      "epoch: 814 loss: [0.01529812]\n",
      "epoch: 815 loss: [0.01530215]\n",
      "epoch: 816 loss: [0.01530615]\n",
      "epoch: 817 loss: [0.01531012]\n",
      "epoch: 818 loss: [0.01531407]\n",
      "epoch: 819 loss: [0.015318]\n",
      "epoch: 820 loss: [0.0153219]\n",
      "epoch: 821 loss: [0.01532577]\n",
      "epoch: 822 loss: [0.01532962]\n",
      "epoch: 823 loss: [0.01533345]\n",
      "epoch: 824 loss: [0.01533726]\n",
      "epoch: 825 loss: [0.01534104]\n",
      "epoch: 826 loss: [0.0153448]\n",
      "epoch: 827 loss: [0.01534853]\n",
      "epoch: 828 loss: [0.01535224]\n",
      "epoch: 829 loss: [0.01535593]\n",
      "epoch: 830 loss: [0.0153596]\n",
      "epoch: 831 loss: [0.01536324]\n",
      "epoch: 832 loss: [0.01536686]\n",
      "epoch: 833 loss: [0.01537046]\n",
      "epoch: 834 loss: [0.01537404]\n",
      "epoch: 835 loss: [0.01537759]\n",
      "epoch: 836 loss: [0.01538112]\n",
      "epoch: 837 loss: [0.01538463]\n",
      "epoch: 838 loss: [0.01538812]\n",
      "epoch: 839 loss: [0.01539159]\n",
      "epoch: 840 loss: [0.01539503]\n",
      "epoch: 841 loss: [0.01539846]\n",
      "epoch: 842 loss: [0.01540186]\n",
      "epoch: 843 loss: [0.01540524]\n",
      "epoch: 844 loss: [0.0154086]\n",
      "epoch: 845 loss: [0.01541194]\n",
      "epoch: 846 loss: [0.01541526]\n",
      "epoch: 847 loss: [0.01541856]\n",
      "epoch: 848 loss: [0.01542183]\n",
      "epoch: 849 loss: [0.01542509]\n",
      "epoch: 850 loss: [0.01542833]\n",
      "epoch: 851 loss: [0.01543154]\n",
      "epoch: 852 loss: [0.01543474]\n",
      "epoch: 853 loss: [0.01543792]\n",
      "epoch: 854 loss: [0.01544108]\n",
      "epoch: 855 loss: [0.01544421]\n",
      "epoch: 856 loss: [0.01544733]\n",
      "epoch: 857 loss: [0.01545043]\n",
      "epoch: 858 loss: [0.01545351]\n",
      "epoch: 859 loss: [0.01545657]\n",
      "epoch: 860 loss: [0.01545961]\n",
      "epoch: 861 loss: [0.01546264]\n",
      "epoch: 862 loss: [0.01546564]\n",
      "epoch: 863 loss: [0.01546862]\n",
      "epoch: 864 loss: [0.01547159]\n",
      "epoch: 865 loss: [0.01547454]\n",
      "epoch: 866 loss: [0.01547747]\n",
      "epoch: 867 loss: [0.01548038]\n",
      "epoch: 868 loss: [0.01548327]\n",
      "epoch: 869 loss: [0.01548615]\n",
      "epoch: 870 loss: [0.01548901]\n",
      "epoch: 871 loss: [0.01549185]\n",
      "epoch: 872 loss: [0.01549467]\n",
      "epoch: 873 loss: [0.01549747]\n",
      "epoch: 874 loss: [0.01550026]\n",
      "epoch: 875 loss: [0.01550303]\n",
      "epoch: 876 loss: [0.01550578]\n",
      "epoch: 877 loss: [0.01550852]\n",
      "epoch: 878 loss: [0.01551124]\n",
      "epoch: 879 loss: [0.01551394]\n",
      "epoch: 880 loss: [0.01551662]\n",
      "epoch: 881 loss: [0.01551929]\n",
      "epoch: 882 loss: [0.01552194]\n",
      "epoch: 883 loss: [0.01552458]\n",
      "epoch: 884 loss: [0.0155272]\n",
      "epoch: 885 loss: [0.0155298]\n",
      "epoch: 886 loss: [0.01553239]\n",
      "epoch: 887 loss: [0.01553495]\n",
      "epoch: 888 loss: [0.01553751]\n",
      "epoch: 889 loss: [0.01554005]\n",
      "epoch: 890 loss: [0.01554257]\n",
      "epoch: 891 loss: [0.01554508]\n",
      "epoch: 892 loss: [0.01554757]\n",
      "epoch: 893 loss: [0.01555004]\n",
      "epoch: 894 loss: [0.0155525]\n",
      "epoch: 895 loss: [0.01555495]\n",
      "epoch: 896 loss: [0.01555737]\n",
      "epoch: 897 loss: [0.01555979]\n",
      "epoch: 898 loss: [0.01556219]\n",
      "epoch: 899 loss: [0.01556457]\n",
      "epoch: 900 loss: [0.01556694]\n",
      "epoch: 901 loss: [0.0155693]\n",
      "epoch: 902 loss: [0.01557164]\n",
      "epoch: 903 loss: [0.01557396]\n",
      "epoch: 904 loss: [0.01557627]\n",
      "epoch: 905 loss: [0.01557857]\n",
      "epoch: 906 loss: [0.01558085]\n",
      "epoch: 907 loss: [0.01558312]\n",
      "epoch: 908 loss: [0.01558537]\n",
      "epoch: 909 loss: [0.01558761]\n",
      "epoch: 910 loss: [0.01558984]\n",
      "epoch: 911 loss: [0.01559205]\n",
      "epoch: 912 loss: [0.01559425]\n",
      "epoch: 913 loss: [0.01559643]\n",
      "epoch: 914 loss: [0.0155986]\n",
      "epoch: 915 loss: [0.01560076]\n",
      "epoch: 916 loss: [0.0156029]\n",
      "epoch: 917 loss: [0.01560503]\n",
      "epoch: 918 loss: [0.01560715]\n",
      "epoch: 919 loss: [0.01560925]\n",
      "epoch: 920 loss: [0.01561134]\n",
      "epoch: 921 loss: [0.01561342]\n",
      "epoch: 922 loss: [0.01561548]\n",
      "epoch: 923 loss: [0.01561753]\n",
      "epoch: 924 loss: [0.01561957]\n",
      "epoch: 925 loss: [0.0156216]\n",
      "epoch: 926 loss: [0.01562361]\n",
      "epoch: 927 loss: [0.01562561]\n",
      "epoch: 928 loss: [0.0156276]\n",
      "epoch: 929 loss: [0.01562958]\n",
      "epoch: 930 loss: [0.01563154]\n",
      "epoch: 931 loss: [0.01563349]\n",
      "epoch: 932 loss: [0.01563543]\n",
      "epoch: 933 loss: [0.01563736]\n",
      "epoch: 934 loss: [0.01563927]\n",
      "epoch: 935 loss: [0.01564118]\n",
      "epoch: 936 loss: [0.01564307]\n",
      "epoch: 937 loss: [0.01564495]\n",
      "epoch: 938 loss: [0.01564681]\n",
      "epoch: 939 loss: [0.01564867]\n",
      "epoch: 940 loss: [0.01565051]\n",
      "epoch: 941 loss: [0.01565235]\n",
      "epoch: 942 loss: [0.01565417]\n",
      "epoch: 943 loss: [0.01565598]\n",
      "epoch: 944 loss: [0.01565777]\n",
      "epoch: 945 loss: [0.01565956]\n",
      "epoch: 946 loss: [0.01566134]\n",
      "epoch: 947 loss: [0.0156631]\n",
      "epoch: 948 loss: [0.01566486]\n",
      "epoch: 949 loss: [0.0156666]\n",
      "epoch: 950 loss: [0.01566833]\n",
      "epoch: 951 loss: [0.01567005]\n",
      "epoch: 952 loss: [0.01567176]\n",
      "epoch: 953 loss: [0.01567346]\n",
      "epoch: 954 loss: [0.01567515]\n",
      "epoch: 955 loss: [0.01567683]\n",
      "epoch: 956 loss: [0.0156785]\n",
      "epoch: 957 loss: [0.01568016]\n",
      "epoch: 958 loss: [0.0156818]\n",
      "epoch: 959 loss: [0.01568344]\n",
      "epoch: 960 loss: [0.01568507]\n",
      "epoch: 961 loss: [0.01568668]\n",
      "epoch: 962 loss: [0.01568829]\n",
      "epoch: 963 loss: [0.01568989]\n",
      "epoch: 964 loss: [0.01569147]\n",
      "epoch: 965 loss: [0.01569305]\n",
      "epoch: 966 loss: [0.01569461]\n",
      "epoch: 967 loss: [0.01569617]\n",
      "epoch: 968 loss: [0.01569772]\n",
      "epoch: 969 loss: [0.01569925]\n",
      "epoch: 970 loss: [0.01570078]\n",
      "epoch: 971 loss: [0.0157023]\n",
      "epoch: 972 loss: [0.01570381]\n",
      "epoch: 973 loss: [0.01570531]\n",
      "epoch: 974 loss: [0.0157068]\n",
      "epoch: 975 loss: [0.01570828]\n",
      "epoch: 976 loss: [0.01570975]\n",
      "epoch: 977 loss: [0.01571121]\n",
      "epoch: 978 loss: [0.01571266]\n",
      "epoch: 979 loss: [0.01571411]\n",
      "epoch: 980 loss: [0.01571554]\n",
      "epoch: 981 loss: [0.01571697]\n",
      "epoch: 982 loss: [0.01571838]\n",
      "epoch: 983 loss: [0.01571979]\n",
      "epoch: 984 loss: [0.01572119]\n",
      "epoch: 985 loss: [0.01572258]\n",
      "epoch: 986 loss: [0.01572396]\n",
      "epoch: 987 loss: [0.01572533]\n",
      "epoch: 988 loss: [0.0157267]\n",
      "epoch: 989 loss: [0.01572805]\n",
      "epoch: 990 loss: [0.0157294]\n",
      "epoch: 991 loss: [0.01573074]\n",
      "epoch: 992 loss: [0.01573207]\n",
      "epoch: 993 loss: [0.01573339]\n",
      "epoch: 994 loss: [0.01573471]\n",
      "epoch: 995 loss: [0.01573601]\n",
      "epoch: 996 loss: [0.01573731]\n",
      "epoch: 997 loss: [0.0157386]\n",
      "epoch: 998 loss: [0.01573988]\n",
      "epoch: 999 loss: [0.01574115]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0.25485312262597065 err: [-0.25485312]\n",
      "[0. 1.] [1.] -> 0.7503139148327709 err: [0.24968609]\n",
      "[1. 0.] [1.] -> 0.7479388771878912 err: [0.25206112]\n",
      "[1. 1.] [1.] -> 1.2433996693946916 err: [-0.24339967]\n"
     ]
    }
   ],
   "source": [
    "# 生の出力（活性化なしで）\n",
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [1.] -> 1 err: [0.]\n",
      "[1. 0.] [1.] -> 1 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "# ステップ関数を適用した後：\n",
    "print_results(model, step_activation, x_list, y_true)"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
