{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習\n",
    "\n",
    "ステップ１で作った関数をもう一度実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# numpyを導入\n",
    "import numpy as np\n",
    "\n",
    "# モデル作成\n",
    "def create_model():    \n",
    "    model = {\n",
    "        # 荷重を -5 ~ 5 の乱数で初期化\n",
    "        \"weights\" : np.random.uniform(-5, 5, 2),    \n",
    "        # バイアスも！\n",
    "        \"bias\"    : np.random.uniform(-5, 5, 1)}\n",
    "    return model\n",
    "\n",
    "# 推論\n",
    "def predict(model, activation, x):\n",
    "    \n",
    "    # 足し算を計算し…\n",
    "    y = model[\"weights\"][0] * x[0] + model[\"weights\"][1] * x[1] + model[\"bias\"]\n",
    "    \n",
    "    # 活性化で処理し、その結果を返す\n",
    "    y = activation(y)\n",
    "    return y\n",
    "\n",
    "# 線形活性化関数\n",
    "def linear_activation(x):\n",
    "    return x\n",
    "\n",
    "# ステップ活性化関数\n",
    "def step_activation(x):\n",
    "    if x >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差を計算\n",
    "\n",
    "正しい答え（ラベル）と推論した答えの差分を用い、学習させる。\n",
    "\n",
    "まず、「AND」のラベルを準備しよう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# 入力\n",
    "x_list = np.array([\n",
    "    [0, 0], \n",
    "    [0, 1], \n",
    "    [1, 0], \n",
    "    [1, 1]\n",
    "], dtype = float)\n",
    "print(x_list.shape)\n",
    "\n",
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [0], \n",
    "    [0], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差（損失）関数を実装しよう。\n",
    "\n",
    "課題により、適切な関数を使うべきが、今回の入門課題はただの「差分」にしよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "def error(y_true, y_pred):\n",
    "    return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示する\n",
    "def print_results(model, activation, x_list, y_true):\n",
    "    \n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    for i in range(data_size):\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        err = error(y_t, y_p)\n",
    "        print(x, y_t, \"->\", y_p, \"err:\", err)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> [0.49003829] err: [-0.49003829]\n",
      "[0. 1.] [0.] -> [-1.20464417] err: [1.20464417]\n",
      "[1. 0.] [0.] -> [-0.29942395] err: [0.29942395]\n",
      "[1. 1.] [1.] -> [-1.99410641] err: [2.99410641]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [0.] -> 0 err: [0.]\n",
      "[1. 0.] [0.] -> 0 err: [0.]\n",
      "[1. 1.] [1.] -> 0 err: [1.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "誤差にて、荷重を調整しよう。ただ、「入力」は「０」であると、出力に影響がないため、入力は「１」のときだけに荷重を調整する、つまり：\n",
    "\n",
    "$$ w_i' = w_i + x_i \\cdot error(y_{true}, y_{pred}) $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 荷重を更新する関数\n",
    "def update_weight(w, x, err):\n",
    "    return w + x * err\n",
    "\n",
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "    \n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "        \n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        \n",
    "        # 誤差を計算\n",
    "        err  = error(y_t, y_p)\n",
    "        mse += err * err\n",
    "            \n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "        \n",
    "        w0   = update_weight(w0, x[0], err)\n",
    "        w1   = update_weight(w1, x[1], err)\n",
    "        bias = update_weight(bias, 1 , err)\n",
    "        \n",
    "        model[\"weights\"][0] = w0[0]\n",
    "        model[\"weights\"][1] = w1[0]\n",
    "        model[\"bias\"] = bias[0]\n",
    "    \n",
    "    #誤差（損失）としては、平均値を返す\n",
    "    return mse / data_size        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: [1.89034347]\n"
     ]
    }
   ],
   "source": [
    "loss = fit_single_step(model, linear_activation, x_list, y_true)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 2.6946824578983133 err: [-2.69468246]\n",
      "[0. 1.] [0.] -> 4.599902677852856 err: [-4.59990268]\n",
      "[1. 0.] [0.] -> 2.9052202199545425 err: [-2.90522022]\n",
      "[1. 1.] [1.] -> 4.810440439909085 err: [-3.81044044]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, activation, x_list, y_true, epochs):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        loss = fit_single_step(model, activation, x_list, y_true)\n",
    "        print(\"epoch:\", i, \"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [3.5614275]\n",
      "epoch: 1 loss: [3.34304399]\n",
      "epoch: 2 loss: [2.97201392]\n",
      "epoch: 3 loss: [6.8535331]\n",
      "epoch: 4 loss: [4.38260034]\n",
      "epoch: 5 loss: [12.3640222]\n",
      "epoch: 6 loss: [7.79318677]\n",
      "epoch: 7 loss: [19.8745113]\n",
      "epoch: 8 loss: [13.20377319]\n",
      "epoch: 9 loss: [29.3850004]\n",
      "epoch: 10 loss: [20.61435961]\n",
      "epoch: 11 loss: [40.8954895]\n",
      "epoch: 12 loss: [30.02494604]\n",
      "epoch: 13 loss: [54.4059786]\n",
      "epoch: 14 loss: [41.43553246]\n",
      "epoch: 15 loss: [69.9164677]\n",
      "epoch: 16 loss: [54.84611888]\n",
      "epoch: 17 loss: [87.4269568]\n",
      "epoch: 18 loss: [70.2567053]\n",
      "epoch: 19 loss: [106.9374459]\n",
      "epoch: 20 loss: [87.66729173]\n",
      "epoch: 21 loss: [128.447935]\n",
      "epoch: 22 loss: [107.07787815]\n",
      "epoch: 23 loss: [151.95842411]\n",
      "epoch: 24 loss: [128.48846457]\n",
      "epoch: 25 loss: [177.46891321]\n",
      "epoch: 26 loss: [151.899051]\n",
      "epoch: 27 loss: [204.97940231]\n",
      "epoch: 28 loss: [177.30963742]\n",
      "epoch: 29 loss: [234.48989141]\n",
      "epoch: 30 loss: [204.72022384]\n",
      "epoch: 31 loss: [266.00038051]\n",
      "epoch: 32 loss: [234.13081027]\n",
      "epoch: 33 loss: [299.51086961]\n",
      "epoch: 34 loss: [265.54139669]\n",
      "epoch: 35 loss: [335.02135871]\n",
      "epoch: 36 loss: [298.95198311]\n",
      "epoch: 37 loss: [372.53184781]\n",
      "epoch: 38 loss: [334.36256954]\n",
      "epoch: 39 loss: [412.04233691]\n",
      "epoch: 40 loss: [371.77315596]\n",
      "epoch: 41 loss: [453.55282601]\n",
      "epoch: 42 loss: [411.18374238]\n",
      "epoch: 43 loss: [497.06331511]\n",
      "epoch: 44 loss: [452.59432881]\n",
      "epoch: 45 loss: [542.57380422]\n",
      "epoch: 46 loss: [496.00491523]\n",
      "epoch: 47 loss: [590.08429332]\n",
      "epoch: 48 loss: [541.41550165]\n",
      "epoch: 49 loss: [639.59478242]\n",
      "epoch: 50 loss: [588.82608807]\n",
      "epoch: 51 loss: [691.10527152]\n",
      "epoch: 52 loss: [638.2366745]\n",
      "epoch: 53 loss: [744.61576062]\n",
      "epoch: 54 loss: [689.64726092]\n",
      "epoch: 55 loss: [800.12624972]\n",
      "epoch: 56 loss: [743.05784734]\n",
      "epoch: 57 loss: [857.63673882]\n",
      "epoch: 58 loss: [798.46843377]\n",
      "epoch: 59 loss: [917.14722792]\n",
      "epoch: 60 loss: [855.87902019]\n",
      "epoch: 61 loss: [978.65771702]\n",
      "epoch: 62 loss: [915.28960661]\n",
      "epoch: 63 loss: [1042.16820612]\n",
      "epoch: 64 loss: [976.70019304]\n",
      "epoch: 65 loss: [1107.67869523]\n",
      "epoch: 66 loss: [1040.11077946]\n",
      "epoch: 67 loss: [1175.18918433]\n",
      "epoch: 68 loss: [1105.52136588]\n",
      "epoch: 69 loss: [1244.69967343]\n",
      "epoch: 70 loss: [1172.93195231]\n",
      "epoch: 71 loss: [1316.21016253]\n",
      "epoch: 72 loss: [1242.34253873]\n",
      "epoch: 73 loss: [1389.72065163]\n",
      "epoch: 74 loss: [1313.75312515]\n",
      "epoch: 75 loss: [1465.23114073]\n",
      "epoch: 76 loss: [1387.16371158]\n",
      "epoch: 77 loss: [1542.74162983]\n",
      "epoch: 78 loss: [1462.574298]\n",
      "epoch: 79 loss: [1622.25211893]\n",
      "epoch: 80 loss: [1539.98488442]\n",
      "epoch: 81 loss: [1703.76260803]\n",
      "epoch: 82 loss: [1619.39547084]\n",
      "epoch: 83 loss: [1787.27309713]\n",
      "epoch: 84 loss: [1700.80605727]\n",
      "epoch: 85 loss: [1872.78358624]\n",
      "epoch: 86 loss: [1784.21664369]\n",
      "epoch: 87 loss: [1960.29407534]\n",
      "epoch: 88 loss: [1869.62723011]\n",
      "epoch: 89 loss: [2049.80456444]\n",
      "epoch: 90 loss: [1957.03781654]\n",
      "epoch: 91 loss: [2141.31505354]\n",
      "epoch: 92 loss: [2046.44840296]\n",
      "epoch: 93 loss: [2234.82554264]\n",
      "epoch: 94 loss: [2137.85898938]\n",
      "epoch: 95 loss: [2330.33603174]\n",
      "epoch: 96 loss: [2231.26957581]\n",
      "epoch: 97 loss: [2427.84652084]\n",
      "epoch: 98 loss: [2326.68016223]\n",
      "epoch: 99 loss: [2527.35700994]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習率とは\n",
    "\n",
    "上記のように、差分だけを直そうとすると、平均的な誤差がお大きくなってしまう。その理由は、確か、勾配の方向が正しいが、ステップが大きいすぎる。つまり、最適な数値から大幅に超えてしまい、段々離れてしまう。\n",
    "\n",
    "「学習率」という係数で、ステップの大きさを小さくし、少しずつ最適な数値に近づくようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 学習は「fit」とよく言われる\n",
    "def fit_single_step(model, activation, x_list, y_true):\n",
    "\n",
    "    # データセットのサイズは入力のshapeから求める\n",
    "    data_size = x_list.shape[0]\n",
    "    \n",
    "    # 誤差の平均\n",
    "    mse = 0\n",
    "    \n",
    "    # さて、１個ずつを処理しよう\n",
    "    for i in range(data_size):\n",
    "        \n",
    "        # 推論\n",
    "        x   = x_list[i]\n",
    "        y_t = y_true[i]\n",
    "        y_p = predict(model, activation, x)\n",
    "        \n",
    "        # 誤差を計算\n",
    "        err = error(y_t, y_p)\n",
    "        mse = err * err\n",
    "        \n",
    "        # 学習率\n",
    "        learning_rate = 0.01\n",
    "\n",
    "        # 荷重を更新\n",
    "        w0   = model[\"weights\"][0]\n",
    "        w1   = model[\"weights\"][1]\n",
    "        bias = model[\"bias\"]\n",
    "        \n",
    "        w0   = update_weight(w0, x[0], err * learning_rate)\n",
    "        w1   = update_weight(w1, x[1], err * learning_rate)\n",
    "        bias = update_weight(bias, 1 , err * learning_rate)\n",
    "        \n",
    "        model[\"weights\"][0] = w0[0]\n",
    "        model[\"weights\"][1] = w1[0]\n",
    "        model[\"bias\"] = bias[0]\n",
    "        \n",
    "    return mse / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [2658.20074063]\n",
      "epoch: 1 loss: [2471.88919735]\n",
      "epoch: 2 loss: [2302.52106815]\n",
      "epoch: 3 loss: [2148.38300169]\n",
      "epoch: 4 loss: [2007.94651139]\n",
      "epoch: 5 loss: [1879.84713479]\n",
      "epoch: 6 loss: [1762.86601016]\n",
      "epoch: 7 loss: [1655.91358519]\n",
      "epoch: 8 loss: [1558.0152062]\n",
      "epoch: 9 loss: [1468.29836673]\n",
      "epoch: 10 loss: [1385.98142038]\n",
      "epoch: 11 loss: [1310.3635861]\n",
      "epoch: 12 loss: [1240.81609463]\n",
      "epoch: 13 loss: [1176.77434261]\n",
      "epoch: 14 loss: [1117.73093676]\n",
      "epoch: 15 loss: [1063.22952464]\n",
      "epoch: 16 loss: [1012.85932031]\n",
      "epoch: 17 loss: [966.25024461]\n",
      "epoch: 18 loss: [923.06860877]\n",
      "epoch: 19 loss: [883.01327875]\n",
      "epoch: 20 loss: [845.81226495]\n",
      "epoch: 21 loss: [811.2196885]\n",
      "epoch: 22 loss: [779.01308094]\n",
      "epoch: 23 loss: [748.99097943]\n",
      "epoch: 24 loss: [720.97078362]\n",
      "epoch: 25 loss: [694.78684479]\n",
      "epoch: 26 loss: [670.28876076]\n",
      "epoch: 27 loss: [647.33985359]\n",
      "epoch: 28 loss: [625.81580947]\n",
      "epoch: 29 loss: [605.6034627]\n",
      "epoch: 30 loss: [586.59970771]\n",
      "epoch: 31 loss: [568.71052497]\n",
      "epoch: 32 loss: [551.85010823]\n",
      "epoch: 33 loss: [535.94008183]\n",
      "epoch: 34 loss: [520.90879847]\n",
      "epoch: 35 loss: [506.69070846]\n",
      "epoch: 36 loss: [493.22579283]\n",
      "epoch: 37 loss: [480.45905344]\n",
      "epoch: 38 loss: [468.34005391]\n",
      "epoch: 39 loss: [456.82250606]\n",
      "epoch: 40 loss: [445.86389691]\n",
      "epoch: 41 loss: [435.42515212]\n",
      "epoch: 42 loss: [425.47033197]\n",
      "epoch: 43 loss: [415.9663565]\n",
      "epoch: 44 loss: [406.88275685]\n",
      "epoch: 45 loss: [398.19145008]\n",
      "epoch: 46 loss: [389.86653506]\n",
      "epoch: 47 loss: [381.88410738]\n",
      "epoch: 48 loss: [374.22209125]\n",
      "epoch: 49 loss: [366.86008681]\n",
      "epoch: 50 loss: [359.77923127]\n",
      "epoch: 51 loss: [352.96207253]\n",
      "epoch: 52 loss: [346.39245404]\n",
      "epoch: 53 loss: [340.05540993]\n",
      "epoch: 54 loss: [333.93706919]\n",
      "epoch: 55 loss: [328.02456833]\n",
      "epoch: 56 loss: [322.3059715]\n",
      "epoch: 57 loss: [316.77019745]\n",
      "epoch: 58 loss: [311.40695274]\n",
      "epoch: 59 loss: [306.20667049]\n",
      "epoch: 60 loss: [301.16045435]\n",
      "epoch: 61 loss: [296.26002706]\n",
      "epoch: 62 loss: [291.49768326]\n",
      "epoch: 63 loss: [286.86624618]\n",
      "epoch: 64 loss: [282.35902779]\n",
      "epoch: 65 loss: [277.96979224]\n",
      "epoch: 66 loss: [273.69272214]\n",
      "epoch: 67 loss: [269.52238763]\n",
      "epoch: 68 loss: [265.45371779]\n",
      "epoch: 69 loss: [261.48197432]\n",
      "epoch: 70 loss: [257.60272735]\n",
      "epoch: 71 loss: [253.81183301]\n",
      "epoch: 72 loss: [250.10541282]\n",
      "epoch: 73 loss: [246.47983459]\n",
      "epoch: 74 loss: [242.93169486]\n",
      "epoch: 75 loss: [239.45780259]\n",
      "epoch: 76 loss: [236.0551641]\n",
      "epoch: 77 loss: [232.7209692]\n",
      "epoch: 78 loss: [229.45257826]\n",
      "epoch: 79 loss: [226.24751033]\n",
      "epoch: 80 loss: [223.10343207]\n",
      "epoch: 81 loss: [220.01814755]\n",
      "epoch: 82 loss: [216.98958874]\n",
      "epoch: 83 loss: [214.01580678]\n",
      "epoch: 84 loss: [211.09496378]\n",
      "epoch: 85 loss: [208.2253253]\n",
      "epoch: 86 loss: [205.40525332]\n",
      "epoch: 87 loss: [202.63319979]\n",
      "epoch: 88 loss: [199.90770049]\n",
      "epoch: 89 loss: [197.22736954]\n",
      "epoch: 90 loss: [194.5908941]\n",
      "epoch: 91 loss: [191.99702959]\n",
      "epoch: 92 loss: [189.44459517]\n",
      "epoch: 93 loss: [186.93246957]\n",
      "epoch: 94 loss: [184.45958721]\n",
      "epoch: 95 loss: [182.02493457]\n",
      "epoch: 96 loss: [179.62754686]\n",
      "epoch: 97 loss: [177.26650486]\n",
      "epoch: 98 loss: [174.94093206]\n",
      "epoch: 99 loss: [172.64999189]\n",
      "epoch: 100 loss: [170.39288529]\n",
      "epoch: 101 loss: [168.16884826]\n",
      "epoch: 102 loss: [165.97714977]\n",
      "epoch: 103 loss: [163.81708967]\n",
      "epoch: 104 loss: [161.68799682]\n",
      "epoch: 105 loss: [159.58922732]\n",
      "epoch: 106 loss: [157.52016286]\n",
      "epoch: 107 loss: [155.4802092]\n",
      "epoch: 108 loss: [153.46879471]\n",
      "epoch: 109 loss: [151.4853691]\n",
      "epoch: 110 loss: [149.52940209]\n",
      "epoch: 111 loss: [147.60038234]\n",
      "epoch: 112 loss: [145.6978163]\n",
      "epoch: 113 loss: [143.82122722]\n",
      "epoch: 114 loss: [141.97015425]\n",
      "epoch: 115 loss: [140.1441515]\n",
      "epoch: 116 loss: [138.34278723]\n",
      "epoch: 117 loss: [136.56564313]\n",
      "epoch: 118 loss: [134.81231355]\n",
      "epoch: 119 loss: [133.08240484]\n",
      "epoch: 120 loss: [131.37553475]\n",
      "epoch: 121 loss: [129.69133182]\n",
      "epoch: 122 loss: [128.02943484]\n",
      "epoch: 123 loss: [126.38949235]\n",
      "epoch: 124 loss: [124.77116212]\n",
      "epoch: 125 loss: [123.17411076]\n",
      "epoch: 126 loss: [121.59801324]\n",
      "epoch: 127 loss: [120.04255256]\n",
      "epoch: 128 loss: [118.50741931]\n",
      "epoch: 129 loss: [116.99231139]\n",
      "epoch: 130 loss: [115.49693364]\n",
      "epoch: 131 loss: [114.02099756]\n",
      "epoch: 132 loss: [112.56422101]\n",
      "epoch: 133 loss: [111.12632795]\n",
      "epoch: 134 loss: [109.7070482]\n",
      "epoch: 135 loss: [108.30611719]\n",
      "epoch: 136 loss: [106.92327571]\n",
      "epoch: 137 loss: [105.55826977]\n",
      "epoch: 138 loss: [104.21085034]\n",
      "epoch: 139 loss: [102.88077319]\n",
      "epoch: 140 loss: [101.56779871]\n",
      "epoch: 141 loss: [100.27169177]\n",
      "epoch: 142 loss: [98.99222151]\n",
      "epoch: 143 loss: [97.72916125]\n",
      "epoch: 144 loss: [96.48228834]\n",
      "epoch: 145 loss: [95.25138399]\n",
      "epoch: 146 loss: [94.03623318]\n",
      "epoch: 147 loss: [92.83662457]\n",
      "epoch: 148 loss: [91.65235032]\n",
      "epoch: 149 loss: [90.48320604]\n",
      "epoch: 150 loss: [89.32899068]\n",
      "epoch: 151 loss: [88.18950641]\n",
      "epoch: 152 loss: [87.06455857]\n",
      "epoch: 153 loss: [85.95395556]\n",
      "epoch: 154 loss: [84.85750876]\n",
      "epoch: 155 loss: [83.77503246]\n",
      "epoch: 156 loss: [82.70634378]\n",
      "epoch: 157 loss: [81.65126262]\n",
      "epoch: 158 loss: [80.60961157]\n",
      "epoch: 159 loss: [79.58121584]\n",
      "epoch: 160 loss: [78.56590324]\n",
      "epoch: 161 loss: [77.56350408]\n",
      "epoch: 162 loss: [76.57385114]\n",
      "epoch: 163 loss: [75.59677958]\n",
      "epoch: 164 loss: [74.63212696]\n",
      "epoch: 165 loss: [73.67973311]\n",
      "epoch: 166 loss: [72.73944015]\n",
      "epoch: 167 loss: [71.81109238]\n",
      "epoch: 168 loss: [70.8945363]\n",
      "epoch: 169 loss: [69.98962054]\n",
      "epoch: 170 loss: [69.0961958]\n",
      "epoch: 171 loss: [68.21411485]\n",
      "epoch: 172 loss: [67.34323245]\n",
      "epoch: 173 loss: [66.48340536]\n",
      "epoch: 174 loss: [65.63449227]\n",
      "epoch: 175 loss: [64.79635377]\n",
      "epoch: 176 loss: [63.96885235]\n",
      "epoch: 177 loss: [63.15185232]\n",
      "epoch: 178 loss: [62.3452198]\n",
      "epoch: 179 loss: [61.54882272]\n",
      "epoch: 180 loss: [60.76253073]\n",
      "epoch: 181 loss: [59.98621522]\n",
      "epoch: 182 loss: [59.21974928]\n",
      "epoch: 183 loss: [58.46300767]\n",
      "epoch: 184 loss: [57.71586678]\n",
      "epoch: 185 loss: [56.97820465]\n",
      "epoch: 186 loss: [56.24990088]\n",
      "epoch: 187 loss: [55.53083666]\n",
      "epoch: 188 loss: [54.82089472]\n",
      "epoch: 189 loss: [54.11995931]\n",
      "epoch: 190 loss: [53.42791621]\n",
      "epoch: 191 loss: [52.74465264]\n",
      "epoch: 192 loss: [52.0700573]\n",
      "epoch: 193 loss: [51.40402032]\n",
      "epoch: 194 loss: [50.74643325]\n",
      "epoch: 195 loss: [50.09718904]\n",
      "epoch: 196 loss: [49.45618201]\n",
      "epoch: 197 loss: [48.82330784]\n",
      "epoch: 198 loss: [48.19846354]\n",
      "epoch: 199 loss: [47.58154746]\n",
      "epoch: 200 loss: [46.97245924]\n",
      "epoch: 201 loss: [46.37109979]\n",
      "epoch: 202 loss: [45.77737132]\n",
      "epoch: 203 loss: [45.19117725]\n",
      "epoch: 204 loss: [44.61242227]\n",
      "epoch: 205 loss: [44.04101225]\n",
      "epoch: 206 loss: [43.47685428]\n",
      "epoch: 207 loss: [42.91985664]\n",
      "epoch: 208 loss: [42.36992875]\n",
      "epoch: 209 loss: [41.82698122]\n",
      "epoch: 210 loss: [41.29092576]\n",
      "epoch: 211 loss: [40.76167522]\n",
      "epoch: 212 loss: [40.23914355]\n",
      "epoch: 213 loss: [39.72324581]\n",
      "epoch: 214 loss: [39.2138981]\n",
      "epoch: 215 loss: [38.71101763]\n",
      "epoch: 216 loss: [38.21452262]\n",
      "epoch: 217 loss: [37.72433236]\n",
      "epoch: 218 loss: [37.24036713]\n",
      "epoch: 219 loss: [36.76254825]\n",
      "epoch: 220 loss: [36.29079801]\n",
      "epoch: 221 loss: [35.8250397]\n",
      "epoch: 222 loss: [35.36519757]\n",
      "epoch: 223 loss: [34.91119685]\n",
      "epoch: 224 loss: [34.46296368]\n",
      "epoch: 225 loss: [34.02042518]\n",
      "epoch: 226 loss: [33.58350934]\n",
      "epoch: 227 loss: [33.15214509]\n",
      "epoch: 228 loss: [32.72626227]\n",
      "epoch: 229 loss: [32.30579157]\n",
      "epoch: 230 loss: [31.89066459]\n",
      "epoch: 231 loss: [31.48081377]\n",
      "epoch: 232 loss: [31.07617242]\n",
      "epoch: 233 loss: [30.67667469]\n",
      "epoch: 234 loss: [30.28225555]\n",
      "epoch: 235 loss: [29.89285079]\n",
      "epoch: 236 loss: [29.50839703]\n",
      "epoch: 237 loss: [29.12883168]\n",
      "epoch: 238 loss: [28.75409293]\n",
      "epoch: 239 loss: [28.38411977]\n",
      "epoch: 240 loss: [28.01885194]\n",
      "epoch: 241 loss: [27.65822995]\n",
      "epoch: 242 loss: [27.30219505]\n",
      "epoch: 243 loss: [26.95068926]\n",
      "epoch: 244 loss: [26.60365531]\n",
      "epoch: 245 loss: [26.26103664]\n",
      "epoch: 246 loss: [25.92277743]\n",
      "epoch: 247 loss: [25.58882255]\n",
      "epoch: 248 loss: [25.25911757]\n",
      "epoch: 249 loss: [24.93360875]\n",
      "epoch: 250 loss: [24.612243]\n",
      "epoch: 251 loss: [24.29496795]\n",
      "epoch: 252 loss: [23.98173185]\n",
      "epoch: 253 loss: [23.67248361]\n",
      "epoch: 254 loss: [23.3671728]\n",
      "epoch: 255 loss: [23.06574961]\n",
      "epoch: 256 loss: [22.76816486]\n",
      "epoch: 257 loss: [22.47437001]\n",
      "epoch: 258 loss: [22.1843171]\n",
      "epoch: 259 loss: [21.8979588]\n",
      "epoch: 260 loss: [21.61524836]\n",
      "epoch: 261 loss: [21.33613964]\n",
      "epoch: 262 loss: [21.06058706]\n",
      "epoch: 263 loss: [20.78854562]\n",
      "epoch: 264 loss: [20.51997091]\n",
      "epoch: 265 loss: [20.25481905]\n",
      "epoch: 266 loss: [19.99304672]\n",
      "epoch: 267 loss: [19.73461116]\n",
      "epoch: 268 loss: [19.47947014]\n",
      "epoch: 269 loss: [19.22758197]\n",
      "epoch: 270 loss: [18.97890546]\n",
      "epoch: 271 loss: [18.73339998]\n",
      "epoch: 272 loss: [18.49102538]\n",
      "epoch: 273 loss: [18.25174203]\n",
      "epoch: 274 loss: [18.0155108]\n",
      "epoch: 275 loss: [17.78229305]\n",
      "epoch: 276 loss: [17.55205062]\n",
      "epoch: 277 loss: [17.32474585]\n",
      "epoch: 278 loss: [17.10034154]\n",
      "epoch: 279 loss: [16.87880097]\n",
      "epoch: 280 loss: [16.66008787]\n",
      "epoch: 281 loss: [16.44416644]\n",
      "epoch: 282 loss: [16.23100131]\n",
      "epoch: 283 loss: [16.0205576]\n",
      "epoch: 284 loss: [15.81280082]\n",
      "epoch: 285 loss: [15.60769695]\n",
      "epoch: 286 loss: [15.40521238]\n",
      "epoch: 287 loss: [15.20531393]\n",
      "epoch: 288 loss: [15.00796884]\n",
      "epoch: 289 loss: [14.81314477]\n",
      "epoch: 290 loss: [14.62080977]\n",
      "epoch: 291 loss: [14.43093231]\n",
      "epoch: 292 loss: [14.24348125]\n",
      "epoch: 293 loss: [14.05842584]\n",
      "epoch: 294 loss: [13.87573573]\n",
      "epoch: 295 loss: [13.69538094]\n",
      "epoch: 296 loss: [13.51733187]\n",
      "epoch: 297 loss: [13.34155931]\n",
      "epoch: 298 loss: [13.16803439]\n",
      "epoch: 299 loss: [12.99672863]\n",
      "epoch: 300 loss: [12.82761389]\n",
      "epoch: 301 loss: [12.66066241]\n",
      "epoch: 302 loss: [12.49584675]\n",
      "epoch: 303 loss: [12.33313984]\n",
      "epoch: 304 loss: [12.17251494]\n",
      "epoch: 305 loss: [12.01394565]\n",
      "epoch: 306 loss: [11.8574059]\n",
      "epoch: 307 loss: [11.70286996]\n",
      "epoch: 308 loss: [11.55031241]\n",
      "epoch: 309 loss: [11.39970816]\n",
      "epoch: 310 loss: [11.25103244]\n",
      "epoch: 311 loss: [11.10426078]\n",
      "epoch: 312 loss: [10.95936903]\n",
      "epoch: 313 loss: [10.81633334]\n",
      "epoch: 314 loss: [10.67513017]\n",
      "epoch: 315 loss: [10.53573626]\n",
      "epoch: 316 loss: [10.39812865]\n",
      "epoch: 317 loss: [10.26228469]\n",
      "epoch: 318 loss: [10.12818199]\n",
      "epoch: 319 loss: [9.99579844]\n",
      "epoch: 320 loss: [9.86511224]\n",
      "epoch: 321 loss: [9.73610184]\n",
      "epoch: 322 loss: [9.60874597]\n",
      "epoch: 323 loss: [9.48302362]\n",
      "epoch: 324 loss: [9.35891405]\n",
      "epoch: 325 loss: [9.2363968]\n",
      "epoch: 326 loss: [9.11545164]\n",
      "epoch: 327 loss: [8.99605862]\n",
      "epoch: 328 loss: [8.87819801]\n",
      "epoch: 329 loss: [8.76185037]\n",
      "epoch: 330 loss: [8.64699648]\n",
      "epoch: 331 loss: [8.53361736]\n",
      "epoch: 332 loss: [8.42169428]\n",
      "epoch: 333 loss: [8.31120875]\n",
      "epoch: 334 loss: [8.20214251]\n",
      "epoch: 335 loss: [8.09447752]\n",
      "epoch: 336 loss: [7.98819598]\n",
      "epoch: 337 loss: [7.88328031]\n",
      "epoch: 338 loss: [7.77971315]\n",
      "epoch: 339 loss: [7.67747737]\n",
      "epoch: 340 loss: [7.57655605]\n",
      "epoch: 341 loss: [7.47693248]\n",
      "epoch: 342 loss: [7.37859016]\n",
      "epoch: 343 loss: [7.28151281]\n",
      "epoch: 344 loss: [7.18568434]\n",
      "epoch: 345 loss: [7.09108888]\n",
      "epoch: 346 loss: [6.99771076]\n",
      "epoch: 347 loss: [6.90553448]\n",
      "epoch: 348 loss: [6.81454477]\n",
      "epoch: 349 loss: [6.72472653]\n",
      "epoch: 350 loss: [6.63606487]\n",
      "epoch: 351 loss: [6.54854508]\n",
      "epoch: 352 loss: [6.46215262]\n",
      "epoch: 353 loss: [6.37687317]\n",
      "epoch: 354 loss: [6.29269255]\n",
      "epoch: 355 loss: [6.20959679]\n",
      "epoch: 356 loss: [6.12757208]\n",
      "epoch: 357 loss: [6.04660479]\n",
      "epoch: 358 loss: [5.96668147]\n",
      "epoch: 359 loss: [5.88778882]\n",
      "epoch: 360 loss: [5.80991373]\n",
      "epoch: 361 loss: [5.73304324]\n",
      "epoch: 362 loss: [5.65716457]\n",
      "epoch: 363 loss: [5.58226508]\n",
      "epoch: 364 loss: [5.50833231]\n",
      "epoch: 365 loss: [5.43535395]\n",
      "epoch: 366 loss: [5.36331784]\n",
      "epoch: 367 loss: [5.29221198]\n",
      "epoch: 368 loss: [5.22202453]\n",
      "epoch: 369 loss: [5.15274378]\n",
      "epoch: 370 loss: [5.08435817]\n",
      "epoch: 371 loss: [5.01685632]\n",
      "epoch: 372 loss: [4.95022695]\n",
      "epoch: 373 loss: [4.88445895]\n",
      "epoch: 374 loss: [4.81954134]\n",
      "epoch: 375 loss: [4.75546328]\n",
      "epoch: 376 loss: [4.69221407]\n",
      "epoch: 377 loss: [4.62978314]\n",
      "epoch: 378 loss: [4.56816006]\n",
      "epoch: 379 loss: [4.50733454]\n",
      "epoch: 380 loss: [4.44729639]\n",
      "epoch: 381 loss: [4.38803559]\n",
      "epoch: 382 loss: [4.32954221]\n",
      "epoch: 383 loss: [4.27180646]\n",
      "epoch: 384 loss: [4.21481869]\n",
      "epoch: 385 loss: [4.15856935]\n",
      "epoch: 386 loss: [4.10304902]\n",
      "epoch: 387 loss: [4.0482484]\n",
      "epoch: 388 loss: [3.99415829]\n",
      "epoch: 389 loss: [3.94076964]\n",
      "epoch: 390 loss: [3.88807349]\n",
      "epoch: 391 loss: [3.836061]\n",
      "epoch: 392 loss: [3.78472345]\n",
      "epoch: 393 loss: [3.7340522]\n",
      "epoch: 394 loss: [3.68403877]\n",
      "epoch: 395 loss: [3.63467474]\n",
      "epoch: 396 loss: [3.58595183]\n",
      "epoch: 397 loss: [3.53786184]\n",
      "epoch: 398 loss: [3.49039669]\n",
      "epoch: 399 loss: [3.44354839]\n",
      "epoch: 400 loss: [3.39730907]\n",
      "epoch: 401 loss: [3.35167094]\n",
      "epoch: 402 loss: [3.30662632]\n",
      "epoch: 403 loss: [3.26216762]\n",
      "epoch: 404 loss: [3.21828736]\n",
      "epoch: 405 loss: [3.17497813]\n",
      "epoch: 406 loss: [3.13223265]\n",
      "epoch: 407 loss: [3.09004369]\n",
      "epoch: 408 loss: [3.04840415]\n",
      "epoch: 409 loss: [3.00730699]\n",
      "epoch: 410 loss: [2.96674528]\n",
      "epoch: 411 loss: [2.92671217]\n",
      "epoch: 412 loss: [2.88720089]\n",
      "epoch: 413 loss: [2.84820478]\n",
      "epoch: 414 loss: [2.80971722]\n",
      "epoch: 415 loss: [2.77173173]\n",
      "epoch: 416 loss: [2.73424186]\n",
      "epoch: 417 loss: [2.69724128]\n",
      "epoch: 418 loss: [2.66072373]\n",
      "epoch: 419 loss: [2.624683]\n",
      "epoch: 420 loss: [2.58911301]\n",
      "epoch: 421 loss: [2.55400772]\n",
      "epoch: 422 loss: [2.51936117]\n",
      "epoch: 423 loss: [2.4851675]\n",
      "epoch: 424 loss: [2.4514209]\n",
      "epoch: 425 loss: [2.41811563]\n",
      "epoch: 426 loss: [2.38524605]\n",
      "epoch: 427 loss: [2.35280657]\n",
      "epoch: 428 loss: [2.32079167]\n",
      "epoch: 429 loss: [2.28919592]\n",
      "epoch: 430 loss: [2.25801393]\n",
      "epoch: 431 loss: [2.22724041]\n",
      "epoch: 432 loss: [2.19687011]\n",
      "epoch: 433 loss: [2.16689786]\n",
      "epoch: 434 loss: [2.13731855]\n",
      "epoch: 435 loss: [2.10812715]\n",
      "epoch: 436 loss: [2.07931867]\n",
      "epoch: 437 loss: [2.0508882]\n",
      "epoch: 438 loss: [2.02283088]\n",
      "epoch: 439 loss: [1.99514194]\n",
      "epoch: 440 loss: [1.96781663]\n",
      "epoch: 441 loss: [1.94085029]\n",
      "epoch: 442 loss: [1.91423831]\n",
      "epoch: 443 loss: [1.88797614]\n",
      "epoch: 444 loss: [1.86205927]\n",
      "epoch: 445 loss: [1.83648328]\n",
      "epoch: 446 loss: [1.81124379]\n",
      "epoch: 447 loss: [1.78633647]\n",
      "epoch: 448 loss: [1.76175704]\n",
      "epoch: 449 loss: [1.7375013]\n",
      "epoch: 450 loss: [1.71356509]\n",
      "epoch: 451 loss: [1.68994429]\n",
      "epoch: 452 loss: [1.66663484]\n",
      "epoch: 453 loss: [1.64363275]\n",
      "epoch: 454 loss: [1.62093405]\n",
      "epoch: 455 loss: [1.59853485]\n",
      "epoch: 456 loss: [1.57643129]\n",
      "epoch: 457 loss: [1.55461956]\n",
      "epoch: 458 loss: [1.53309591]\n",
      "epoch: 459 loss: [1.51185662]\n",
      "epoch: 460 loss: [1.49089804]\n",
      "epoch: 461 loss: [1.47021655]\n",
      "epoch: 462 loss: [1.44980858]\n",
      "epoch: 463 loss: [1.42967061]\n",
      "epoch: 464 loss: [1.40979916]\n",
      "epoch: 465 loss: [1.39019078]\n",
      "epoch: 466 loss: [1.3708421]\n",
      "epoch: 467 loss: [1.35174977]\n",
      "epoch: 468 loss: [1.33291047]\n",
      "epoch: 469 loss: [1.31432094]\n",
      "epoch: 470 loss: [1.29597797]\n",
      "epoch: 471 loss: [1.27787837]\n",
      "epoch: 472 loss: [1.26001901]\n",
      "epoch: 473 loss: [1.24239677]\n",
      "epoch: 474 loss: [1.2250086]\n",
      "epoch: 475 loss: [1.20785149]\n",
      "epoch: 476 loss: [1.19092244]\n",
      "epoch: 477 loss: [1.17421851]\n",
      "epoch: 478 loss: [1.15773679]\n",
      "epoch: 479 loss: [1.14147441]\n",
      "epoch: 480 loss: [1.12542854]\n",
      "epoch: 481 loss: [1.10959638]\n",
      "epoch: 482 loss: [1.09397517]\n",
      "epoch: 483 loss: [1.07856217]\n",
      "epoch: 484 loss: [1.06335471]\n",
      "epoch: 485 loss: [1.04835011]\n",
      "epoch: 486 loss: [1.03354576]\n",
      "epoch: 487 loss: [1.01893906]\n",
      "epoch: 488 loss: [1.00452746]\n",
      "epoch: 489 loss: [0.99030842]\n",
      "epoch: 490 loss: [0.97627947]\n",
      "epoch: 491 loss: [0.96243813]\n",
      "epoch: 492 loss: [0.94878198]\n",
      "epoch: 493 loss: [0.93530862]\n",
      "epoch: 494 loss: [0.92201568]\n",
      "epoch: 495 loss: [0.90890082]\n",
      "epoch: 496 loss: [0.89596174]\n",
      "epoch: 497 loss: [0.88319615]\n",
      "epoch: 498 loss: [0.8706018]\n",
      "epoch: 499 loss: [0.85817648]\n",
      "epoch: 500 loss: [0.845918]\n",
      "epoch: 501 loss: [0.83382418]\n",
      "epoch: 502 loss: [0.82189289]\n",
      "epoch: 503 loss: [0.81012202]\n",
      "epoch: 504 loss: [0.79850949]\n",
      "epoch: 505 loss: [0.78705325]\n",
      "epoch: 506 loss: [0.77575126]\n",
      "epoch: 507 loss: [0.76460151]\n",
      "epoch: 508 loss: [0.75360204]\n",
      "epoch: 509 loss: [0.74275088]\n",
      "epoch: 510 loss: [0.73204612]\n",
      "epoch: 511 loss: [0.72148584]\n",
      "epoch: 512 loss: [0.71106816]\n",
      "epoch: 513 loss: [0.70079124]\n",
      "epoch: 514 loss: [0.69065324]\n",
      "epoch: 515 loss: [0.68065235]\n",
      "epoch: 516 loss: [0.67078679]\n",
      "epoch: 517 loss: [0.6610548]\n",
      "epoch: 518 loss: [0.65145463]\n",
      "epoch: 519 loss: [0.64198457]\n",
      "epoch: 520 loss: [0.63264293]\n",
      "epoch: 521 loss: [0.62342802]\n",
      "epoch: 522 loss: [0.6143382]\n",
      "epoch: 523 loss: [0.60537184]\n",
      "epoch: 524 loss: [0.59652733]\n",
      "epoch: 525 loss: [0.58780307]\n",
      "epoch: 526 loss: [0.57919751]\n",
      "epoch: 527 loss: [0.57070908]\n",
      "epoch: 528 loss: [0.56233626]\n",
      "epoch: 529 loss: [0.55407754]\n",
      "epoch: 530 loss: [0.54593143]\n",
      "epoch: 531 loss: [0.53789647]\n",
      "epoch: 532 loss: [0.52997118]\n",
      "epoch: 533 loss: [0.52215415]\n",
      "epoch: 534 loss: [0.51444396]\n",
      "epoch: 535 loss: [0.5068392]\n",
      "epoch: 536 loss: [0.49933851]\n",
      "epoch: 537 loss: [0.49194051]\n",
      "epoch: 538 loss: [0.48464387]\n",
      "epoch: 539 loss: [0.47744726]\n",
      "epoch: 540 loss: [0.47034936]\n",
      "epoch: 541 loss: [0.46334889]\n",
      "epoch: 542 loss: [0.45644457]\n",
      "epoch: 543 loss: [0.44963513]\n",
      "epoch: 544 loss: [0.44291934]\n",
      "epoch: 545 loss: [0.43629596]\n",
      "epoch: 546 loss: [0.42976379]\n",
      "epoch: 547 loss: [0.42332162]\n",
      "epoch: 548 loss: [0.41696828]\n",
      "epoch: 549 loss: [0.41070259]\n",
      "epoch: 550 loss: [0.40452341]\n",
      "epoch: 551 loss: [0.39842961]\n",
      "epoch: 552 loss: [0.39242005]\n",
      "epoch: 553 loss: [0.38649363]\n",
      "epoch: 554 loss: [0.38064926]\n",
      "epoch: 555 loss: [0.37488586]\n",
      "epoch: 556 loss: [0.36920237]\n",
      "epoch: 557 loss: [0.36359772]\n",
      "epoch: 558 loss: [0.35807089]\n",
      "epoch: 559 loss: [0.35262085]\n",
      "epoch: 560 loss: [0.34724659]\n",
      "epoch: 561 loss: [0.34194711]\n",
      "epoch: 562 loss: [0.33672142]\n",
      "epoch: 563 loss: [0.33156855]\n",
      "epoch: 564 loss: [0.32648754]\n",
      "epoch: 565 loss: [0.32147744]\n",
      "epoch: 566 loss: [0.31653731]\n",
      "epoch: 567 loss: [0.31166624]\n",
      "epoch: 568 loss: [0.30686331]\n",
      "epoch: 569 loss: [0.30212762]\n",
      "epoch: 570 loss: [0.29745827]\n",
      "epoch: 571 loss: [0.2928544]\n",
      "epoch: 572 loss: [0.28831514]\n",
      "epoch: 573 loss: [0.28383963]\n",
      "epoch: 574 loss: [0.27942703]\n",
      "epoch: 575 loss: [0.27507651]\n",
      "epoch: 576 loss: [0.27078724]\n",
      "epoch: 577 loss: [0.26655841]\n",
      "epoch: 578 loss: [0.26238922]\n",
      "epoch: 579 loss: [0.25827889]\n",
      "epoch: 580 loss: [0.25422662]\n",
      "epoch: 581 loss: [0.25023165]\n",
      "epoch: 582 loss: [0.24629322]\n",
      "epoch: 583 loss: [0.24241058]\n",
      "epoch: 584 loss: [0.23858299]\n",
      "epoch: 585 loss: [0.23480971]\n",
      "epoch: 586 loss: [0.23109003]\n",
      "epoch: 587 loss: [0.22742323]\n",
      "epoch: 588 loss: [0.2238086]\n",
      "epoch: 589 loss: [0.22024546]\n",
      "epoch: 590 loss: [0.21673311]\n",
      "epoch: 591 loss: [0.21327089]\n",
      "epoch: 592 loss: [0.20985811]\n",
      "epoch: 593 loss: [0.20649414]\n",
      "epoch: 594 loss: [0.2031783]\n",
      "epoch: 595 loss: [0.19990997]\n",
      "epoch: 596 loss: [0.1966885]\n",
      "epoch: 597 loss: [0.19351328]\n",
      "epoch: 598 loss: [0.19038368]\n",
      "epoch: 599 loss: [0.18729909]\n",
      "epoch: 600 loss: [0.18425891]\n",
      "epoch: 601 loss: [0.18126256]\n",
      "epoch: 602 loss: [0.17830943]\n",
      "epoch: 603 loss: [0.17539896]\n",
      "epoch: 604 loss: [0.17253057]\n",
      "epoch: 605 loss: [0.1697037]\n",
      "epoch: 606 loss: [0.16691779]\n",
      "epoch: 607 loss: [0.16417229]\n",
      "epoch: 608 loss: [0.16146666]\n",
      "epoch: 609 loss: [0.15880037]\n",
      "epoch: 610 loss: [0.15617289]\n",
      "epoch: 611 loss: [0.15358369]\n",
      "epoch: 612 loss: [0.15103227]\n",
      "epoch: 613 loss: [0.1485181]\n",
      "epoch: 614 loss: [0.1460407]\n",
      "epoch: 615 loss: [0.14359957]\n",
      "epoch: 616 loss: [0.14119421]\n",
      "epoch: 617 loss: [0.13882416]\n",
      "epoch: 618 loss: [0.13648892]\n",
      "epoch: 619 loss: [0.13418804]\n",
      "epoch: 620 loss: [0.13192104]\n",
      "epoch: 621 loss: [0.12968748]\n",
      "epoch: 622 loss: [0.12748689]\n",
      "epoch: 623 loss: [0.12531884]\n",
      "epoch: 624 loss: [0.12318288]\n",
      "epoch: 625 loss: [0.12107859]\n",
      "epoch: 626 loss: [0.11900552]\n",
      "epoch: 627 loss: [0.11696327]\n",
      "epoch: 628 loss: [0.1149514]\n",
      "epoch: 629 loss: [0.11296952]\n",
      "epoch: 630 loss: [0.1110172]\n",
      "epoch: 631 loss: [0.10909406]\n",
      "epoch: 632 loss: [0.1071997]\n",
      "epoch: 633 loss: [0.10533372]\n",
      "epoch: 634 loss: [0.10349574]\n",
      "epoch: 635 loss: [0.10168538]\n",
      "epoch: 636 loss: [0.09990226]\n",
      "epoch: 637 loss: [0.09814601]\n",
      "epoch: 638 loss: [0.09641627]\n",
      "epoch: 639 loss: [0.09471267]\n",
      "epoch: 640 loss: [0.09303486]\n",
      "epoch: 641 loss: [0.09138249]\n",
      "epoch: 642 loss: [0.08975521]\n",
      "epoch: 643 loss: [0.08815267]\n",
      "epoch: 644 loss: [0.08657454]\n",
      "epoch: 645 loss: [0.08502049]\n",
      "epoch: 646 loss: [0.08349018]\n",
      "epoch: 647 loss: [0.08198329]\n",
      "epoch: 648 loss: [0.0804995]\n",
      "epoch: 649 loss: [0.07903849]\n",
      "epoch: 650 loss: [0.07759996]\n",
      "epoch: 651 loss: [0.07618358]\n",
      "epoch: 652 loss: [0.07478906]\n",
      "epoch: 653 loss: [0.0734161]\n",
      "epoch: 654 loss: [0.0720644]\n",
      "epoch: 655 loss: [0.07073366]\n",
      "epoch: 656 loss: [0.0694236]\n",
      "epoch: 657 loss: [0.06813394]\n",
      "epoch: 658 loss: [0.06686439]\n",
      "epoch: 659 loss: [0.06561468]\n",
      "epoch: 660 loss: [0.06438453]\n",
      "epoch: 661 loss: [0.06317366]\n",
      "epoch: 662 loss: [0.06198183]\n",
      "epoch: 663 loss: [0.06080875]\n",
      "epoch: 664 loss: [0.05965418]\n",
      "epoch: 665 loss: [0.05851785]\n",
      "epoch: 666 loss: [0.05739951]\n",
      "epoch: 667 loss: [0.05629892]\n",
      "epoch: 668 loss: [0.05521582]\n",
      "epoch: 669 loss: [0.05414997]\n",
      "epoch: 670 loss: [0.05310114]\n",
      "epoch: 671 loss: [0.05206909]\n",
      "epoch: 672 loss: [0.05105357]\n",
      "epoch: 673 loss: [0.05005437]\n",
      "epoch: 674 loss: [0.04907126]\n",
      "epoch: 675 loss: [0.048104]\n",
      "epoch: 676 loss: [0.04715239]\n",
      "epoch: 677 loss: [0.04621619]\n",
      "epoch: 678 loss: [0.0452952]\n",
      "epoch: 679 loss: [0.0443892]\n",
      "epoch: 680 loss: [0.04349798]\n",
      "epoch: 681 loss: [0.04262134]\n",
      "epoch: 682 loss: [0.04175907]\n",
      "epoch: 683 loss: [0.04091096]\n",
      "epoch: 684 loss: [0.04007682]\n",
      "epoch: 685 loss: [0.03925646]\n",
      "epoch: 686 loss: [0.03844967]\n",
      "epoch: 687 loss: [0.03765628]\n",
      "epoch: 688 loss: [0.03687607]\n",
      "epoch: 689 loss: [0.03610889]\n",
      "epoch: 690 loss: [0.03535453]\n",
      "epoch: 691 loss: [0.03461281]\n",
      "epoch: 692 loss: [0.03388357]\n",
      "epoch: 693 loss: [0.03316661]\n",
      "epoch: 694 loss: [0.03246178]\n",
      "epoch: 695 loss: [0.03176888]\n",
      "epoch: 696 loss: [0.03108777]\n",
      "epoch: 697 loss: [0.03041826]\n",
      "epoch: 698 loss: [0.02976019]\n",
      "epoch: 699 loss: [0.0291134]\n",
      "epoch: 700 loss: [0.02847774]\n",
      "epoch: 701 loss: [0.02785303]\n",
      "epoch: 702 loss: [0.02723912]\n",
      "epoch: 703 loss: [0.02663587]\n",
      "epoch: 704 loss: [0.02604311]\n",
      "epoch: 705 loss: [0.0254607]\n",
      "epoch: 706 loss: [0.02488849]\n",
      "epoch: 707 loss: [0.02432633]\n",
      "epoch: 708 loss: [0.02377407]\n",
      "epoch: 709 loss: [0.02323158]\n",
      "epoch: 710 loss: [0.02269872]\n",
      "epoch: 711 loss: [0.02217534]\n",
      "epoch: 712 loss: [0.02166131]\n",
      "epoch: 713 loss: [0.02115649]\n",
      "epoch: 714 loss: [0.02066075]\n",
      "epoch: 715 loss: [0.02017396]\n",
      "epoch: 716 loss: [0.019696]\n",
      "epoch: 717 loss: [0.01922672]\n",
      "epoch: 718 loss: [0.01876601]\n",
      "epoch: 719 loss: [0.01831373]\n",
      "epoch: 720 loss: [0.01786978]\n",
      "epoch: 721 loss: [0.01743402]\n",
      "epoch: 722 loss: [0.01700634]\n",
      "epoch: 723 loss: [0.01658662]\n",
      "epoch: 724 loss: [0.01617474]\n",
      "epoch: 725 loss: [0.01577059]\n",
      "epoch: 726 loss: [0.01537406]\n",
      "epoch: 727 loss: [0.01498502]\n",
      "epoch: 728 loss: [0.01460338]\n",
      "epoch: 729 loss: [0.01422902]\n",
      "epoch: 730 loss: [0.01386184]\n",
      "epoch: 731 loss: [0.01350173]\n",
      "epoch: 732 loss: [0.01314859]\n",
      "epoch: 733 loss: [0.01280231]\n",
      "epoch: 734 loss: [0.01246279]\n",
      "epoch: 735 loss: [0.01212994]\n",
      "epoch: 736 loss: [0.01180364]\n",
      "epoch: 737 loss: [0.01148381]\n",
      "epoch: 738 loss: [0.01117035]\n",
      "epoch: 739 loss: [0.01086317]\n",
      "epoch: 740 loss: [0.01056216]\n",
      "epoch: 741 loss: [0.01026724]\n",
      "epoch: 742 loss: [0.00997832]\n",
      "epoch: 743 loss: [0.00969531]\n",
      "epoch: 744 loss: [0.00941811]\n",
      "epoch: 745 loss: [0.00914665]\n",
      "epoch: 746 loss: [0.00888083]\n",
      "epoch: 747 loss: [0.00862057]\n",
      "epoch: 748 loss: [0.00836579]\n",
      "epoch: 749 loss: [0.0081164]\n",
      "epoch: 750 loss: [0.00787232]\n",
      "epoch: 751 loss: [0.00763347]\n",
      "epoch: 752 loss: [0.00739977]\n",
      "epoch: 753 loss: [0.00717115]\n",
      "epoch: 754 loss: [0.00694752]\n",
      "epoch: 755 loss: [0.00672881]\n",
      "epoch: 756 loss: [0.00651495]\n",
      "epoch: 757 loss: [0.00630585]\n",
      "epoch: 758 loss: [0.00610146]\n",
      "epoch: 759 loss: [0.00590169]\n",
      "epoch: 760 loss: [0.00570647]\n",
      "epoch: 761 loss: [0.00551574]\n",
      "epoch: 762 loss: [0.00532942]\n",
      "epoch: 763 loss: [0.00514744]\n",
      "epoch: 764 loss: [0.00496975]\n",
      "epoch: 765 loss: [0.00479627]\n",
      "epoch: 766 loss: [0.00462694]\n",
      "epoch: 767 loss: [0.00446169]\n",
      "epoch: 768 loss: [0.00430045]\n",
      "epoch: 769 loss: [0.00414318]\n",
      "epoch: 770 loss: [0.0039898]\n",
      "epoch: 771 loss: [0.00384025]\n",
      "epoch: 772 loss: [0.00369447]\n",
      "epoch: 773 loss: [0.00355241]\n",
      "epoch: 774 loss: [0.00341401]\n",
      "epoch: 775 loss: [0.0032792]\n",
      "epoch: 776 loss: [0.00314793]\n",
      "epoch: 777 loss: [0.00302014]\n",
      "epoch: 778 loss: [0.00289579]\n",
      "epoch: 779 loss: [0.00277481]\n",
      "epoch: 780 loss: [0.00265714]\n",
      "epoch: 781 loss: [0.00254275]\n",
      "epoch: 782 loss: [0.00243157]\n",
      "epoch: 783 loss: [0.00232355]\n",
      "epoch: 784 loss: [0.00221865]\n",
      "epoch: 785 loss: [0.00211681]\n",
      "epoch: 786 loss: [0.00201798]\n",
      "epoch: 787 loss: [0.00192211]\n",
      "epoch: 788 loss: [0.00182916]\n",
      "epoch: 789 loss: [0.00173908]\n",
      "epoch: 790 loss: [0.00165182]\n",
      "epoch: 791 loss: [0.00156733]\n",
      "epoch: 792 loss: [0.00148558]\n",
      "epoch: 793 loss: [0.00140651]\n",
      "epoch: 794 loss: [0.00133008]\n",
      "epoch: 795 loss: [0.00125625]\n",
      "epoch: 796 loss: [0.00118497]\n",
      "epoch: 797 loss: [0.0011162]\n",
      "epoch: 798 loss: [0.0010499]\n",
      "epoch: 799 loss: [0.00098603]\n",
      "epoch: 800 loss: [0.00092455]\n",
      "epoch: 801 loss: [0.00086542]\n",
      "epoch: 802 loss: [0.0008086]\n",
      "epoch: 803 loss: [0.00075404]\n",
      "epoch: 804 loss: [0.00070172]\n",
      "epoch: 805 loss: [0.00065159]\n",
      "epoch: 806 loss: [0.00060362]\n",
      "epoch: 807 loss: [0.00055776]\n",
      "epoch: 808 loss: [0.00051399]\n",
      "epoch: 809 loss: [0.00047227]\n",
      "epoch: 810 loss: [0.00043256]\n",
      "epoch: 811 loss: [0.00039483]\n",
      "epoch: 812 loss: [0.00035904]\n",
      "epoch: 813 loss: [0.00032516]\n",
      "epoch: 814 loss: [0.00029315]\n",
      "epoch: 815 loss: [0.000263]\n",
      "epoch: 816 loss: [0.00023465]\n",
      "epoch: 817 loss: [0.00020808]\n",
      "epoch: 818 loss: [0.00018326]\n",
      "epoch: 819 loss: [0.00016016]\n",
      "epoch: 820 loss: [0.00013874]\n",
      "epoch: 821 loss: [0.00011898]\n",
      "epoch: 822 loss: [0.00010085]\n",
      "epoch: 823 loss: [8.43202675e-05]\n",
      "epoch: 824 loss: [6.93567518e-05]\n",
      "epoch: 825 loss: [5.59343128e-05]\n",
      "epoch: 826 loss: [4.40250319e-05]\n",
      "epoch: 827 loss: [3.36013957e-05]\n",
      "epoch: 828 loss: [2.46362901e-05]\n",
      "epoch: 829 loss: [1.71029956e-05]\n",
      "epoch: 830 loss: [1.09751811e-05]\n",
      "epoch: 831 loss: [6.2268996e-06]\n",
      "epoch: 832 loss: [2.83258231e-06]\n",
      "epoch: 833 loss: [7.67034037e-07]\n",
      "epoch: 834 loss: [5.42793605e-09]\n",
      "epoch: 835 loss: [5.2330062e-07]\n",
      "epoch: 836 loss: [2.29654725e-06]\n",
      "epoch: 837 loss: [5.30141672e-06]\n",
      "epoch: 838 loss: [9.51450685e-06]\n",
      "epoch: 839 loss: [1.49127597e-05]\n",
      "epoch: 840 loss: [2.14734571e-05]\n",
      "epoch: 841 loss: [2.91742156e-05]\n",
      "epoch: 842 loss: [3.79929824e-05]\n",
      "epoch: 843 loss: [4.79080308e-05]\n",
      "epoch: 844 loss: [5.88979557e-05]\n",
      "epoch: 845 loss: [7.09416692e-05]\n",
      "epoch: 846 loss: [8.40183963e-05]\n",
      "epoch: 847 loss: [9.8107671e-05]\n",
      "epoch: 848 loss: [0.00011319]\n",
      "epoch: 849 loss: [0.00012924]\n",
      "epoch: 850 loss: [0.00014625]\n",
      "epoch: 851 loss: [0.00016419]\n",
      "epoch: 852 loss: [0.00018305]\n",
      "epoch: 853 loss: [0.0002028]\n",
      "epoch: 854 loss: [0.00022343]\n",
      "epoch: 855 loss: [0.00024492]\n",
      "epoch: 856 loss: [0.00026725]\n",
      "epoch: 857 loss: [0.0002904]\n",
      "epoch: 858 loss: [0.00031436]\n",
      "epoch: 859 loss: [0.00033911]\n",
      "epoch: 860 loss: [0.00036463]\n",
      "epoch: 861 loss: [0.00039091]\n",
      "epoch: 862 loss: [0.00041793]\n",
      "epoch: 863 loss: [0.00044568]\n",
      "epoch: 864 loss: [0.00047413]\n",
      "epoch: 865 loss: [0.00050327]\n",
      "epoch: 866 loss: [0.00053309]\n",
      "epoch: 867 loss: [0.00056357]\n",
      "epoch: 868 loss: [0.0005947]\n",
      "epoch: 869 loss: [0.00062647]\n",
      "epoch: 870 loss: [0.00065885]\n",
      "epoch: 871 loss: [0.00069183]\n",
      "epoch: 872 loss: [0.0007254]\n",
      "epoch: 873 loss: [0.00075955]\n",
      "epoch: 874 loss: [0.00079427]\n",
      "epoch: 875 loss: [0.00082953]\n",
      "epoch: 876 loss: [0.00086533]\n",
      "epoch: 877 loss: [0.00090165]\n",
      "epoch: 878 loss: [0.00093848]\n",
      "epoch: 879 loss: [0.00097581]\n",
      "epoch: 880 loss: [0.00101363]\n",
      "epoch: 881 loss: [0.00105191]\n",
      "epoch: 882 loss: [0.00109066]\n",
      "epoch: 883 loss: [0.00112986]\n",
      "epoch: 884 loss: [0.0011695]\n",
      "epoch: 885 loss: [0.00120957]\n",
      "epoch: 886 loss: [0.00125005]\n",
      "epoch: 887 loss: [0.00129094]\n",
      "epoch: 888 loss: [0.00133221]\n",
      "epoch: 889 loss: [0.00137388]\n",
      "epoch: 890 loss: [0.00141591]\n",
      "epoch: 891 loss: [0.00145831]\n",
      "epoch: 892 loss: [0.00150106]\n",
      "epoch: 893 loss: [0.00154415]\n",
      "epoch: 894 loss: [0.00158757]\n",
      "epoch: 895 loss: [0.00163132]\n",
      "epoch: 896 loss: [0.00167538]\n",
      "epoch: 897 loss: [0.00171974]\n",
      "epoch: 898 loss: [0.0017644]\n",
      "epoch: 899 loss: [0.00180934]\n",
      "epoch: 900 loss: [0.00185456]\n",
      "epoch: 901 loss: [0.00190004]\n",
      "epoch: 902 loss: [0.00194579]\n",
      "epoch: 903 loss: [0.00199178]\n",
      "epoch: 904 loss: [0.00203802]\n",
      "epoch: 905 loss: [0.00208449]\n",
      "epoch: 906 loss: [0.00213119]\n",
      "epoch: 907 loss: [0.0021781]\n",
      "epoch: 908 loss: [0.00222523]\n",
      "epoch: 909 loss: [0.00227255]\n",
      "epoch: 910 loss: [0.00232008]\n",
      "epoch: 911 loss: [0.00236778]\n",
      "epoch: 912 loss: [0.00241567]\n",
      "epoch: 913 loss: [0.00246373]\n",
      "epoch: 914 loss: [0.00251196]\n",
      "epoch: 915 loss: [0.00256035]\n",
      "epoch: 916 loss: [0.00260888]\n",
      "epoch: 917 loss: [0.00265757]\n",
      "epoch: 918 loss: [0.00270639]\n",
      "epoch: 919 loss: [0.00275534]\n",
      "epoch: 920 loss: [0.00280442]\n",
      "epoch: 921 loss: [0.00285362]\n",
      "epoch: 922 loss: [0.00290294]\n",
      "epoch: 923 loss: [0.00295236]\n",
      "epoch: 924 loss: [0.00300188]\n",
      "epoch: 925 loss: [0.00305149]\n",
      "epoch: 926 loss: [0.0031012]\n",
      "epoch: 927 loss: [0.00315099]\n",
      "epoch: 928 loss: [0.00320086]\n",
      "epoch: 929 loss: [0.0032508]\n",
      "epoch: 930 loss: [0.00330081]\n",
      "epoch: 931 loss: [0.00335089]\n",
      "epoch: 932 loss: [0.00340102]\n",
      "epoch: 933 loss: [0.0034512]\n",
      "epoch: 934 loss: [0.00350143]\n",
      "epoch: 935 loss: [0.00355171]\n",
      "epoch: 936 loss: [0.00360202]\n",
      "epoch: 937 loss: [0.00365236]\n",
      "epoch: 938 loss: [0.00370273]\n",
      "epoch: 939 loss: [0.00375313]\n",
      "epoch: 940 loss: [0.00380354]\n",
      "epoch: 941 loss: [0.00385398]\n",
      "epoch: 942 loss: [0.00390442]\n",
      "epoch: 943 loss: [0.00395487]\n",
      "epoch: 944 loss: [0.00400532]\n",
      "epoch: 945 loss: [0.00405577]\n",
      "epoch: 946 loss: [0.00410621]\n",
      "epoch: 947 loss: [0.00415664]\n",
      "epoch: 948 loss: [0.00420707]\n",
      "epoch: 949 loss: [0.00425747]\n",
      "epoch: 950 loss: [0.00430785]\n",
      "epoch: 951 loss: [0.00435821]\n",
      "epoch: 952 loss: [0.00440854]\n",
      "epoch: 953 loss: [0.00445884]\n",
      "epoch: 954 loss: [0.00450911]\n",
      "epoch: 955 loss: [0.00455934]\n",
      "epoch: 956 loss: [0.00460952]\n",
      "epoch: 957 loss: [0.00465966]\n",
      "epoch: 958 loss: [0.00470976]\n",
      "epoch: 959 loss: [0.0047598]\n",
      "epoch: 960 loss: [0.00480979]\n",
      "epoch: 961 loss: [0.00485972]\n",
      "epoch: 962 loss: [0.00490959]\n",
      "epoch: 963 loss: [0.0049594]\n",
      "epoch: 964 loss: [0.00500915]\n",
      "epoch: 965 loss: [0.00505883]\n",
      "epoch: 966 loss: [0.00510843]\n",
      "epoch: 967 loss: [0.00515797]\n",
      "epoch: 968 loss: [0.00520742]\n",
      "epoch: 969 loss: [0.0052568]\n",
      "epoch: 970 loss: [0.0053061]\n",
      "epoch: 971 loss: [0.00535532]\n",
      "epoch: 972 loss: [0.00540445]\n",
      "epoch: 973 loss: [0.00545349]\n",
      "epoch: 974 loss: [0.00550244]\n",
      "epoch: 975 loss: [0.0055513]\n",
      "epoch: 976 loss: [0.00560007]\n",
      "epoch: 977 loss: [0.00564874]\n",
      "epoch: 978 loss: [0.00569731]\n",
      "epoch: 979 loss: [0.00574578]\n",
      "epoch: 980 loss: [0.00579415]\n",
      "epoch: 981 loss: [0.00584241]\n",
      "epoch: 982 loss: [0.00589057]\n",
      "epoch: 983 loss: [0.00593862]\n",
      "epoch: 984 loss: [0.00598656]\n",
      "epoch: 985 loss: [0.00603438]\n",
      "epoch: 986 loss: [0.0060821]\n",
      "epoch: 987 loss: [0.0061297]\n",
      "epoch: 988 loss: [0.00617718]\n",
      "epoch: 989 loss: [0.00622455]\n",
      "epoch: 990 loss: [0.00627179]\n",
      "epoch: 991 loss: [0.00631892]\n",
      "epoch: 992 loss: [0.00636592]\n",
      "epoch: 993 loss: [0.0064128]\n",
      "epoch: 994 loss: [0.00645955]\n",
      "epoch: 995 loss: [0.00650617]\n",
      "epoch: 996 loss: [0.00655267]\n",
      "epoch: 997 loss: [0.00659904]\n",
      "epoch: 998 loss: [0.00664528]\n",
      "epoch: 999 loss: [0.00669138]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> -0.38211971865603694 err: [0.38211972]\n",
      "[0. 1.] [0.] -> 0.22760394660223576 err: [-0.22760395]\n",
      "[1. 0.] [0.] -> 0.23158263753971936 err: [-0.23158264]\n",
      "[1. 1.] [1.] -> 0.8413063027979921 err: [0.1586937]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [0.] -> 0 err: [0.]\n",
      "[1. 0.] [0.] -> 0 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## 練習:「OR」を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "#期待してる出力（ラベル）\n",
    "y_true = np.array([\n",
    "    [0], \n",
    "    [1], \n",
    "    [1], \n",
    "    [1]\n",
    "], dtype = float)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論とラベルの誤差は：\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: [1.50668551]\n",
      "epoch: 1 loss: [1.28741126]\n",
      "epoch: 2 loss: [1.09780564]\n",
      "epoch: 3 loss: [0.93404651]\n",
      "epoch: 4 loss: [0.79279078]\n",
      "epoch: 5 loss: [0.67111526]\n",
      "epoch: 6 loss: [0.56646477]\n",
      "epoch: 7 loss: [0.47660662]\n",
      "epoch: 8 loss: [0.39959064]\n",
      "epoch: 9 loss: [0.3337141]\n",
      "epoch: 10 loss: [0.277491]\n",
      "epoch: 11 loss: [0.22962505]\n",
      "epoch: 12 loss: [0.18898604]\n",
      "epoch: 13 loss: [0.15458908]\n",
      "epoch: 14 loss: [0.12557642]\n",
      "epoch: 15 loss: [0.10120147]\n",
      "epoch: 16 loss: [0.08081488]\n",
      "epoch: 17 loss: [0.06385225]\n",
      "epoch: 18 loss: [0.04982339]\n",
      "epoch: 19 loss: [0.03830295]\n",
      "epoch: 20 loss: [0.02892217]\n",
      "epoch: 21 loss: [0.02136165]\n",
      "epoch: 22 loss: [0.01534508]\n",
      "epoch: 23 loss: [0.01063365]\n",
      "epoch: 24 loss: [0.00702131]\n",
      "epoch: 25 loss: [0.00433044]\n",
      "epoch: 26 loss: [0.00240821]\n",
      "epoch: 27 loss: [0.00112336]\n",
      "epoch: 28 loss: [0.00036331]\n",
      "epoch: 29 loss: [3.17719886e-05]\n",
      "epoch: 30 loss: [4.65268338e-05]\n",
      "epoch: 31 loss: [0.00033759]\n",
      "epoch: 32 loss: [0.00084554]\n",
      "epoch: 33 loss: [0.00152012]\n",
      "epoch: 34 loss: [0.00231895]\n",
      "epoch: 35 loss: [0.00320647]\n",
      "epoch: 36 loss: [0.00415301]\n",
      "epoch: 37 loss: [0.0051339]\n",
      "epoch: 38 loss: [0.00612884]\n",
      "epoch: 39 loss: [0.00712121]\n",
      "epoch: 40 loss: [0.00809758]\n",
      "epoch: 41 loss: [0.00904722]\n",
      "epoch: 42 loss: [0.00996168]\n",
      "epoch: 43 loss: [0.01083447]\n",
      "epoch: 44 loss: [0.01166075]\n",
      "epoch: 45 loss: [0.01243704]\n",
      "epoch: 46 loss: [0.01316104]\n",
      "epoch: 47 loss: [0.01383141]\n",
      "epoch: 48 loss: [0.01444759]\n",
      "epoch: 49 loss: [0.0150097]\n",
      "epoch: 50 loss: [0.01551837]\n",
      "epoch: 51 loss: [0.01597467]\n",
      "epoch: 52 loss: [0.01638]\n",
      "epoch: 53 loss: [0.01673603]\n",
      "epoch: 54 loss: [0.01704462]\n",
      "epoch: 55 loss: [0.01730778]\n",
      "epoch: 56 loss: [0.01752763]\n",
      "epoch: 57 loss: [0.01770633]\n",
      "epoch: 58 loss: [0.01784609]\n",
      "epoch: 59 loss: [0.01794912]\n",
      "epoch: 60 loss: [0.01801761]\n",
      "epoch: 61 loss: [0.01805373]\n",
      "epoch: 62 loss: [0.01805957]\n",
      "epoch: 63 loss: [0.0180372]\n",
      "epoch: 64 loss: [0.01798861]\n",
      "epoch: 65 loss: [0.01791571]\n",
      "epoch: 66 loss: [0.01782035]\n",
      "epoch: 67 loss: [0.01770427]\n",
      "epoch: 68 loss: [0.01756918]\n",
      "epoch: 69 loss: [0.01741665]\n",
      "epoch: 70 loss: [0.01724822]\n",
      "epoch: 71 loss: [0.01706532]\n",
      "epoch: 72 loss: [0.0168693]\n",
      "epoch: 73 loss: [0.01666144]\n",
      "epoch: 74 loss: [0.01644296]\n",
      "epoch: 75 loss: [0.01621498]\n",
      "epoch: 76 loss: [0.01597856]\n",
      "epoch: 77 loss: [0.01573471]\n",
      "epoch: 78 loss: [0.01548435]\n",
      "epoch: 79 loss: [0.01522835]\n",
      "epoch: 80 loss: [0.01496753]\n",
      "epoch: 81 loss: [0.01470264]\n",
      "epoch: 82 loss: [0.01443438]\n",
      "epoch: 83 loss: [0.0141634]\n",
      "epoch: 84 loss: [0.01389032]\n",
      "epoch: 85 loss: [0.01361569]\n",
      "epoch: 86 loss: [0.01334003]\n",
      "epoch: 87 loss: [0.01306383]\n",
      "epoch: 88 loss: [0.01278752]\n",
      "epoch: 89 loss: [0.01251151]\n",
      "epoch: 90 loss: [0.01223617]\n",
      "epoch: 91 loss: [0.01196186]\n",
      "epoch: 92 loss: [0.01168888]\n",
      "epoch: 93 loss: [0.01141753]\n",
      "epoch: 94 loss: [0.01114807]\n",
      "epoch: 95 loss: [0.01088073]\n",
      "epoch: 96 loss: [0.01061575]\n",
      "epoch: 97 loss: [0.0103533]\n",
      "epoch: 98 loss: [0.01009358]\n",
      "epoch: 99 loss: [0.00983674]\n",
      "epoch: 100 loss: [0.00958293]\n",
      "epoch: 101 loss: [0.00933228]\n",
      "epoch: 102 loss: [0.0090849]\n",
      "epoch: 103 loss: [0.0088409]\n",
      "epoch: 104 loss: [0.00860036]\n",
      "epoch: 105 loss: [0.00836336]\n",
      "epoch: 106 loss: [0.00812997]\n",
      "epoch: 107 loss: [0.00790025]\n",
      "epoch: 108 loss: [0.00767425]\n",
      "epoch: 109 loss: [0.00745201]\n",
      "epoch: 110 loss: [0.00723355]\n",
      "epoch: 111 loss: [0.00701892]\n",
      "epoch: 112 loss: [0.00680812]\n",
      "epoch: 113 loss: [0.00660116]\n",
      "epoch: 114 loss: [0.00639807]\n",
      "epoch: 115 loss: [0.00619884]\n",
      "epoch: 116 loss: [0.00600347]\n",
      "epoch: 117 loss: [0.00581195]\n",
      "epoch: 118 loss: [0.00562427]\n",
      "epoch: 119 loss: [0.00544042]\n",
      "epoch: 120 loss: [0.00526039]\n",
      "epoch: 121 loss: [0.00508415]\n",
      "epoch: 122 loss: [0.00491168]\n",
      "epoch: 123 loss: [0.00474295]\n",
      "epoch: 124 loss: [0.00457794]\n",
      "epoch: 125 loss: [0.00441663]\n",
      "epoch: 126 loss: [0.00425897]\n",
      "epoch: 127 loss: [0.00410494]\n",
      "epoch: 128 loss: [0.0039545]\n",
      "epoch: 129 loss: [0.00380762]\n",
      "epoch: 130 loss: [0.00366426]\n",
      "epoch: 131 loss: [0.00352438]\n",
      "epoch: 132 loss: [0.00338795]\n",
      "epoch: 133 loss: [0.00325492]\n",
      "epoch: 134 loss: [0.00312526]\n",
      "epoch: 135 loss: [0.00299892]\n",
      "epoch: 136 loss: [0.00287587]\n",
      "epoch: 137 loss: [0.00275607]\n",
      "epoch: 138 loss: [0.00263947]\n",
      "epoch: 139 loss: [0.00252602]\n",
      "epoch: 140 loss: [0.0024157]\n",
      "epoch: 141 loss: [0.00230845]\n",
      "epoch: 142 loss: [0.00220424]\n",
      "epoch: 143 loss: [0.00210302]\n",
      "epoch: 144 loss: [0.00200475]\n",
      "epoch: 145 loss: [0.00190939]\n",
      "epoch: 146 loss: [0.0018169]\n",
      "epoch: 147 loss: [0.00172723]\n",
      "epoch: 148 loss: [0.00164035]\n",
      "epoch: 149 loss: [0.0015562]\n",
      "epoch: 150 loss: [0.00147476]\n",
      "epoch: 151 loss: [0.00139598]\n",
      "epoch: 152 loss: [0.00131981]\n",
      "epoch: 153 loss: [0.00124623]\n",
      "epoch: 154 loss: [0.00117518]\n",
      "epoch: 155 loss: [0.00110664]\n",
      "epoch: 156 loss: [0.00104055]\n",
      "epoch: 157 loss: [0.00097688]\n",
      "epoch: 158 loss: [0.0009156]\n",
      "epoch: 159 loss: [0.00085666]\n",
      "epoch: 160 loss: [0.00080002]\n",
      "epoch: 161 loss: [0.00074566]\n",
      "epoch: 162 loss: [0.00069352]\n",
      "epoch: 163 loss: [0.00064358]\n",
      "epoch: 164 loss: [0.0005958]\n",
      "epoch: 165 loss: [0.00055014]\n",
      "epoch: 166 loss: [0.00050657]\n",
      "epoch: 167 loss: [0.00046505]\n",
      "epoch: 168 loss: [0.00042555]\n",
      "epoch: 169 loss: [0.00038803]\n",
      "epoch: 170 loss: [0.00035246]\n",
      "epoch: 171 loss: [0.00031881]\n",
      "epoch: 172 loss: [0.00028704]\n",
      "epoch: 173 loss: [0.00025712]\n",
      "epoch: 174 loss: [0.00022902]\n",
      "epoch: 175 loss: [0.00020271]\n",
      "epoch: 176 loss: [0.00017815]\n",
      "epoch: 177 loss: [0.00015531]\n",
      "epoch: 178 loss: [0.00013417]\n",
      "epoch: 179 loss: [0.0001147]\n",
      "epoch: 180 loss: [9.6859168e-05]\n",
      "epoch: 181 loss: [8.06241885e-05]\n",
      "epoch: 182 loss: [6.59657389e-05]\n",
      "epoch: 183 loss: [5.28553773e-05]\n",
      "epoch: 184 loss: [4.12650548e-05]\n",
      "epoch: 185 loss: [3.11671117e-05]\n",
      "epoch: 186 loss: [2.2534274e-05]\n",
      "epoch: 187 loss: [1.53396499e-05]\n",
      "epoch: 188 loss: [9.55672614e-06]\n",
      "epoch: 189 loss: [5.15936421e-06]\n",
      "epoch: 190 loss: [2.12179644e-06]\n",
      "epoch: 191 loss: [4.18622244e-07]\n",
      "epoch: 192 loss: [2.48041714e-08]\n",
      "epoch: 193 loss: [9.15663966e-07]\n",
      "epoch: 194 loss: [3.0668786e-06]\n",
      "epoch: 195 loss: [6.4544763e-06]\n",
      "epoch: 196 loss: [1.10548325e-05]\n",
      "epoch: 197 loss: [1.68446659e-05]\n",
      "epoch: 198 loss: [2.38010343e-05]\n",
      "epoch: 199 loss: [3.19013309e-05]\n",
      "epoch: 200 loss: [4.11232797e-05]\n",
      "epoch: 201 loss: [5.14449323e-05]\n",
      "epoch: 202 loss: [6.28446632e-05]\n",
      "epoch: 203 loss: [7.53011661e-05]\n",
      "epoch: 204 loss: [8.87934499e-05]\n",
      "epoch: 205 loss: [0.0001033]\n",
      "epoch: 206 loss: [0.0001188]\n",
      "epoch: 207 loss: [0.00013528]\n",
      "epoch: 208 loss: [0.00015271]\n",
      "epoch: 209 loss: [0.00017108]\n",
      "epoch: 210 loss: [0.00019036]\n",
      "epoch: 211 loss: [0.00021054]\n",
      "epoch: 212 loss: [0.0002316]\n",
      "epoch: 213 loss: [0.00025352]\n",
      "epoch: 214 loss: [0.00027628]\n",
      "epoch: 215 loss: [0.00029987]\n",
      "epoch: 216 loss: [0.00032426]\n",
      "epoch: 217 loss: [0.00034945]\n",
      "epoch: 218 loss: [0.0003754]\n",
      "epoch: 219 loss: [0.00040212]\n",
      "epoch: 220 loss: [0.00042957]\n",
      "epoch: 221 loss: [0.00045774]\n",
      "epoch: 222 loss: [0.00048663]\n",
      "epoch: 223 loss: [0.0005162]\n",
      "epoch: 224 loss: [0.00054645]\n",
      "epoch: 225 loss: [0.00057737]\n",
      "epoch: 226 loss: [0.00060892]\n",
      "epoch: 227 loss: [0.00064111]\n",
      "epoch: 228 loss: [0.00067392]\n",
      "epoch: 229 loss: [0.00070733]\n",
      "epoch: 230 loss: [0.00074133]\n",
      "epoch: 231 loss: [0.0007759]\n",
      "epoch: 232 loss: [0.00081103]\n",
      "epoch: 233 loss: [0.00084671]\n",
      "epoch: 234 loss: [0.00088292]\n",
      "epoch: 235 loss: [0.00091966]\n",
      "epoch: 236 loss: [0.0009569]\n",
      "epoch: 237 loss: [0.00099464]\n",
      "epoch: 238 loss: [0.00103286]\n",
      "epoch: 239 loss: [0.00107155]\n",
      "epoch: 240 loss: [0.00111071]\n",
      "epoch: 241 loss: [0.0011503]\n",
      "epoch: 242 loss: [0.00119034]\n",
      "epoch: 243 loss: [0.0012308]\n",
      "epoch: 244 loss: [0.00127167]\n",
      "epoch: 245 loss: [0.00131294]\n",
      "epoch: 246 loss: [0.00135461]\n",
      "epoch: 247 loss: [0.00139665]\n",
      "epoch: 248 loss: [0.00143906]\n",
      "epoch: 249 loss: [0.00148183]\n",
      "epoch: 250 loss: [0.00152496]\n",
      "epoch: 251 loss: [0.00156842]\n",
      "epoch: 252 loss: [0.0016122]\n",
      "epoch: 253 loss: [0.00165631]\n",
      "epoch: 254 loss: [0.00170073]\n",
      "epoch: 255 loss: [0.00174545]\n",
      "epoch: 256 loss: [0.00179045]\n",
      "epoch: 257 loss: [0.00183574]\n",
      "epoch: 258 loss: [0.00188131]\n",
      "epoch: 259 loss: [0.00192713]\n",
      "epoch: 260 loss: [0.00197321]\n",
      "epoch: 261 loss: [0.00201954]\n",
      "epoch: 262 loss: [0.00206611]\n",
      "epoch: 263 loss: [0.00211291]\n",
      "epoch: 264 loss: [0.00215992]\n",
      "epoch: 265 loss: [0.00220716]\n",
      "epoch: 266 loss: [0.0022546]\n",
      "epoch: 267 loss: [0.00230223]\n",
      "epoch: 268 loss: [0.00235006]\n",
      "epoch: 269 loss: [0.00239807]\n",
      "epoch: 270 loss: [0.00244626]\n",
      "epoch: 271 loss: [0.00249462]\n",
      "epoch: 272 loss: [0.00254313]\n",
      "epoch: 273 loss: [0.00259181]\n",
      "epoch: 274 loss: [0.00264063]\n",
      "epoch: 275 loss: [0.00268959]\n",
      "epoch: 276 loss: [0.00273869]\n",
      "epoch: 277 loss: [0.00278791]\n",
      "epoch: 278 loss: [0.00283726]\n",
      "epoch: 279 loss: [0.00288672]\n",
      "epoch: 280 loss: [0.0029363]\n",
      "epoch: 281 loss: [0.00298597]\n",
      "epoch: 282 loss: [0.00303575]\n",
      "epoch: 283 loss: [0.00308561]\n",
      "epoch: 284 loss: [0.00313556]\n",
      "epoch: 285 loss: [0.00318559]\n",
      "epoch: 286 loss: [0.0032357]\n",
      "epoch: 287 loss: [0.00328587]\n",
      "epoch: 288 loss: [0.00333611]\n",
      "epoch: 289 loss: [0.00338641]\n",
      "epoch: 290 loss: [0.00343676]\n",
      "epoch: 291 loss: [0.00348716]\n",
      "epoch: 292 loss: [0.0035376]\n",
      "epoch: 293 loss: [0.00358809]\n",
      "epoch: 294 loss: [0.0036386]\n",
      "epoch: 295 loss: [0.00368915]\n",
      "epoch: 296 loss: [0.00373972]\n",
      "epoch: 297 loss: [0.00379031]\n",
      "epoch: 298 loss: [0.00384091]\n",
      "epoch: 299 loss: [0.00389153]\n",
      "epoch: 300 loss: [0.00394216]\n",
      "epoch: 301 loss: [0.00399278]\n",
      "epoch: 302 loss: [0.00404341]\n",
      "epoch: 303 loss: [0.00409403]\n",
      "epoch: 304 loss: [0.00414464]\n",
      "epoch: 305 loss: [0.00419524]\n",
      "epoch: 306 loss: [0.00424582]\n",
      "epoch: 307 loss: [0.00429638]\n",
      "epoch: 308 loss: [0.00434692]\n",
      "epoch: 309 loss: [0.00439743]\n",
      "epoch: 310 loss: [0.00444791]\n",
      "epoch: 311 loss: [0.00449835]\n",
      "epoch: 312 loss: [0.00454875]\n",
      "epoch: 313 loss: [0.00459911]\n",
      "epoch: 314 loss: [0.00464943]\n",
      "epoch: 315 loss: [0.0046997]\n",
      "epoch: 316 loss: [0.00474992]\n",
      "epoch: 317 loss: [0.00480009]\n",
      "epoch: 318 loss: [0.00485019]\n",
      "epoch: 319 loss: [0.00490024]\n",
      "epoch: 320 loss: [0.00495023]\n",
      "epoch: 321 loss: [0.00500014]\n",
      "epoch: 322 loss: [0.00504999]\n",
      "epoch: 323 loss: [0.00509977]\n",
      "epoch: 324 loss: [0.00514948]\n",
      "epoch: 325 loss: [0.00519911]\n",
      "epoch: 326 loss: [0.00524866]\n",
      "epoch: 327 loss: [0.00529813]\n",
      "epoch: 328 loss: [0.00534751]\n",
      "epoch: 329 loss: [0.00539681]\n",
      "epoch: 330 loss: [0.00544602]\n",
      "epoch: 331 loss: [0.00549514]\n",
      "epoch: 332 loss: [0.00554416]\n",
      "epoch: 333 loss: [0.0055931]\n",
      "epoch: 334 loss: [0.00564193]\n",
      "epoch: 335 loss: [0.00569066]\n",
      "epoch: 336 loss: [0.0057393]\n",
      "epoch: 337 loss: [0.00578783]\n",
      "epoch: 338 loss: [0.00583625]\n",
      "epoch: 339 loss: [0.00588457]\n",
      "epoch: 340 loss: [0.00593277]\n",
      "epoch: 341 loss: [0.00598087]\n",
      "epoch: 342 loss: [0.00602886]\n",
      "epoch: 343 loss: [0.00607673]\n",
      "epoch: 344 loss: [0.00612448]\n",
      "epoch: 345 loss: [0.00617212]\n",
      "epoch: 346 loss: [0.00621964]\n",
      "epoch: 347 loss: [0.00626703]\n",
      "epoch: 348 loss: [0.00631431]\n",
      "epoch: 349 loss: [0.00636146]\n",
      "epoch: 350 loss: [0.00640848]\n",
      "epoch: 351 loss: [0.00645538]\n",
      "epoch: 352 loss: [0.00650216]\n",
      "epoch: 353 loss: [0.0065488]\n",
      "epoch: 354 loss: [0.00659531]\n",
      "epoch: 355 loss: [0.00664169]\n",
      "epoch: 356 loss: [0.00668794]\n",
      "epoch: 357 loss: [0.00673405]\n",
      "epoch: 358 loss: [0.00678003]\n",
      "epoch: 359 loss: [0.00682587]\n",
      "epoch: 360 loss: [0.00687157]\n",
      "epoch: 361 loss: [0.00691714]\n",
      "epoch: 362 loss: [0.00696256]\n",
      "epoch: 363 loss: [0.00700785]\n",
      "epoch: 364 loss: [0.00705299]\n",
      "epoch: 365 loss: [0.00709799]\n",
      "epoch: 366 loss: [0.00714285]\n",
      "epoch: 367 loss: [0.00718756]\n",
      "epoch: 368 loss: [0.00723213]\n",
      "epoch: 369 loss: [0.00727655]\n",
      "epoch: 370 loss: [0.00732083]\n",
      "epoch: 371 loss: [0.00736495]\n",
      "epoch: 372 loss: [0.00740893]\n",
      "epoch: 373 loss: [0.00745276]\n",
      "epoch: 374 loss: [0.00749644]\n",
      "epoch: 375 loss: [0.00753998]\n",
      "epoch: 376 loss: [0.00758335]\n",
      "epoch: 377 loss: [0.00762658]\n",
      "epoch: 378 loss: [0.00766966]\n",
      "epoch: 379 loss: [0.00771258]\n",
      "epoch: 380 loss: [0.00775535]\n",
      "epoch: 381 loss: [0.00779797]\n",
      "epoch: 382 loss: [0.00784043]\n",
      "epoch: 383 loss: [0.00788274]\n",
      "epoch: 384 loss: [0.00792489]\n",
      "epoch: 385 loss: [0.00796688]\n",
      "epoch: 386 loss: [0.00800872]\n",
      "epoch: 387 loss: [0.00805041]\n",
      "epoch: 388 loss: [0.00809193]\n",
      "epoch: 389 loss: [0.0081333]\n",
      "epoch: 390 loss: [0.00817451]\n",
      "epoch: 391 loss: [0.00821557]\n",
      "epoch: 392 loss: [0.00825646]\n",
      "epoch: 393 loss: [0.0082972]\n",
      "epoch: 394 loss: [0.00833778]\n",
      "epoch: 395 loss: [0.0083782]\n",
      "epoch: 396 loss: [0.00841846]\n",
      "epoch: 397 loss: [0.00845856]\n",
      "epoch: 398 loss: [0.0084985]\n",
      "epoch: 399 loss: [0.00853828]\n",
      "epoch: 400 loss: [0.0085779]\n",
      "epoch: 401 loss: [0.00861736]\n",
      "epoch: 402 loss: [0.00865666]\n",
      "epoch: 403 loss: [0.0086958]\n",
      "epoch: 404 loss: [0.00873478]\n",
      "epoch: 405 loss: [0.0087736]\n",
      "epoch: 406 loss: [0.00881226]\n",
      "epoch: 407 loss: [0.00885076]\n",
      "epoch: 408 loss: [0.0088891]\n",
      "epoch: 409 loss: [0.00892728]\n",
      "epoch: 410 loss: [0.00896529]\n",
      "epoch: 411 loss: [0.00900315]\n",
      "epoch: 412 loss: [0.00904084]\n",
      "epoch: 413 loss: [0.00907838]\n",
      "epoch: 414 loss: [0.00911575]\n",
      "epoch: 415 loss: [0.00915296]\n",
      "epoch: 416 loss: [0.00919002]\n",
      "epoch: 417 loss: [0.00922691]\n",
      "epoch: 418 loss: [0.00926364]\n",
      "epoch: 419 loss: [0.00930021]\n",
      "epoch: 420 loss: [0.00933662]\n",
      "epoch: 421 loss: [0.00937287]\n",
      "epoch: 422 loss: [0.00940897]\n",
      "epoch: 423 loss: [0.0094449]\n",
      "epoch: 424 loss: [0.00948067]\n",
      "epoch: 425 loss: [0.00951628]\n",
      "epoch: 426 loss: [0.00955173]\n",
      "epoch: 427 loss: [0.00958703]\n",
      "epoch: 428 loss: [0.00962216]\n",
      "epoch: 429 loss: [0.00965714]\n",
      "epoch: 430 loss: [0.00969195]\n",
      "epoch: 431 loss: [0.00972661]\n",
      "epoch: 432 loss: [0.00976111]\n",
      "epoch: 433 loss: [0.00979546]\n",
      "epoch: 434 loss: [0.00982964]\n",
      "epoch: 435 loss: [0.00986367]\n",
      "epoch: 436 loss: [0.00989754]\n",
      "epoch: 437 loss: [0.00993125]\n",
      "epoch: 438 loss: [0.00996481]\n",
      "epoch: 439 loss: [0.00999821]\n",
      "epoch: 440 loss: [0.01003146]\n",
      "epoch: 441 loss: [0.01006454]\n",
      "epoch: 442 loss: [0.01009748]\n",
      "epoch: 443 loss: [0.01013026]\n",
      "epoch: 444 loss: [0.01016288]\n",
      "epoch: 445 loss: [0.01019535]\n",
      "epoch: 446 loss: [0.01022766]\n",
      "epoch: 447 loss: [0.01025982]\n",
      "epoch: 448 loss: [0.01029183]\n",
      "epoch: 449 loss: [0.01032368]\n",
      "epoch: 450 loss: [0.01035539]\n",
      "epoch: 451 loss: [0.01038693]\n",
      "epoch: 452 loss: [0.01041833]\n",
      "epoch: 453 loss: [0.01044957]\n",
      "epoch: 454 loss: [0.01048067]\n",
      "epoch: 455 loss: [0.01051161]\n",
      "epoch: 456 loss: [0.0105424]\n",
      "epoch: 457 loss: [0.01057304]\n",
      "epoch: 458 loss: [0.01060353]\n",
      "epoch: 459 loss: [0.01063388]\n",
      "epoch: 460 loss: [0.01066407]\n",
      "epoch: 461 loss: [0.01069411]\n",
      "epoch: 462 loss: [0.01072401]\n",
      "epoch: 463 loss: [0.01075376]\n",
      "epoch: 464 loss: [0.01078336]\n",
      "epoch: 465 loss: [0.01081281]\n",
      "epoch: 466 loss: [0.01084212]\n",
      "epoch: 467 loss: [0.01087128]\n",
      "epoch: 468 loss: [0.01090029]\n",
      "epoch: 469 loss: [0.01092916]\n",
      "epoch: 470 loss: [0.01095789]\n",
      "epoch: 471 loss: [0.01098647]\n",
      "epoch: 472 loss: [0.0110149]\n",
      "epoch: 473 loss: [0.0110432]\n",
      "epoch: 474 loss: [0.01107135]\n",
      "epoch: 475 loss: [0.01109935]\n",
      "epoch: 476 loss: [0.01112722]\n",
      "epoch: 477 loss: [0.01115494]\n",
      "epoch: 478 loss: [0.01118252]\n",
      "epoch: 479 loss: [0.01120996]\n",
      "epoch: 480 loss: [0.01123727]\n",
      "epoch: 481 loss: [0.01126443]\n",
      "epoch: 482 loss: [0.01129145]\n",
      "epoch: 483 loss: [0.01131833]\n",
      "epoch: 484 loss: [0.01134508]\n",
      "epoch: 485 loss: [0.01137168]\n",
      "epoch: 486 loss: [0.01139815]\n",
      "epoch: 487 loss: [0.01142448]\n",
      "epoch: 488 loss: [0.01145068]\n",
      "epoch: 489 loss: [0.01147674]\n",
      "epoch: 490 loss: [0.01150266]\n",
      "epoch: 491 loss: [0.01152845]\n",
      "epoch: 492 loss: [0.01155411]\n",
      "epoch: 493 loss: [0.01157963]\n",
      "epoch: 494 loss: [0.01160501]\n",
      "epoch: 495 loss: [0.01163027]\n",
      "epoch: 496 loss: [0.01165539]\n",
      "epoch: 497 loss: [0.01168038]\n",
      "epoch: 498 loss: [0.01170523]\n",
      "epoch: 499 loss: [0.01172996]\n",
      "epoch: 500 loss: [0.01175456]\n",
      "epoch: 501 loss: [0.01177902]\n",
      "epoch: 502 loss: [0.01180336]\n",
      "epoch: 503 loss: [0.01182756]\n",
      "epoch: 504 loss: [0.01185164]\n",
      "epoch: 505 loss: [0.01187559]\n",
      "epoch: 506 loss: [0.01189942]\n",
      "epoch: 507 loss: [0.01192311]\n",
      "epoch: 508 loss: [0.01194668]\n",
      "epoch: 509 loss: [0.01197012]\n",
      "epoch: 510 loss: [0.01199344]\n",
      "epoch: 511 loss: [0.01201663]\n",
      "epoch: 512 loss: [0.0120397]\n",
      "epoch: 513 loss: [0.01206265]\n",
      "epoch: 514 loss: [0.01208547]\n",
      "epoch: 515 loss: [0.01210817]\n",
      "epoch: 516 loss: [0.01213074]\n",
      "epoch: 517 loss: [0.01215319]\n",
      "epoch: 518 loss: [0.01217553]\n",
      "epoch: 519 loss: [0.01219774]\n",
      "epoch: 520 loss: [0.01221983]\n",
      "epoch: 521 loss: [0.0122418]\n",
      "epoch: 522 loss: [0.01226365]\n",
      "epoch: 523 loss: [0.01228538]\n",
      "epoch: 524 loss: [0.01230699]\n",
      "epoch: 525 loss: [0.01232849]\n",
      "epoch: 526 loss: [0.01234987]\n",
      "epoch: 527 loss: [0.01237113]\n",
      "epoch: 528 loss: [0.01239228]\n",
      "epoch: 529 loss: [0.01241331]\n",
      "epoch: 530 loss: [0.01243422]\n",
      "epoch: 531 loss: [0.01245502]\n",
      "epoch: 532 loss: [0.0124757]\n",
      "epoch: 533 loss: [0.01249628]\n",
      "epoch: 534 loss: [0.01251673]\n",
      "epoch: 535 loss: [0.01253708]\n",
      "epoch: 536 loss: [0.01255731]\n",
      "epoch: 537 loss: [0.01257743]\n",
      "epoch: 538 loss: [0.01259744]\n",
      "epoch: 539 loss: [0.01261734]\n",
      "epoch: 540 loss: [0.01263713]\n",
      "epoch: 541 loss: [0.01265681]\n",
      "epoch: 542 loss: [0.01267638]\n",
      "epoch: 543 loss: [0.01269584]\n",
      "epoch: 544 loss: [0.01271519]\n",
      "epoch: 545 loss: [0.01273444]\n",
      "epoch: 546 loss: [0.01275358]\n",
      "epoch: 547 loss: [0.01277261]\n",
      "epoch: 548 loss: [0.01279153]\n",
      "epoch: 549 loss: [0.01281035]\n",
      "epoch: 550 loss: [0.01282906]\n",
      "epoch: 551 loss: [0.01284767]\n",
      "epoch: 552 loss: [0.01286618]\n",
      "epoch: 553 loss: [0.01288458]\n",
      "epoch: 554 loss: [0.01290287]\n",
      "epoch: 555 loss: [0.01292107]\n",
      "epoch: 556 loss: [0.01293916]\n",
      "epoch: 557 loss: [0.01295715]\n",
      "epoch: 558 loss: [0.01297504]\n",
      "epoch: 559 loss: [0.01299283]\n",
      "epoch: 560 loss: [0.01301051]\n",
      "epoch: 561 loss: [0.0130281]\n",
      "epoch: 562 loss: [0.01304559]\n",
      "epoch: 563 loss: [0.01306298]\n",
      "epoch: 564 loss: [0.01308027]\n",
      "epoch: 565 loss: [0.01309746]\n",
      "epoch: 566 loss: [0.01311456]\n",
      "epoch: 567 loss: [0.01313156]\n",
      "epoch: 568 loss: [0.01314846]\n",
      "epoch: 569 loss: [0.01316526]\n",
      "epoch: 570 loss: [0.01318197]\n",
      "epoch: 571 loss: [0.01319859]\n",
      "epoch: 572 loss: [0.01321511]\n",
      "epoch: 573 loss: [0.01323153]\n",
      "epoch: 574 loss: [0.01324787]\n",
      "epoch: 575 loss: [0.01326411]\n",
      "epoch: 576 loss: [0.01328025]\n",
      "epoch: 577 loss: [0.0132963]\n",
      "epoch: 578 loss: [0.01331227]\n",
      "epoch: 579 loss: [0.01332814]\n",
      "epoch: 580 loss: [0.01334392]\n",
      "epoch: 581 loss: [0.01335961]\n",
      "epoch: 582 loss: [0.0133752]\n",
      "epoch: 583 loss: [0.01339071]\n",
      "epoch: 584 loss: [0.01340613]\n",
      "epoch: 585 loss: [0.01342147]\n",
      "epoch: 586 loss: [0.01343671]\n",
      "epoch: 587 loss: [0.01345186]\n",
      "epoch: 588 loss: [0.01346693]\n",
      "epoch: 589 loss: [0.01348191]\n",
      "epoch: 590 loss: [0.01349681]\n",
      "epoch: 591 loss: [0.01351162]\n",
      "epoch: 592 loss: [0.01352634]\n",
      "epoch: 593 loss: [0.01354098]\n",
      "epoch: 594 loss: [0.01355553]\n",
      "epoch: 595 loss: [0.01357]\n",
      "epoch: 596 loss: [0.01358439]\n",
      "epoch: 597 loss: [0.01359869]\n",
      "epoch: 598 loss: [0.01361291]\n",
      "epoch: 599 loss: [0.01362704]\n",
      "epoch: 600 loss: [0.0136411]\n",
      "epoch: 601 loss: [0.01365507]\n",
      "epoch: 602 loss: [0.01366896]\n",
      "epoch: 603 loss: [0.01368277]\n",
      "epoch: 604 loss: [0.0136965]\n",
      "epoch: 605 loss: [0.01371015]\n",
      "epoch: 606 loss: [0.01372373]\n",
      "epoch: 607 loss: [0.01373722]\n",
      "epoch: 608 loss: [0.01375063]\n",
      "epoch: 609 loss: [0.01376396]\n",
      "epoch: 610 loss: [0.01377722]\n",
      "epoch: 611 loss: [0.0137904]\n",
      "epoch: 612 loss: [0.0138035]\n",
      "epoch: 613 loss: [0.01381653]\n",
      "epoch: 614 loss: [0.01382948]\n",
      "epoch: 615 loss: [0.01384235]\n",
      "epoch: 616 loss: [0.01385515]\n",
      "epoch: 617 loss: [0.01386787]\n",
      "epoch: 618 loss: [0.01388052]\n",
      "epoch: 619 loss: [0.0138931]\n",
      "epoch: 620 loss: [0.0139056]\n",
      "epoch: 621 loss: [0.01391802]\n",
      "epoch: 622 loss: [0.01393038]\n",
      "epoch: 623 loss: [0.01394266]\n",
      "epoch: 624 loss: [0.01395487]\n",
      "epoch: 625 loss: [0.013967]\n",
      "epoch: 626 loss: [0.01397907]\n",
      "epoch: 627 loss: [0.01399106]\n",
      "epoch: 628 loss: [0.01400299]\n",
      "epoch: 629 loss: [0.01401484]\n",
      "epoch: 630 loss: [0.01402663]\n",
      "epoch: 631 loss: [0.01403834]\n",
      "epoch: 632 loss: [0.01404998]\n",
      "epoch: 633 loss: [0.01406156]\n",
      "epoch: 634 loss: [0.01407307]\n",
      "epoch: 635 loss: [0.01408451]\n",
      "epoch: 636 loss: [0.01409588]\n",
      "epoch: 637 loss: [0.01410718]\n",
      "epoch: 638 loss: [0.01411842]\n",
      "epoch: 639 loss: [0.01412959]\n",
      "epoch: 640 loss: [0.0141407]\n",
      "epoch: 641 loss: [0.01415174]\n",
      "epoch: 642 loss: [0.01416271]\n",
      "epoch: 643 loss: [0.01417362]\n",
      "epoch: 644 loss: [0.01418446]\n",
      "epoch: 645 loss: [0.01419524]\n",
      "epoch: 646 loss: [0.01420596]\n",
      "epoch: 647 loss: [0.01421661]\n",
      "epoch: 648 loss: [0.0142272]\n",
      "epoch: 649 loss: [0.01423772]\n",
      "epoch: 650 loss: [0.01424818]\n",
      "epoch: 651 loss: [0.01425858]\n",
      "epoch: 652 loss: [0.01426892]\n",
      "epoch: 653 loss: [0.0142792]\n",
      "epoch: 654 loss: [0.01428941]\n",
      "epoch: 655 loss: [0.01429957]\n",
      "epoch: 656 loss: [0.01430966]\n",
      "epoch: 657 loss: [0.0143197]\n",
      "epoch: 658 loss: [0.01432967]\n",
      "epoch: 659 loss: [0.01433958]\n",
      "epoch: 660 loss: [0.01434944]\n",
      "epoch: 661 loss: [0.01435923]\n",
      "epoch: 662 loss: [0.01436897]\n",
      "epoch: 663 loss: [0.01437865]\n",
      "epoch: 664 loss: [0.01438827]\n",
      "epoch: 665 loss: [0.01439783]\n",
      "epoch: 666 loss: [0.01440734]\n",
      "epoch: 667 loss: [0.01441679]\n",
      "epoch: 668 loss: [0.01442618]\n",
      "epoch: 669 loss: [0.01443552]\n",
      "epoch: 670 loss: [0.0144448]\n",
      "epoch: 671 loss: [0.01445402]\n",
      "epoch: 672 loss: [0.01446319]\n",
      "epoch: 673 loss: [0.01447231]\n",
      "epoch: 674 loss: [0.01448137]\n",
      "epoch: 675 loss: [0.01449037]\n",
      "epoch: 676 loss: [0.01449932]\n",
      "epoch: 677 loss: [0.01450822]\n",
      "epoch: 678 loss: [0.01451706]\n",
      "epoch: 679 loss: [0.01452585]\n",
      "epoch: 680 loss: [0.01453459]\n",
      "epoch: 681 loss: [0.01454328]\n",
      "epoch: 682 loss: [0.01455191]\n",
      "epoch: 683 loss: [0.01456049]\n",
      "epoch: 684 loss: [0.01456902]\n",
      "epoch: 685 loss: [0.0145775]\n",
      "epoch: 686 loss: [0.01458592]\n",
      "epoch: 687 loss: [0.0145943]\n",
      "epoch: 688 loss: [0.01460262]\n",
      "epoch: 689 loss: [0.0146109]\n",
      "epoch: 690 loss: [0.01461912]\n",
      "epoch: 691 loss: [0.0146273]\n",
      "epoch: 692 loss: [0.01463542]\n",
      "epoch: 693 loss: [0.0146435]\n",
      "epoch: 694 loss: [0.01465153]\n",
      "epoch: 695 loss: [0.01465951]\n",
      "epoch: 696 loss: [0.01466744]\n",
      "epoch: 697 loss: [0.01467532]\n",
      "epoch: 698 loss: [0.01468316]\n",
      "epoch: 699 loss: [0.01469094]\n",
      "epoch: 700 loss: [0.01469869]\n",
      "epoch: 701 loss: [0.01470638]\n",
      "epoch: 702 loss: [0.01471403]\n",
      "epoch: 703 loss: [0.01472163]\n",
      "epoch: 704 loss: [0.01472918]\n",
      "epoch: 705 loss: [0.01473669]\n",
      "epoch: 706 loss: [0.01474415]\n",
      "epoch: 707 loss: [0.01475157]\n",
      "epoch: 708 loss: [0.01475895]\n",
      "epoch: 709 loss: [0.01476627]\n",
      "epoch: 710 loss: [0.01477356]\n",
      "epoch: 711 loss: [0.0147808]\n",
      "epoch: 712 loss: [0.01478799]\n",
      "epoch: 713 loss: [0.01479515]\n",
      "epoch: 714 loss: [0.01480226]\n",
      "epoch: 715 loss: [0.01480932]\n",
      "epoch: 716 loss: [0.01481634]\n",
      "epoch: 717 loss: [0.01482332]\n",
      "epoch: 718 loss: [0.01483026]\n",
      "epoch: 719 loss: [0.01483715]\n",
      "epoch: 720 loss: [0.01484401]\n",
      "epoch: 721 loss: [0.01485082]\n",
      "epoch: 722 loss: [0.01485759]\n",
      "epoch: 723 loss: [0.01486432]\n",
      "epoch: 724 loss: [0.01487101]\n",
      "epoch: 725 loss: [0.01487765]\n",
      "epoch: 726 loss: [0.01488426]\n",
      "epoch: 727 loss: [0.01489082]\n",
      "epoch: 728 loss: [0.01489735]\n",
      "epoch: 729 loss: [0.01490384]\n",
      "epoch: 730 loss: [0.01491028]\n",
      "epoch: 731 loss: [0.01491669]\n",
      "epoch: 732 loss: [0.01492306]\n",
      "epoch: 733 loss: [0.01492939]\n",
      "epoch: 734 loss: [0.01493568]\n",
      "epoch: 735 loss: [0.01494193]\n",
      "epoch: 736 loss: [0.01494815]\n",
      "epoch: 737 loss: [0.01495432]\n",
      "epoch: 738 loss: [0.01496046]\n",
      "epoch: 739 loss: [0.01496656]\n",
      "epoch: 740 loss: [0.01497262]\n",
      "epoch: 741 loss: [0.01497865]\n",
      "epoch: 742 loss: [0.01498464]\n",
      "epoch: 743 loss: [0.01499059]\n",
      "epoch: 744 loss: [0.01499651]\n",
      "epoch: 745 loss: [0.01500239]\n",
      "epoch: 746 loss: [0.01500824]\n",
      "epoch: 747 loss: [0.01501404]\n",
      "epoch: 748 loss: [0.01501982]\n",
      "epoch: 749 loss: [0.01502555]\n",
      "epoch: 750 loss: [0.01503126]\n",
      "epoch: 751 loss: [0.01503692]\n",
      "epoch: 752 loss: [0.01504256]\n",
      "epoch: 753 loss: [0.01504816]\n",
      "epoch: 754 loss: [0.01505372]\n",
      "epoch: 755 loss: [0.01505925]\n",
      "epoch: 756 loss: [0.01506474]\n",
      "epoch: 757 loss: [0.01507021]\n",
      "epoch: 758 loss: [0.01507564]\n",
      "epoch: 759 loss: [0.01508103]\n",
      "epoch: 760 loss: [0.01508639]\n",
      "epoch: 761 loss: [0.01509172]\n",
      "epoch: 762 loss: [0.01509702]\n",
      "epoch: 763 loss: [0.01510228]\n",
      "epoch: 764 loss: [0.01510751]\n",
      "epoch: 765 loss: [0.01511271]\n",
      "epoch: 766 loss: [0.01511788]\n",
      "epoch: 767 loss: [0.01512302]\n",
      "epoch: 768 loss: [0.01512812]\n",
      "epoch: 769 loss: [0.01513319]\n",
      "epoch: 770 loss: [0.01513824]\n",
      "epoch: 771 loss: [0.01514325]\n",
      "epoch: 772 loss: [0.01514823]\n",
      "epoch: 773 loss: [0.01515318]\n",
      "epoch: 774 loss: [0.0151581]\n",
      "epoch: 775 loss: [0.01516298]\n",
      "epoch: 776 loss: [0.01516784]\n",
      "epoch: 777 loss: [0.01517267]\n",
      "epoch: 778 loss: [0.01517747]\n",
      "epoch: 779 loss: [0.01518224]\n",
      "epoch: 780 loss: [0.01518698]\n",
      "epoch: 781 loss: [0.01519169]\n",
      "epoch: 782 loss: [0.01519637]\n",
      "epoch: 783 loss: [0.01520102]\n",
      "epoch: 784 loss: [0.01520565]\n",
      "epoch: 785 loss: [0.01521024]\n",
      "epoch: 786 loss: [0.01521481]\n",
      "epoch: 787 loss: [0.01521935]\n",
      "epoch: 788 loss: [0.01522386]\n",
      "epoch: 789 loss: [0.01522834]\n",
      "epoch: 790 loss: [0.0152328]\n",
      "epoch: 791 loss: [0.01523723]\n",
      "epoch: 792 loss: [0.01524163]\n",
      "epoch: 793 loss: [0.015246]\n",
      "epoch: 794 loss: [0.01525035]\n",
      "epoch: 795 loss: [0.01525467]\n",
      "epoch: 796 loss: [0.01525896]\n",
      "epoch: 797 loss: [0.01526323]\n",
      "epoch: 798 loss: [0.01526747]\n",
      "epoch: 799 loss: [0.01527168]\n",
      "epoch: 800 loss: [0.01527587]\n",
      "epoch: 801 loss: [0.01528003]\n",
      "epoch: 802 loss: [0.01528417]\n",
      "epoch: 803 loss: [0.01528828]\n",
      "epoch: 804 loss: [0.01529236]\n",
      "epoch: 805 loss: [0.01529642]\n",
      "epoch: 806 loss: [0.01530046]\n",
      "epoch: 807 loss: [0.01530447]\n",
      "epoch: 808 loss: [0.01530846]\n",
      "epoch: 809 loss: [0.01531242]\n",
      "epoch: 810 loss: [0.01531635]\n",
      "epoch: 811 loss: [0.01532026]\n",
      "epoch: 812 loss: [0.01532415]\n",
      "epoch: 813 loss: [0.01532801]\n",
      "epoch: 814 loss: [0.01533185]\n",
      "epoch: 815 loss: [0.01533567]\n",
      "epoch: 816 loss: [0.01533946]\n",
      "epoch: 817 loss: [0.01534323]\n",
      "epoch: 818 loss: [0.01534698]\n",
      "epoch: 819 loss: [0.0153507]\n",
      "epoch: 820 loss: [0.0153544]\n",
      "epoch: 821 loss: [0.01535807]\n",
      "epoch: 822 loss: [0.01536173]\n",
      "epoch: 823 loss: [0.01536536]\n",
      "epoch: 824 loss: [0.01536897]\n",
      "epoch: 825 loss: [0.01537255]\n",
      "epoch: 826 loss: [0.01537612]\n",
      "epoch: 827 loss: [0.01537966]\n",
      "epoch: 828 loss: [0.01538318]\n",
      "epoch: 829 loss: [0.01538668]\n",
      "epoch: 830 loss: [0.01539015]\n",
      "epoch: 831 loss: [0.01539361]\n",
      "epoch: 832 loss: [0.01539704]\n",
      "epoch: 833 loss: [0.01540045]\n",
      "epoch: 834 loss: [0.01540384]\n",
      "epoch: 835 loss: [0.01540721]\n",
      "epoch: 836 loss: [0.01541056]\n",
      "epoch: 837 loss: [0.01541389]\n",
      "epoch: 838 loss: [0.0154172]\n",
      "epoch: 839 loss: [0.01542048]\n",
      "epoch: 840 loss: [0.01542375]\n",
      "epoch: 841 loss: [0.015427]\n",
      "epoch: 842 loss: [0.01543022]\n",
      "epoch: 843 loss: [0.01543343]\n",
      "epoch: 844 loss: [0.01543661]\n",
      "epoch: 845 loss: [0.01543978]\n",
      "epoch: 846 loss: [0.01544293]\n",
      "epoch: 847 loss: [0.01544605]\n",
      "epoch: 848 loss: [0.01544916]\n",
      "epoch: 849 loss: [0.01545225]\n",
      "epoch: 850 loss: [0.01545532]\n",
      "epoch: 851 loss: [0.01545837]\n",
      "epoch: 852 loss: [0.0154614]\n",
      "epoch: 853 loss: [0.01546441]\n",
      "epoch: 854 loss: [0.0154674]\n",
      "epoch: 855 loss: [0.01547038]\n",
      "epoch: 856 loss: [0.01547334]\n",
      "epoch: 857 loss: [0.01547627]\n",
      "epoch: 858 loss: [0.01547919]\n",
      "epoch: 859 loss: [0.01548209]\n",
      "epoch: 860 loss: [0.01548498]\n",
      "epoch: 861 loss: [0.01548784]\n",
      "epoch: 862 loss: [0.01549069]\n",
      "epoch: 863 loss: [0.01549352]\n",
      "epoch: 864 loss: [0.01549633]\n",
      "epoch: 865 loss: [0.01549913]\n",
      "epoch: 866 loss: [0.01550191]\n",
      "epoch: 867 loss: [0.01550467]\n",
      "epoch: 868 loss: [0.01550741]\n",
      "epoch: 869 loss: [0.01551013]\n",
      "epoch: 870 loss: [0.01551284]\n",
      "epoch: 871 loss: [0.01551554]\n",
      "epoch: 872 loss: [0.01551821]\n",
      "epoch: 873 loss: [0.01552087]\n",
      "epoch: 874 loss: [0.01552351]\n",
      "epoch: 875 loss: [0.01552614]\n",
      "epoch: 876 loss: [0.01552875]\n",
      "epoch: 877 loss: [0.01553134]\n",
      "epoch: 878 loss: [0.01553392]\n",
      "epoch: 879 loss: [0.01553648]\n",
      "epoch: 880 loss: [0.01553902]\n",
      "epoch: 881 loss: [0.01554155]\n",
      "epoch: 882 loss: [0.01554406]\n",
      "epoch: 883 loss: [0.01554656]\n",
      "epoch: 884 loss: [0.01554904]\n",
      "epoch: 885 loss: [0.01555151]\n",
      "epoch: 886 loss: [0.01555396]\n",
      "epoch: 887 loss: [0.0155564]\n",
      "epoch: 888 loss: [0.01555882]\n",
      "epoch: 889 loss: [0.01556122]\n",
      "epoch: 890 loss: [0.01556361]\n",
      "epoch: 891 loss: [0.01556599]\n",
      "epoch: 892 loss: [0.01556835]\n",
      "epoch: 893 loss: [0.0155707]\n",
      "epoch: 894 loss: [0.01557303]\n",
      "epoch: 895 loss: [0.01557534]\n",
      "epoch: 896 loss: [0.01557765]\n",
      "epoch: 897 loss: [0.01557994]\n",
      "epoch: 898 loss: [0.01558221]\n",
      "epoch: 899 loss: [0.01558447]\n",
      "epoch: 900 loss: [0.01558671]\n",
      "epoch: 901 loss: [0.01558895]\n",
      "epoch: 902 loss: [0.01559116]\n",
      "epoch: 903 loss: [0.01559337]\n",
      "epoch: 904 loss: [0.01559556]\n",
      "epoch: 905 loss: [0.01559773]\n",
      "epoch: 906 loss: [0.0155999]\n",
      "epoch: 907 loss: [0.01560205]\n",
      "epoch: 908 loss: [0.01560418]\n",
      "epoch: 909 loss: [0.0156063]\n",
      "epoch: 910 loss: [0.01560841]\n",
      "epoch: 911 loss: [0.01561051]\n",
      "epoch: 912 loss: [0.01561259]\n",
      "epoch: 913 loss: [0.01561466]\n",
      "epoch: 914 loss: [0.01561672]\n",
      "epoch: 915 loss: [0.01561876]\n",
      "epoch: 916 loss: [0.01562079]\n",
      "epoch: 917 loss: [0.01562281]\n",
      "epoch: 918 loss: [0.01562482]\n",
      "epoch: 919 loss: [0.01562681]\n",
      "epoch: 920 loss: [0.01562879]\n",
      "epoch: 921 loss: [0.01563076]\n",
      "epoch: 922 loss: [0.01563272]\n",
      "epoch: 923 loss: [0.01563466]\n",
      "epoch: 924 loss: [0.01563659]\n",
      "epoch: 925 loss: [0.01563851]\n",
      "epoch: 926 loss: [0.01564042]\n",
      "epoch: 927 loss: [0.01564232]\n",
      "epoch: 928 loss: [0.0156442]\n",
      "epoch: 929 loss: [0.01564607]\n",
      "epoch: 930 loss: [0.01564794]\n",
      "epoch: 931 loss: [0.01564978]\n",
      "epoch: 932 loss: [0.01565162]\n",
      "epoch: 933 loss: [0.01565345]\n",
      "epoch: 934 loss: [0.01565526]\n",
      "epoch: 935 loss: [0.01565707]\n",
      "epoch: 936 loss: [0.01565886]\n",
      "epoch: 937 loss: [0.01566064]\n",
      "epoch: 938 loss: [0.01566241]\n",
      "epoch: 939 loss: [0.01566417]\n",
      "epoch: 940 loss: [0.01566591]\n",
      "epoch: 941 loss: [0.01566765]\n",
      "epoch: 942 loss: [0.01566938]\n",
      "epoch: 943 loss: [0.01567109]\n",
      "epoch: 944 loss: [0.01567279]\n",
      "epoch: 945 loss: [0.01567449]\n",
      "epoch: 946 loss: [0.01567617]\n",
      "epoch: 947 loss: [0.01567784]\n",
      "epoch: 948 loss: [0.0156795]\n",
      "epoch: 949 loss: [0.01568116]\n",
      "epoch: 950 loss: [0.0156828]\n",
      "epoch: 951 loss: [0.01568443]\n",
      "epoch: 952 loss: [0.01568605]\n",
      "epoch: 953 loss: [0.01568766]\n",
      "epoch: 954 loss: [0.01568926]\n",
      "epoch: 955 loss: [0.01569085]\n",
      "epoch: 956 loss: [0.01569243]\n",
      "epoch: 957 loss: [0.015694]\n",
      "epoch: 958 loss: [0.01569556]\n",
      "epoch: 959 loss: [0.01569711]\n",
      "epoch: 960 loss: [0.01569865]\n",
      "epoch: 961 loss: [0.01570019]\n",
      "epoch: 962 loss: [0.01570171]\n",
      "epoch: 963 loss: [0.01570322]\n",
      "epoch: 964 loss: [0.01570472]\n",
      "epoch: 965 loss: [0.01570622]\n",
      "epoch: 966 loss: [0.0157077]\n",
      "epoch: 967 loss: [0.01570918]\n",
      "epoch: 968 loss: [0.01571064]\n",
      "epoch: 969 loss: [0.0157121]\n",
      "epoch: 970 loss: [0.01571354]\n",
      "epoch: 971 loss: [0.01571498]\n",
      "epoch: 972 loss: [0.01571641]\n",
      "epoch: 973 loss: [0.01571783]\n",
      "epoch: 974 loss: [0.01571924]\n",
      "epoch: 975 loss: [0.01572065]\n",
      "epoch: 976 loss: [0.01572204]\n",
      "epoch: 977 loss: [0.01572342]\n",
      "epoch: 978 loss: [0.0157248]\n",
      "epoch: 979 loss: [0.01572617]\n",
      "epoch: 980 loss: [0.01572753]\n",
      "epoch: 981 loss: [0.01572888]\n",
      "epoch: 982 loss: [0.01573022]\n",
      "epoch: 983 loss: [0.01573155]\n",
      "epoch: 984 loss: [0.01573288]\n",
      "epoch: 985 loss: [0.0157342]\n",
      "epoch: 986 loss: [0.01573551]\n",
      "epoch: 987 loss: [0.01573681]\n",
      "epoch: 988 loss: [0.0157381]\n",
      "epoch: 989 loss: [0.01573938]\n",
      "epoch: 990 loss: [0.01574066]\n",
      "epoch: 991 loss: [0.01574193]\n",
      "epoch: 992 loss: [0.01574319]\n",
      "epoch: 993 loss: [0.01574444]\n",
      "epoch: 994 loss: [0.01574568]\n",
      "epoch: 995 loss: [0.01574692]\n",
      "epoch: 996 loss: [0.01574815]\n",
      "epoch: 997 loss: [0.01574937]\n",
      "epoch: 998 loss: [0.01575059]\n",
      "epoch: 999 loss: [0.01575179]\n"
     ]
    }
   ],
   "source": [
    "fit(model, linear_activation, x_list, y_true, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0.254729173621862 err: [-0.25472917]\n",
      "[0. 1.] [1.] -> 0.7502406377334313 err: [0.24975936]\n",
      "[1. 0.] [1.] -> 0.7479704476485354 err: [0.25202955]\n",
      "[1. 1.] [1.] -> 1.2434819117601048 err: [-0.24348191]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, linear_activation, x_list, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.] [0.] -> 0 err: [0.]\n",
      "[0. 1.] [1.] -> 1 err: [0.]\n",
      "[1. 0.] [1.] -> 1 err: [0.]\n",
      "[1. 1.] [1.] -> 1 err: [0.]\n"
     ]
    }
   ],
   "source": [
    "print_results(model, step_activation, x_list, y_true)"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
